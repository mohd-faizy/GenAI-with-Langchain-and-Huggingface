{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db15d3b6",
   "metadata": {},
   "source": [
    "# üó®Ô∏è**Prompts in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20e7ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![author](https://img.shields.io/badge/author-mohd--faizy-red)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18386d53",
   "metadata": {},
   "source": [
    "## **Parameters:** `Temperature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2b71",
   "metadata": {},
   "source": [
    "### üî• What is Temperature in LLMs?\n",
    "\n",
    "* **Temperature** controls how *random or predictable* a language model‚Äôs output is.\n",
    "* It affects how the model picks the *next word* in a sentence.\n",
    "\n",
    "\n",
    "### üßä Low Temperature (Closer to 0)\n",
    "\n",
    "* Makes the output **more focused and predictable**.\n",
    "* The model chooses **high-probability words** more often.\n",
    "* Good for **factual tasks** (like answering questions or coding).\n",
    "* Example: If \"blue\" has 80% chance and \"green\" 20%, a low temp might choose \"blue\" 99% of the time.\n",
    "\n",
    "\n",
    "### üî• High Temperature (Above 1)\n",
    "\n",
    "* Makes the output **more creative and random**.\n",
    "* The model is more likely to try **less common words**.\n",
    "* Great for **brainstorming or creative writing**.\n",
    "* In the same example, \"green\" might now be picked 40% of the time.\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è How It Works (Simplified)\n",
    "\n",
    "* The model gives *scores* (called **logits**) to each possible next word.\n",
    "* Temperature scales these scores **before** turning them into probabilities:\n",
    "\n",
    "  * `Temp = 1`: no change to scores.\n",
    "  * `Temp < 1`: increases the gap between scores ‚Üí more confident choices.\n",
    "  * `Temp > 1`: narrows the gap ‚Üí more variety in output.\n",
    "\n",
    "\n",
    "### üéØ When to Use What?\n",
    "\n",
    "| Use Case            | Recommended Temperature |\n",
    "| ------------------- | ----------------------- |\n",
    "| Technical support   | 0.2 ‚Äì 0.5 (focused)     |\n",
    "| Code generation     | 0.2 ‚Äì 0.4 (precise)     |\n",
    "| Creative writing    | 0.7 ‚Äì 1.2 (diverse)     |\n",
    "| Casual conversation | 0.6 ‚Äì 0.9 (engaging)    |\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏è Things to Watch Out For\n",
    "\n",
    "* **Too low** (e.g. 0): Model becomes repetitive or boring.\n",
    "* **Too high** (e.g. 2): Responses can become weird or incoherent.\n",
    "* **Balance is key** ‚Äì Experiment to find the sweet spot for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233b1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In realms of code and digital space,\n",
      "Langchain weaves a cognitive pace,\n",
      "With chains of thought and links so fine,\n",
      "It crafts responses that are truly divine,\n",
      "A language model of unparalleled pace.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Code using Groq (Mixtral / LLaMA via Langchain)\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  \n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "result = model.invoke(\"Write a 5 line poen on Langchain.\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f54602",
   "metadata": {},
   "source": [
    "## **Prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d47629",
   "metadata": {},
   "source": [
    ">`Prompts` are the input instructions or queries given to a model to guide its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47db52",
   "metadata": {},
   "source": [
    "### ***Types of Prompts?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871ce81",
   "metadata": {},
   "source": [
    "**Static vs. Dynamic Prompts** \n",
    "\n",
    "| **Aspect**                | **Static Prompts**                                                                      | **Dynamic Prompts**                                                                                                    |\n",
    "| ------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**            | Hard-coded prompts written by the user or developer without structure or placeholders   | Template-based prompts with placeholders filled in at runtime                                                          |\n",
    "| **Control**               | Full control is given to the user; the system has minimal control over prompt structure | Programmer controls the structure, limiting variability and improving consistency                                      |\n",
    "| **Risk of Hallucination** | Higher ‚Äì due to vague or incorrect user input                                           | Lower ‚Äì prompt templates guide the LLM, reducing ambiguity                                                             |\n",
    "| **Consistency**           | Low ‚Äì depends on user input, which varies widely                                        | High ‚Äì ensures uniform tone, format, and style of outputs                                                              |\n",
    "| **Reusability**           | Low ‚Äì specific to each use case, not generalizable                                      | High ‚Äì templates can be reused across different tasks or inputs                                                        |\n",
    "| **User Input Example**    | `\"Summarize 'Attention Is All You Need' in simple terms\"`                               | Template: `\"Summarize the paper titled {paper_input} in a {explanation_style} style and {explanation_length} length.\"` |\n",
    "| **Best Use Case**         | Quick experiments or exploratory tasks                                                  | Production-grade apps, tools requiring consistent outputs, or when guiding LLM behavior is important                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Import Dependencies\n",
    "# ============================\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ============================\n",
    "# Load Environment Variables\n",
    "# ============================\n",
    "load_dotenv()\n",
    "\n",
    "# ============================\n",
    "# Initialize the LLM Model\n",
    "# ============================\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Streamlit UI Setup\n",
    "# ============================\n",
    "st.title(\"AI Research Paper Summarizer\")\n",
    "st.subheader(\"Generate tailored summaries of foundational AI research papers\")\n",
    "\n",
    "# ---------- User Input Controls ----------\n",
    "paper_input = st.selectbox(\n",
    "    \"Select Research Paper\",\n",
    "    [\n",
    "        \"Attention Is All You Need\",\n",
    "        \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"GPT-3: Language Models are Few-Shot Learners\",\n",
    "        \"Diffusion Models Beat GANs on Image Synthesis\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select Explanation Style\",\n",
    "    [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    ")\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select Explanation Length\",\n",
    "    [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Prompt Template Definition\n",
    "# ============================\n",
    "template = PromptTemplate(\n",
    "    input_variables=['paper_input', 'style_input', 'length_input'],\n",
    "    template=\"\"\"\n",
    "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "Explanation Style: {style_input}  \n",
    "Explanation Length: {length_input}  \n",
    "\n",
    "1. Mathematical Details:\n",
    "   - Include relevant mathematical equations if present in the paper.\n",
    "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
    "2. Analogies:\n",
    "   - Use relatable analogies to simplify complex ideas.\n",
    "\n",
    "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
    "Ensure the summary is clear, accurate, and aligned with the selected style and length.\n",
    "\"\"\",\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Generate Prompt from Inputs\n",
    "# ============================\n",
    "prompt = template.invoke({\n",
    "    'paper_input': paper_input,\n",
    "    'style_input': style_input,\n",
    "    'length_input': length_input\n",
    "})\n",
    "\n",
    "# ============================\n",
    "# Generate and Display Output\n",
    "# ============================\n",
    "if st.button('Summarize'):\n",
    "    result = model.invoke(prompt)\n",
    "    st.subheader(\"Summary\")\n",
    "    st.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443d3cf",
   "metadata": {},
   "source": [
    "**When `validate_template=True` is set:**\n",
    "\n",
    "- LangChain checks that all placeholders in your template (e.g., {paper_input}, {style_input}) are:\n",
    "- Properly provided in the `input_variables` list.\n",
    "- Referenced correctly in the actual text of the template.\n",
    "\n",
    "If there‚Äôs a mismatch (e.g., missing a variable or typo like {paper_inpt} instead of {paper_input}), it will raise an error immediately, helping you catch bugs early.\n",
    "\n",
    "```python\n",
    "template = PromptTemplate(\n",
    "    input_variables=['topic'],\n",
    "    template=\"Summarize the topic: {topik}\",  # typo here\n",
    "    validate_template=True\n",
    ")\n",
    "```\n",
    "This will raise an error like:\n",
    "\n",
    "```text\n",
    "ValueError: Found a placeholder in the template that is not in input_variables: 'topik'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb83da0",
   "metadata": {},
   "source": [
    "### ***Why `PromptTemplate`?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadde8cc",
   "metadata": {},
   "source": [
    "A **PromptTemplate** in LangChain is a structured way to create prompts dynamically by inserting variables into a predefined template. Instead of hardcoding prompts, PromptTemplate allows you to define placeholders that can be filled in at runtime with different inputs. This makes it **reusable**, **flexible**, and **easy to manage**, especially when working with dynamic user inputs or automated workflows.\n",
    "\n",
    "**Why use PromptTemplate over `f-strings`?**\n",
    "\n",
    "| **Reason**                   | **Explanation**                                                                                              |\n",
    "| ---------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **1. Default Validation**    | `PromptTemplate` checks if all required variables are provided, avoiding runtime errors from missing values. |\n",
    "| **2. Reusable**              | Once defined, a prompt can be reused with different inputs across workflows or chains.                       |\n",
    "| **3. LangChain Ecosystem**   | Seamlessly integrates with LangChain components like chains, memory, and tools ‚Äî enabling composability.     |\n",
    "| **4. Cleaner Abstractions**  | Separates prompt logic from business logic, improving code readability and maintainability.                  |\n",
    "| **5. Format Agnostic**       | Works well with multi-line templates, few-shot examples, and advanced formatting ‚Äî better than raw strings.  |\n",
    "| **6. Debugging and Logging** | Easier to log, visualize, and test templates independently when debugging prompt engineering workflows.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10a520",
   "metadata": {},
   "source": [
    "### ***Loading Prompt Templet form `template.json`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1eca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "st.title(\"AI Research Paper Summarizer\")\n",
    "st.subheader(\"Generate tailored summaries of foundational AI research papers\")\n",
    "\n",
    "# ---------- User Input Controls ----------\n",
    "paper_input = st.selectbox(\n",
    "    \"Select Research Paper\",\n",
    "    [\n",
    "        \"Attention Is All You Need\",\n",
    "        \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"GPT-3: Language Models are Few-Shot Learners\",\n",
    "        \"Diffusion Models Beat GANs on Image Synthesis\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select Explanation Style\",\n",
    "    [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    ")\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select Explanation Length\",\n",
    "    [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n",
    ")\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# ‚≠êLoading Prompt Template from JSON‚≠ê\n",
    "# =====================================\n",
    "template = load_prompt(\"04_prompts/template.json\")\n",
    "\n",
    "\n",
    "# Generate Prompt from Inputs\n",
    "prompt = template.invoke({\n",
    "    'paper_input': paper_input,\n",
    "    'style_input': style_input,\n",
    "    'length_input': length_input\n",
    "})\n",
    "\n",
    "# Generate and Display Output\n",
    "if st.button('Summarize'):\n",
    "    result = model.invoke(prompt)\n",
    "    st.subheader(\"Summary\")\n",
    "\n",
    "    st.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ee63c",
   "metadata": {},
   "source": [
    "### ***Chain***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0630759",
   "metadata": {},
   "source": [
    "Rather than designing the prompt separately and then passing it to the model, we streamline the process by constructing a chain directly.\n",
    "\n",
    "**Instead of this 2-step process:**\n",
    "\n",
    "```python\n",
    "prompt = template.invoke({...})\n",
    "result = model.invoke(prompt)\n",
    "```\n",
    "we can do:\n",
    "\n",
    "```python\n",
    "chain = template | model\n",
    "result = chain.invoke({...})\n",
    "```\n",
    "\n",
    "**This approach:**\n",
    "- Is cleaner and more modular\n",
    "- Allows better reusability\n",
    "- Scales well if you later add things like \n",
    "  - output `parsers`\n",
    "  - `retrievers`\n",
    "  - `memory` components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "st.title(\"AI Research Paper Summarizer\")\n",
    "st.subheader(\"Generate tailored summaries of foundational AI research papers\")\n",
    "\n",
    "# ---------- User Input Controls ----------\n",
    "paper_input = st.selectbox(\n",
    "    \"Select Research Paper\",\n",
    "    [\n",
    "        \"Attention Is All You Need\",\n",
    "        \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"GPT-3: Language Models are Few-Shot Learners\",\n",
    "        \"Diffusion Models Beat GANs on Image Synthesis\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select Explanation Style\",\n",
    "    [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    ")\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select Explanation Length\",\n",
    "    [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# ‚≠êLoading Prompt Template from JSON‚≠ê\n",
    "# =====================================\n",
    "template = load_prompt(\"04_prompts/template.json\")\n",
    "\n",
    "\n",
    "\n",
    "if st.button('Summarize'):\n",
    "    chain = template | model\n",
    "    result = chain.invoke({\n",
    "        'paper_input':paper_input,\n",
    "        'style_input':style_input,\n",
    "        'length_input':length_input\n",
    "    })\n",
    "    st.write(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI-with-Langchain-and-Huggingface (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
