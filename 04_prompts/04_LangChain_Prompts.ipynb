{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db15d3b6",
   "metadata": {},
   "source": [
    "# üó®Ô∏è**Prompts in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20e7ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "![author](https://img.shields.io/badge/author-mohd--faizy-red)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2b71",
   "metadata": {},
   "source": [
    "üî•**What is Temperature in LLMs?**\n",
    "\n",
    "* **Temperature** controls how *random or predictable* a language model‚Äôs output is.\n",
    "* It affects how the model picks the *next word* in a sentence.\n",
    "\n",
    "\n",
    "üßä**Low Temperature (Closer to 0)**\n",
    "\n",
    "* Makes the output **more focused and predictable**.\n",
    "* The model chooses **high-probability words** more often.\n",
    "* Good for **factual tasks** (like answering questions or coding).\n",
    "* Example: If \"blue\" has 80% chance and \"green\" 20%, a low temp might choose \"blue\" 99% of the time.\n",
    "\n",
    "\n",
    "üî•**High Temperature (Above 1)**\n",
    "\n",
    "* Makes the output **more creative and random**.\n",
    "* The model is more likely to try **less common words**.\n",
    "* Great for **brainstorming or creative writing**.\n",
    "* In the same example, \"green\" might now be picked 40% of the time.\n",
    "\n",
    "\n",
    "‚öôÔ∏è**How It Works (Simplified)**\n",
    "\n",
    "* The model gives *scores* (called **logits**) to each possible next word.\n",
    "* Temperature scales these scores **before** turning them into probabilities:\n",
    "\n",
    "  * `Temp = 1`: no change to scores.\n",
    "  * `Temp < 1`: increases the gap between scores ‚Üí more confident choices.\n",
    "  * `Temp > 1`: narrows the gap ‚Üí more variety in output.\n",
    "\n",
    "\n",
    "üéØ**When to Use What?**\n",
    "\n",
    "| Use Case            | Recommended Temperature |\n",
    "| ------------------- | ----------------------- |\n",
    "| Technical support   | 0.2 ‚Äì 0.5 (focused)     |\n",
    "| Code generation     | 0.2 ‚Äì 0.4 (precise)     |\n",
    "| Creative writing    | 0.7 ‚Äì 1.2 (diverse)     |\n",
    "| Casual conversation | 0.6 ‚Äì 0.9 (engaging)    |\n",
    "\n",
    "\n",
    "‚ö†Ô∏è**Things to Watch Out For**\n",
    "\n",
    "* **Too low** (e.g. 0): Model becomes repetitive or boring.\n",
    "* **Too high** (e.g. 2): Responses can become weird or incoherent.\n",
    "* **Balance is key** ‚Äì Experiment to find the sweet spot for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In realms of code and digital space,\n",
      "Langchain weaves a cognitive pace,\n",
      "With chains of thought and links so fine,\n",
      "It crafts responses that are truly divine,\n",
      "A language model of unparalleled pace.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Code using Groq (Mixtral / LLaMA via Langchain)\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  \n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "result = model.invoke(\"Write a 5 line poem on Langchain.\")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f54602",
   "metadata": {},
   "source": [
    "## üó®Ô∏è**Prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d47629",
   "metadata": {},
   "source": [
    ">`Prompts` are the input instructions or queries given to a model to guide its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47db52",
   "metadata": {},
   "source": [
    "### ***Types of Prompts?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871ce81",
   "metadata": {},
   "source": [
    "**Static vs. Dynamic Prompts** \n",
    "\n",
    "| **Aspect**                | **Static Prompts**                                                                      | **Dynamic Prompts**                                                                                                    |\n",
    "| ------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Definition**            | Hard-coded prompts written by the user or developer without structure or placeholders   | Template-based prompts with placeholders filled in at runtime                                                          |\n",
    "| **Control**               | Full control is given to the user; the system has minimal control over prompt structure | Programmer controls the structure, limiting variability and improving consistency                                      |\n",
    "| **Risk of Hallucination** | Higher ‚Äì due to vague or incorrect user input                                           | Lower ‚Äì prompt templates guide the LLM, reducing ambiguity                                                             |\n",
    "| **Consistency**           | Low ‚Äì depends on user input, which varies widely                                        | High ‚Äì ensures uniform tone, format, and style of outputs                                                              |\n",
    "| **Reusability**           | Low ‚Äì specific to each use case, not generalizable                                      | High ‚Äì templates can be reused across different tasks or inputs                                                        |\n",
    "| **User Input Example**    | `\"Summarize 'Attention Is All You Need' in simple terms\"`                               | Template: `\"Summarize the paper titled {paper_input} in a {explanation_style} style and {explanation_length} length.\"` |\n",
    "| **Best Use Case**         | Quick experiments or exploratory tasks                                                  | Production-grade apps, tools requiring consistent outputs, or when guiding LLM behavior is important                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ecb59",
   "metadata": {},
   "source": [
    "# **‚≠ïResearch Assistant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Import Dependencies\n",
    "# ============================\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ============================\n",
    "# Load Environment Variables\n",
    "# ============================\n",
    "load_dotenv()\n",
    "\n",
    "# ============================\n",
    "# Initialize the LLM Model\n",
    "# ============================\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Streamlit UI Setup\n",
    "# ============================\n",
    "st.title(\"AI Research Paper Summarizer\")\n",
    "st.subheader(\"Generate tailored summaries of foundational AI research papers\")\n",
    "\n",
    "# ---------- User Input Controls ----------\n",
    "paper_input = st.selectbox(\n",
    "    \"Select Research Paper\",\n",
    "    [\n",
    "        \"Attention Is All You Need (Vaswani et al., 2017)\",\n",
    "        \"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\",\n",
    "        \"GPT-3: Language Models are Few-Shot Learners (Brown et al., 2020)\",\n",
    "        \"Diffusion Models Beat GANs on Image Synthesis (Dhariwal & Nichol, 2021)\",\n",
    "        \"AlexNet: ImageNet Classification with Deep CNNs (Krizhevsky et al., 2012)\",\n",
    "        \"ResNet: Deep Residual Learning for Image Recognition (He et al., 2015)\",\n",
    "        \"GANs: Generative Adversarial Nets (Goodfellow et al., 2014)\",\n",
    "        \"Word2Vec: Efficient Estimation of Word Representations (Mikolov et al., 2013)\",\n",
    "        \"Transformers in Vision: An Image is Worth 16x16 Words (Dosovitskiy et al., 2020, ViT)\",\n",
    "        \"DQN: Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)\",\n",
    "        \"AlphaGo: Mastering the Game of Go with Deep Neural Networks (Silver et al., 2016)\",\n",
    "        \"CLIP: Learning Transferable Visual Models from Natural Language Supervision (Radford et al., 2021)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select Explanation Style\",\n",
    "    [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    ")\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select Explanation Length\",\n",
    "    [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Prompt Template Definition\n",
    "# ============================\n",
    "template = PromptTemplate(\n",
    "    input_variables=['paper_input', 'style_input', 'length_input'],\n",
    "    template=\"\"\"\n",
    "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "Explanation Style: {style_input}  \n",
    "Explanation Length: {length_input}  \n",
    "\n",
    "1. Mathematical Details:\n",
    "   - Include relevant mathematical equations if present in the paper.\n",
    "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
    "2. Analogies:\n",
    "   - Use relatable analogies to simplify complex ideas.\n",
    "\n",
    "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
    "Ensure the summary is clear, accurate, and aligned with the selected style and length.\n",
    "\"\"\",\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Generate Prompt from Inputs\n",
    "# ============================\n",
    "prompt = template.invoke({\n",
    "    'paper_input': paper_input,\n",
    "    'style_input': style_input,\n",
    "    'length_input': length_input\n",
    "})\n",
    "\n",
    "# ============================\n",
    "# Generate and Display Output\n",
    "# ============================\n",
    "if st.button('Summarize'):\n",
    "    result = model.invoke(prompt)\n",
    "    st.subheader(\"Summary\")\n",
    "    st.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443d3cf",
   "metadata": {},
   "source": [
    "**When `validate_template=True` is set:**\n",
    "\n",
    "- LangChain checks that all placeholders in your template (e.g., {paper_input}, {style_input}) are:\n",
    "- Properly provided in the `input_variables` list.\n",
    "- Referenced correctly in the actual text of the template.\n",
    "\n",
    "If there‚Äôs a mismatch (e.g., missing a variable or typo like {paper_inpt} instead of {paper_input}), it will raise an error immediately, helping you catch bugs early.\n",
    "\n",
    "```python\n",
    "template = PromptTemplate(\n",
    "    input_variables=['topic'],\n",
    "    template=\"Summarize the topic: {topik}\",  # typo here\n",
    "    validate_template=True\n",
    ")\n",
    "```\n",
    "This will raise an error like:\n",
    "\n",
    "```text\n",
    "ValueError: Found a placeholder in the template that is not in input_variables: 'topik'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb83da0",
   "metadata": {},
   "source": [
    "### ***Why `PromptTemplate`?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74306357",
   "metadata": {},
   "source": [
    "‚≠ê`generator.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ================================\n",
    "# Define the prompt template\n",
    "# ================================\n",
    "template = PromptTemplate(\n",
    "    input_variables=['paper_input', 'style_input', 'length_input'],\n",
    "    template=\"\"\"\n",
    "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "Explanation Style: {style_input}  \n",
    "Explanation Length: {length_input}  \n",
    "\n",
    "1. Mathematical Details:  \n",
    "   - Include relevant mathematical equations if present in the paper.  \n",
    "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \n",
    "\n",
    "2. Analogies:  \n",
    "   - Use relatable analogies to simplify complex ideas.  \n",
    "\n",
    "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.  \n",
    "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
    "\"\"\",\n",
    "    validate_template=True  # Ensures template variables match input_variables\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Save the prompt template to disk\n",
    "# ================================\n",
    "template.save('template.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadde8cc",
   "metadata": {},
   "source": [
    "A **PromptTemplate** in LangChain is a structured way to create prompts dynamically by inserting variables into a predefined template. Instead of hardcoding prompts, PromptTemplate allows you to define placeholders that can be filled in at runtime with different inputs. This makes it **reusable**, **flexible**, and **easy to manage**, especially when working with dynamic user inputs or automated workflows.\n",
    "\n",
    "**Why use PromptTemplate over `f-strings`?**\n",
    "\n",
    "| **Reason**                   | **Explanation**                                                                                              |\n",
    "| ---------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **1. Default Validation**    | `PromptTemplate` checks if all required variables are provided, avoiding runtime errors from missing values. |\n",
    "| **2. Reusable**              | Once defined, a prompt can be reused with different inputs across workflows or chains.                       |\n",
    "| **3. LangChain Ecosystem**   | Seamlessly integrates with LangChain components like chains, memory, and tools ‚Äî enabling composability.     |\n",
    "| **4. Cleaner Abstractions**  | Separates prompt logic from business logic, improving code readability and maintainability.                  |\n",
    "| **5. Format Agnostic**       | Works well with multi-line templates, few-shot examples, and advanced formatting ‚Äî better than raw strings.  |\n",
    "| **6. Debugging and Logging** | Easier to log, visualize, and test templates independently when debugging prompt engineering workflows.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ee63c",
   "metadata": {},
   "source": [
    "## **Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10a520",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è ***Loading Prompt Templet form `template.json`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d243d4",
   "metadata": {},
   "source": [
    "‚≠ê`app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Import Dependencies\n",
    "# ============================\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "# ============================\n",
    "# Load Environment Variables\n",
    "# ============================\n",
    "load_dotenv()\n",
    "\n",
    "# ============================\n",
    "# Initialize the LLM Model\n",
    "# ============================\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    # temperature=0.7,\n",
    "    # max_tokens=100\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Streamlit UI Setup\n",
    "# ============================\n",
    "st.title(\"AI Research Paper Summarizer\")\n",
    "st.subheader(\"Generate tailored summaries of foundational AI research papers\")\n",
    "\n",
    "# ---------- User Input Controls ----------\n",
    "paper_input = st.selectbox(\n",
    "    \"Select Research Paper\",\n",
    "    [\n",
    "        \"Attention Is All You Need (Vaswani et al., 2017)\",\n",
    "        \"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\",\n",
    "        \"GPT-3: Language Models are Few-Shot Learners (Brown et al., 2020)\",\n",
    "        \"Diffusion Models Beat GANs on Image Synthesis (Dhariwal & Nichol, 2021)\",\n",
    "        \"AlexNet: ImageNet Classification with Deep CNNs (Krizhevsky et al., 2012)\",\n",
    "        \"ResNet: Deep Residual Learning for Image Recognition (He et al., 2015)\",\n",
    "        \"GANs: Generative Adversarial Nets (Goodfellow et al., 2014)\",\n",
    "        \"Word2Vec: Efficient Estimation of Word Representations (Mikolov et al., 2013)\",\n",
    "        \"Transformers in Vision: An Image is Worth 16x16 Words (Dosovitskiy et al., 2020, ViT)\",\n",
    "        \"DQN: Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)\",\n",
    "        \"AlphaGo: Mastering the Game of Go with Deep Neural Networks (Silver et al., 2016)\",\n",
    "        \"CLIP: Learning Transferable Visual Models from Natural Language Supervision (Radford et al., 2021)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select Explanation Style\",\n",
    "    [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"]\n",
    ")\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select Explanation Length\",\n",
    "    [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"]\n",
    ")\n",
    "\n",
    "# =================================\n",
    "# Loading Prompt Template from JSON\n",
    "# =================================\n",
    "template = load_prompt(\"04_prompts/Research_Assistant/template.json\")\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Generating Prompt from USER INPUT and then Sending to Model using Chain\n",
    "# =======================================================================\n",
    "if st.button('Summarize'):\n",
    "    chain = template | model          # Create a chain from the template and model\n",
    "    result = chain.invoke({           # Invoke the chain with user inputs\n",
    "        'paper_input':paper_input,\n",
    "        'style_input':style_input,\n",
    "        'length_input':length_input\n",
    "    })\n",
    "    st.subheader(\"Summary\")\n",
    "    st.write(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0630759",
   "metadata": {},
   "source": [
    "Rather than designing the prompt separately and then passing it to the model, we streamline the process by constructing a chain directly.\n",
    "\n",
    "**Instead of this 2-step process:**\n",
    "\n",
    "```python\n",
    "prompt = template.invoke({...})\n",
    "result = model.invoke(prompt)\n",
    "```\n",
    "we can do:\n",
    "\n",
    "```python\n",
    "chain = template | model\n",
    "result = chain.invoke({...})\n",
    "```\n",
    "\n",
    "**This approach:**\n",
    "- Is cleaner and more modular\n",
    "- Allows better reusability\n",
    "- Scales well if you later add things like \n",
    "  - output `parsers`\n",
    "  - `retrievers`\n",
    "  - `memory` components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f986e0",
   "metadata": {},
   "source": [
    "# **‚≠ïChatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcb670",
   "metadata": {},
   "source": [
    "### **Prompt Templet**\n",
    "\n",
    "```markdown\n",
    "Model\n",
    "|\n",
    "‚îî‚îÄ‚îÄ> `model.invoke()`\n",
    "            ‚îú‚îÄ‚îÄ> Single Message (single-turn standalone queries)\n",
    "            ‚îÇ         ‚îú‚îÄ‚îÄ> üî¥Static Message\n",
    "            ‚îÇ         ‚îî‚îÄ‚îÄ> üü¢Dynamic Message (`PromptTemplate`)\n",
    "            ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ> List of Messages (multi-turn conversation)\n",
    "                    ‚îú‚îÄ‚îÄ> üî¥Static Message\n",
    "                    ‚îÇ          ‚îú‚îÄ‚îÄ> `SystemMessage`\n",
    "                    ‚îÇ          ‚îú‚îÄ‚îÄ> `HumanMessage`\n",
    "                    ‚îÇ          ‚îî‚îÄ‚îÄ> `AIMessage`\n",
    "                    ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ> üü¢Dynamic Message (`ChatPromptTemplate`)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcbf1e",
   "metadata": {},
   "source": [
    "### üó®Ô∏è**Types of Messages**\n",
    "\n",
    "1. **`SystemMessage`**\n",
    "\n",
    "   * Provides **instructions, rules, or context** to the AI.\n",
    "   * Usually sets behavior or role of the model.\n",
    "   * Example:\n",
    "\n",
    "     ```python\n",
    "     SystemMessage(content=\"You are a helpful assistant that explains in simple terms.\")\n",
    "     ```\n",
    "\n",
    "2. **`HumanMessage`**\n",
    "\n",
    "   * Represents the **user‚Äôs input** (what the person asks or says).\n",
    "   * Example:\n",
    "\n",
    "     ```python\n",
    "     HumanMessage(content=\"Explain quantum computing in simple words.\")\n",
    "     ```\n",
    "\n",
    "3. **`AIMessage`**\n",
    "\n",
    "   * Represents the **AI‚Äôs response** back to the user.\n",
    "   * Example:\n",
    "\n",
    "     ```python\n",
    "     AIMessage(content=\"Quantum computing is like using special rules of physics to solve problems faster.\")\n",
    "     ```\n",
    "\n",
    "\n",
    "**Example Conversation with Messages:**\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a math tutor.\"),\n",
    "    HumanMessage(content=\"What is 2 + 2?\"),\n",
    "    AIMessage(content=\"The answer is 4.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cf4e8",
   "metadata": {},
   "source": [
    "**Static**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`messages.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11e8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "AI:  The number 2 is greater than 0. \n",
      "\n",
      "So, the answer is: 2 is greater than 0.\n",
      "AI:  The greater number is 2. Multiplying 2 by 10 gives:\n",
      "\n",
      "2 √ó 10 = 20\n",
      "\n",
      "So, the result is 20.\n",
      "[SystemMessage(content='You are an helpful AI Assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='which one is greater 0 or 2', additional_kwargs={}, response_metadata={}), AIMessage(content='The number 2 is greater than 0. \\n\\nSo, the answer is: 2 is greater than 0.', additional_kwargs={}, response_metadata={}), HumanMessage(content='multiply the greater number with 10', additional_kwargs={}, response_metadata={}), AIMessage(content='The greater number is 2. Multiplying 2 by 10 gives:\\n\\n2 √ó 10 = 20\\n\\nSo, the result is 20.', additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    # temperature=0.7,\n",
    "    # max_tokens=100\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"You are an helpful AI Assistant!\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    print(\"AI: \",response.content)\n",
    "\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20653a8e",
   "metadata": {},
   "source": [
    "**Dynamic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf508d8d",
   "metadata": {},
   "source": [
    "`chat_prompt_templet.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72ccab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is transformers', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'AI','topic':'transformers'})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a9167",
   "metadata": {},
   "source": [
    "**`MessagePlaceholder`**\n",
    "\n",
    "> A Message Placeholder in LangChain is a special placeholder used inside a `ChatPromptTemplet` to dynamically insert chat history or a list of messages at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11399c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import json\n",
    "\n",
    "# chat template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful customer support agent'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{query}')\n",
    "])\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# load chat history Text\n",
    "# with open('chat_history.txt') as f:\n",
    "#     chat_history.extend(f.readlines())\n",
    "\n",
    "# load chat history JSON\n",
    "with open('chat_history.json', 'r') as f:\n",
    "    chat_history = json.load(f)\n",
    "\n",
    "print(chat_history)\n",
    "\n",
    "# create prompt\n",
    "prompt = chat_template.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'query': 'Where is my refund?'\n",
    "})\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI-with-Langchain-and-Huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
