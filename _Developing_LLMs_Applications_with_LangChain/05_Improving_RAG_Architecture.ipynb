{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14f8f92",
   "metadata": {},
   "source": [
    "# **üî∑üî∑Improving the RAG Architectureüî∑üî∑**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a1f11",
   "metadata": {},
   "source": [
    "Discover state-of-the-art techniques for loading, splitting, and retrieving documents, including loading Python files, splitting semantically, and using MRR and self-query retrieval methods. Learn to evaluate your RAG architecture using robust metrics and frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11d858",
   "metadata": {},
   "source": [
    "![img_1](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0501.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319202c0",
   "metadata": {},
   "source": [
    "## **‚≠ê01: Loading and Splitting code files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728f428",
   "metadata": {},
   "source": [
    "This is useful for integrating codebases into RAG systems‚Äîfor tasks like code `summarization`, `documentation generation`, or `code assistance`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ea4d2",
   "metadata": {},
   "source": [
    "### **‚≠ïLoading Markdown Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e2d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI with Langchain and Huggingface ü§ó\n",
      "\n",
      "This repository serves as a comprehensive guide for integrating Langchain with Huggingface models, enabling you to build, deploy, and optimize cutting-edge AI applications through hands-on projects and real-world examples.\n",
      "\n",
      "GenAI Overview\n",
      "\n",
      "Overview of Generative AI Pipeline\n",
      "\n",
      "author\n",
      "\n",
      "Python 3.9+\n",
      "\n",
      "Streamlit\n",
      "\n",
      "Ollama\n",
      "\n",
      "LangChain\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "License: MIT\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "GenAI with Langchain and Huggingface ü§ó\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Overview\n",
      "\n",
      "Key Features\n",
      "\n",
      "Types of Generative AI\n",
      "\n",
      "Supported Model Types\n",
      "\n",
      "‚≠êBuilder's Perspective\n",
      "\n",
      "1. Foundation Model Architecture\n",
      "\n",
      "2. Model Training Pipeline\n",
      "\n",
      "3. Data Processing\n",
      "\n",
      "4. Model Architecture\n",
      "\n",
      "5. Training Infrastructure\n",
      "\n",
      "6. Deployment Strategy\n",
      "\n",
      "‚≠êUser's Perspective\n",
      "\n",
      "1. Interface Design\n",
      "\n",
      "2. User Interaction\n",
      "\n",
      "3. Response Generation\n",
      "\n",
      "4. System Integration\n",
      "\n",
      "5. Performance Metrics\n",
      "\n",
      "Installation\n",
      "\n",
      "Getting Started\n",
      "\n",
      "Examples\n",
      "\n",
      "Contributing\n",
      "\n",
      "‚öñ ‚û§ License\n",
      "\n",
      "‚ù§Ô∏è Support\n",
      "\n",
      "ü™ôCredits and Inspiration\n",
      "\n",
      "üîóConnect with me\n",
      "\n",
      "Overview\n",
      "\n",
      "This repository demonstrates the power of combining Langchain's composability with Huggingface's state-of-the-art models. We provide comprehensive examples and implementations for various Generative AI applications, from text generation to multimodal systems.\n",
      "\n",
      "Key Features\n",
      "\n",
      "üîó Langchain Integration: Seamless integration with Langchain's powerful components\n",
      "\n",
      "ü§ó Huggingface Models: Access to cutting-edge pre-trained models\n",
      "\n",
      "üìö Comprehensive Examples: Detailed notebooks and use cases\n",
      "\n",
      "üõ†Ô∏è Production-Ready Code: Optimized implementations for real-world applications\n",
      "\n",
      "üìà Performance Metrics: Detailed analysis and benchmarking tools\n",
      "\n",
      "Types of Generative AI\n",
      "\n",
      "Types of Generative AI\n",
      "\n",
      "Different Types of Generative AI Models and Their Applications\n",
      "\n",
      "Supported Model Types\n",
      "\n",
      "Text Generation Models\n",
      "\n",
      "GPT-based models\n",
      "\n",
      "T5 variants\n",
      "\n",
      "BERT derivatives\n",
      "\n",
      "Image Generation\n",
      "\n",
      "Stable Diffusion\n",
      "\n",
      "DALL-E integration\n",
      "\n",
      "Midjourney-like implementations\n",
      "\n",
      "Audio Processing\n",
      "\n",
      "Speech-to-Text\n",
      "\n",
      "Text-to-Speech\n",
      "\n",
      "Audio Generation\n",
      "\n",
      "‚≠êBuilder's Perspective\n",
      "\n",
      "1. Foundation Model Architecture\n",
      "\n",
      "Foundation Model\n",
      "\n",
      "2. Model Training Pipeline\n",
      "\n",
      "Training Pipeline\n",
      "\n",
      "3. Data Processing\n",
      "\n",
      "Data Processing\n",
      "\n",
      "4. Model Architecture\n",
      "\n",
      "Model Architecture\n",
      "\n",
      "5. Training Infrastructure\n",
      "\n",
      "Training Infrastructure\n",
      "\n",
      "6. Deployment Strategy\n",
      "\n",
      "Deployment Strategy\n",
      "\n",
      "‚≠êUser's Perspective\n",
      "\n",
      "1. Interface Design\n",
      "\n",
      "Interface Design\n",
      "\n",
      "2. User Interaction\n",
      "\n",
      "User Interaction\n",
      "\n",
      "3. Response Generation\n",
      "\n",
      "Response Generation\n",
      "\n",
      "4. System Integration\n",
      "\n",
      "System Integration\n",
      "\n",
      "5. Performance Metrics\n",
      "\n",
      "Performance Metrics\n",
      "\n",
      "Installation\n",
      "\n",
      "```bash\n",
      "\n",
      "Clone the repository\n",
      "\n",
      "git clone https://github.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface.git\n",
      "\n",
      "Navigate to the project directory\n",
      "\n",
      "cd GenAI-with-Langchain-and-Huggingface\n",
      "\n",
      "Create and activate virtual environment\n",
      "\n",
      "python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate\n",
      "\n",
      "Install required packages\n",
      "\n",
      "pip install -r requirements.txt\n",
      "\n",
      "```\n",
      "\n",
      "Getting Started\n",
      "\n",
      "```python from langchain import LLMChain from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "Initialize model and tokenizer\n",
      "\n",
      "model_name = \"gpt2\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name)\n",
      "\n",
      "Create a simple chain\n",
      "\n",
      "chain = LLMChain( llm=model, prompt_template=\"Write a story about {topic}\" )\n",
      "\n",
      "Run the chain\n",
      "\n",
      "result = chain.run(topic=\"space exploration\") print(result) ```\n",
      "\n",
      "Examples\n",
      "\n",
      "Check our examples directory for complete implementations: - Text Generation Pipeline - Image Generation with Stable Diffusion - Question Answering Systems - Document Analysis - Chatbot Implementation\n",
      "\n",
      "Contributing\n",
      "\n",
      "Fork the repository\n",
      "\n",
      "Create your feature branch (git checkout -b feature/AmazingFeature)\n",
      "\n",
      "Commit your changes (git commit -m 'Add some AmazingFeature')\n",
      "\n",
      "Push to the branch (git push origin feature/AmazingFeature)\n",
      "\n",
      "Open a Pull Request\n",
      "\n",
      "‚öñ ‚û§ License\n",
      "\n",
      "This project is licensed under the MIT License. See the LICENSE file for details.\n",
      "\n",
      "‚ù§Ô∏è Support\n",
      "\n",
      "If you find this repository helpful, show your support by starring it! For questions or feedback, reach out on Twitter(X).\n",
      "\n",
      "ü™ôCredits and Inspiration\n",
      "\n",
      "This repository is inspired by the excellent course content created by Nitish on the CampusX YouTube channel & DataCamps Course Developing LLMs with LangChain. The implementation and examples in this repository are based on his comprehensive tutorials on Generative AI with Langchain and Huggingface.\n",
      "\n",
      "üîóConnect with me\n",
      "\n",
      "‚û§ If you have questions or feedback, feel free to reach out!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\README.md'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\README.md\"\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(file_path=PATH)\n",
    "markdown_content = loader.load()\n",
    "\n",
    "print(markdown_content[0].page_content)  # Print the content of the first document\n",
    "print(markdown_content[0].metadata)      # Print the metadata of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058ded6",
   "metadata": {},
   "source": [
    "### **‚≠ïLoading Python Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61660089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='from abc import ABC, abstractmethod\n",
      "\n",
      "# Abstract base class for all LLMs\n",
      "class LLM(ABC):\n",
      "    @abstractmethod\n",
      "    def complete_sentence(self, prompt):\n",
      "        pass\n",
      "\n",
      "# Concrete implementations of LLM\n",
      "class OpenAI(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... OpenAI end of sentence.\"\n",
      "\n",
      "class Anthropic(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... Anthropic end of sentence.\"\n",
      "\n",
      "class GooglePaLM(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... Google PaLM end of sentence.\"\n",
      "\n",
      "class Cohere(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... Cohere end of sentence.\"\n",
      "\n",
      "class Mistral(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... Mistral end of sentence.\"\n",
      "\n",
      "class CustomLLM(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        suffix = \" ... CustomLLM generated response.\"\n",
      "        return prompt + suffix\n",
      "\n",
      "# Test function to run all LLMs\n",
      "def test_llms():\n",
      "    prompt = \"The future of AI is\"\n",
      "    llms = [OpenAI(), Anthropic(), GooglePaLM(), Cohere(), Mistral(), CustomLLM()]\n",
      "    for llm in llms:\n",
      "        print(f\"{llm.__class__.__name__}: {llm.complete_sentence(prompt)}\")\n",
      "\n",
      "# Only run if script is executed directly\n",
      "if __name__ == \"__main__\":\n",
      "    test_llms()\n",
      "' metadata={'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\pyfile.py'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\pyfile.py\"\n",
    "\n",
    "loader = PythonLoader(file_path=PATH)\n",
    "python_data = loader.load()\n",
    "\n",
    "print(python_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b749624",
   "metadata": {},
   "source": [
    "### **‚≠ïSplitting Code Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aead694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "Chunk 2:\n",
      "# Abstract base class for all LLMs\n",
      "class LLM(ABC):\n",
      "    @abstractmethod\n",
      "    def complete_sentence(self, prompt):\n",
      "        pass\n",
      "\n",
      "Chunk 3:\n",
      "# Concrete implementations of LLM\n",
      "class OpenAI(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... OpenAI end of sentence.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    "    )\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5cb61",
   "metadata": {},
   "source": [
    "### **‚≠ïLanguage-Specific Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc9a26",
   "metadata": {},
   "source": [
    "- Instead of naive splitting, LangChain can split code using language-aware separators like:\n",
    "\n",
    "  - `\\nclass`, `\\ndef` , `\\n\\tdef` \n",
    "\n",
    "- This ensures that each chunk is a logical code unit‚Äîsuch as an entire function or class‚Äîrather than arbitrary lines.\n",
    "\n",
    "- Especially beneficial for code analysis or generation, as it maintains semantic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11cc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "# Abstract base class for all LLMs\n",
      "\n",
      "Chunk 2:\n",
      "class LLM(ABC):\n",
      "    @abstractmethod\n",
      "    def complete_sentence(self, prompt):\n",
      "        pass\n",
      "\n",
      "# Concrete implementations of LLM\n",
      "\n",
      "Chunk 3:\n",
      "class OpenAI(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ... OpenAI end of sentence.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdffdb",
   "metadata": {},
   "source": [
    "## **‚≠ê02:Advanced Splitting Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b7e87",
   "metadata": {},
   "source": [
    "![img_2](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0502.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a87ff",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Limitations of Basic Splitting**\n",
    "\n",
    "- **Lack of Context Awareness:** Simple character-based splitting might break a function or paragraph in unnatural places, reducing model performance.\n",
    "\n",
    "- **Mismatch with Model Processing:** Since LLMs process tokens, character limits may not align with model capabilities, leading to token overflow or inefficient use of input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1d551",
   "metadata": {},
   "source": [
    "### **‚≠ïToken-Based Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1d162",
   "metadata": {},
   "source": [
    "- Splits are calculated by token count, which aligns with how LLMs consume input.\n",
    "- This ensures each chunk fits within the model‚Äôs token limit and avoids truncation.\n",
    "- Prevents loss of meaning due to mid-token splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "No. tokens: 10\n",
      "Mary had a little lamb, it's fleece was white\n",
      "\n",
      "Chunk 2:\n",
      "No. tokens: 5\n",
      " was white as snow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "# Get encoding for model\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "# Initialize the TokenTextSplitter\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=encoding.name,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = splitter.split_text(example_string)\n",
    "\n",
    "# Count tokens in each chunk and print them\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = len(encoding.encode(chunk))\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {token_count}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1efefb",
   "metadata": {},
   "source": [
    "`cl100k_base` is the tokenizer encoding used for models like:\n",
    "\n",
    "- gpt-4\n",
    "- gpt-4-32k\n",
    "- gpt-3.5-turbo\n",
    "- gpt-3.5-turbo-16k\n",
    "- and now also used as a fallback when a model like gpt-4o-mini isn't directly supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "No. tokens: 13\n",
      "Mary had a little lamb, its fleece was white as snow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "example_string = \"Mary had a little lamb, its fleece was white as snow.\"\n",
    "\n",
    "# Get encoding for the model\n",
    "# Use the 'cl100k_base' encoding for GPT-3.5 and GPT-4 models\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Set up token-based text splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    encoding_name=encoding.name,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Wrap the string in a Document object and split into chunks\n",
    "documents = [Document(page_content=example_string)]\n",
    "chunks = token_splitter.split_documents(documents)\n",
    "\n",
    "# Display the token count in each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {len(encoding.encode(chunk.page_content))}\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda16844",
   "metadata": {},
   "source": [
    "### **‚≠ïSemantic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eade434",
   "metadata": {},
   "source": [
    "- Uses embedding models to understand the content and split based on semantic boundaries (logical breakpoints in meaning).\n",
    "\n",
    "- Employs gradient thresholding to decide where one idea ends and another begins.\n",
    "\n",
    "- Produces coherent, context-rich chunks that enhance downstream task accuracy (like answering or summarizing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc1819",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_community.document_transformers import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Instantiate an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(api_key=\"<OPENAI_API_TOKEN>\", model='text-embedding-3-small')\n",
    "\n",
    "# Create the semantic text splitter with desired parameters\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embedding_model, breakpoint_threshold_type=\"gradient\", breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "print(chunks[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f49e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='from abc import ABC, abstractmethod\n",
      "\n",
      "# Abstract base class for all LLMs\n",
      "class LLM(ABC):\n",
      "    @abstractmethod\n",
      "    def complete_sentence(self, prompt):\n",
      "        pass\n",
      "\n",
      "# Concrete implementations of LLM\n",
      "class OpenAI(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...' metadata={'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\pyfile.py'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI  \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Google embedding model used to convert text into high-dimensional vectors\n",
    "# This model helps in understanding the meaning of text for semantic processing\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create an instance of SemanticChunker to split text based on semantic changes (meaningful segments)\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,                         # Pass the embedding model\n",
    "    breakpoint_threshold_type=\"gradient\",          # Method to detect split points based on semantic gradient\n",
    "    breakpoint_threshold_amount=0.8                # Sensitivity of chunk splitting (higher = fewer splits)\n",
    ")\n",
    "\n",
    "# Split the input documents into semantically coherent chunks\n",
    "chunks = semantic_splitter.split_documents(python_data)\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec763bcc",
   "metadata": {},
   "source": [
    "## **‚≠ê03: Optimizing document retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d32dd",
   "metadata": {},
   "source": [
    "### üîç **Dense vs. Sparse Retrieval in RAG Pipelines**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c849e",
   "metadata": {},
   "source": [
    "When building Retrieval-Augmented Generation (RAG) systems‚Äîlike those in LangChain‚Äîyou typically choose between **dense** and **sparse** retrieval methods.\n",
    "\n",
    "- üß™ **Dense Retrieval**\n",
    "  -  Uses neural networks (e.g., transformers) to encode documents and queries into **dense vectors**‚Äîcompact numerical representations that capture meaning.\n",
    "  -  Relevance is measured via **vector similarity** (like cosine similarity or dot product).\n",
    "  - `Pros.` Vs `Cons.`:\n",
    "     - **‚úÖ Pros:**\n",
    "       - Captures **semantic meaning**‚Äîgood with synonyms, paraphrasing, and abstract queries.\n",
    "       - Powerful for **open-domain** or fuzzy information retrieval.\n",
    "     - **‚ö†Ô∏è Cons:**\n",
    "       -  Requires **expensive training** and GPU-based inference.\n",
    "       -  Harder to **interpret** why a document was retrieved.\n",
    "\n",
    "\n",
    "-  üìö **Sparse Retrieval**\n",
    "   - Based on **keyword matching** using traditional IR methods.\n",
    "   - Works with **bag-of-words** models‚Äîeach word is treated separately and sparsely.\n",
    "   - **Common Techniques**:\n",
    "      -  **TF-IDF** (*Term Frequency‚ÄìInverse Document Frequency*):\n",
    "         - Measures how important a word is to a document.\n",
    "         - > If a term appears often in one document but rarely across others, it gets a higher score.\n",
    "      - **BM25** (*Best Matching 25*):\n",
    "        - An advanced ranking function in the Okapi family.\n",
    "        - > It refines TF-IDF by adjusting for **term frequency saturation** and **document length**.\n",
    "     - `Pros.` Vs `Cons.`:\n",
    "       - **‚úÖ Pros:**\n",
    "         -  **Fast**, resource-efficient, and easy to **interpret**.\n",
    "         -  Great for **rare terms** and exact keyword matches.\n",
    "       - **‚ö†Ô∏è Cons:**\n",
    "         -  Struggles with **synonyms** or **semantic similarity**.\n",
    "         -  Can miss documents that are relevant but use **different wording**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4beab",
   "metadata": {},
   "source": [
    "### üß† **TF-IDF vs. BM25: Quick Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea47dfc",
   "metadata": {},
   "source": [
    "| Feature           | TF-IDF                                 | BM25                                    |\n",
    "| ----------------- | -------------------------------------- | --------------------------------------- |\n",
    "| Scoring Basis     | Term frequency √ó inverse document freq | Improved term weighting with saturation |\n",
    "| Handles Long Docs | ‚ùå No                                   | ‚úÖ Yes                                   |\n",
    "| Customizable      | Limited                                | ‚úÖ Adjustable with `k1` and `b` params   |\n",
    "| Used In           | Classic search engines, baseline NLP   | Modern IR, LangChain RAG pipelines      |\n",
    "\n",
    "\n",
    "\n",
    "- üõ†Ô∏è **In LangChain Pipelines**\n",
    "  - **`BM25` is often preferred** over `TF-IDF` because it:\n",
    "    - Handles **longer documents** better.\n",
    "    - Reduces over-penalization for **repeated keywords**.\n",
    "    - Generally provides more **balanced scoring**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c39e79",
   "metadata": {},
   "source": [
    "## **‚≠ê04: Introduction to RAG evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd0e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64112413",
   "metadata": {},
   "source": [
    "# üß© ***Full code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5204d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "from abc import ABC, abstractmethod\n",
      "\n",
      "# Abstract base class for all LLMs\n",
      "class LLM(ABC):\n",
      "    @abstractmethod\n",
      "    def complete_sentence(self, prompt):\n",
      "        pass\n",
      "\n",
      "# Concrete implementations of LLM\n",
      "class OpenAI(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...\n",
      "------------------------------------------------------------\n",
      "Chunk 2:\n",
      "OpenAI end of sentence.\"\n",
      "\n",
      "class Anthropic(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...\n",
      "------------------------------------------------------------\n",
      "Chunk 3:\n",
      "Anthropic end of sentence.\"\n",
      "\n",
      "class GooglePaLM(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...\n",
      "------------------------------------------------------------\n",
      "Chunk 4:\n",
      "Google PaLM end of sentence.\"\n",
      "\n",
      "class Cohere(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...\n",
      "------------------------------------------------------------\n",
      "Chunk 5:\n",
      "Cohere end of sentence.\"\n",
      "\n",
      "class Mistral(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        return prompt + \" ...\n",
      "------------------------------------------------------------\n",
      "Chunk 6:\n",
      "Mistral end of sentence.\"\n",
      "\n",
      "class CustomLLM(LLM):\n",
      "    def complete_sentence(self, prompt):\n",
      "        suffix = \" ... CustomLLM generated response.\"\n",
      "        return prompt + suffix\n",
      "\n",
      "# Test function to run all LLMs\n",
      "def test_llms():\n",
      "    prompt = \"The future of AI is\"\n",
      "    llms = [OpenAI(), Anthropic(), GooglePaLM(), Cohere(), Mistral(), CustomLLM()]\n",
      "    for llm in llms:\n",
      "        print(f\"{llm.__class__.__name__}: {llm.complete_sentence(prompt)}\")\n",
      "\n",
      "# Only run if script is executed directly\n",
      "if __name__ == \"__main__\":\n",
      "    test_llms()\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Load environment variables (expects GOOGLE_API_KEY in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Step 2: Define the path to the Python file\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\pyfile.py\"\n",
    "\n",
    "# Step 3: Load the Python file as LangChain Documents\n",
    "loader = PythonLoader(file_path=PATH)\n",
    "python_data = loader.load()  # Returns a list of Document objects\n",
    "\n",
    "# Step 4: Initialize Google embedding model for semantic chunking\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Step 5: Create a SemanticChunker instance\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\",  # Use gradient-based breakpoints\n",
    "    breakpoint_threshold_amount=0.8        # Threshold for chunk separation\n",
    ")\n",
    "\n",
    "# Step 6: Perform semantic chunking on the loaded documents\n",
    "chunks = semantic_splitter.split_documents(python_data)\n",
    "\n",
    "# Step 7: Print out all chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n{'-'*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
