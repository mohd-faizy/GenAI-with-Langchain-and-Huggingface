{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14f8f92",
   "metadata": {},
   "source": [
    "# **üî∑üî∑Improving the RAG Architectureüî∑üî∑**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a1f11",
   "metadata": {},
   "source": [
    "Discover state-of-the-art techniques for loading, splitting, and retrieving documents, including loading Python files, splitting semantically, and using MRR and self-query retrieval methods. Learn to evaluate your RAG architecture using robust metrics and frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319202c0",
   "metadata": {},
   "source": [
    "## **‚≠ê01: Loading and Splitting code files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826125e",
   "metadata": {},
   "source": [
    "![img_1](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0501.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728f428",
   "metadata": {},
   "source": [
    "This is useful for integrating codebases into RAG systems‚Äîfor tasks like code `summarization`, `documentation generation`, or `code assistance`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ea4d2",
   "metadata": {},
   "source": [
    "### **‚≠ïLoading Markdown Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\README.md\"\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(file_path=PATH)\n",
    "markdown_content = loader.load()\n",
    "\n",
    "print(markdown_content[0].page_content)  # Print the content of the first document\n",
    "print(markdown_content[0].metadata)      # Print the metadata of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058ded6",
   "metadata": {},
   "source": [
    "### **‚≠ïLoading Python Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61660089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\pyfile.py\"\n",
    "\n",
    "loader = PythonLoader(file_path=PATH)\n",
    "python_data = loader.load()\n",
    "\n",
    "print(python_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b749624",
   "metadata": {},
   "source": [
    "### **‚≠ïSplitting Code Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aead694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    "    )\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5cb61",
   "metadata": {},
   "source": [
    "### **‚≠ïLanguage-Specific Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc9a26",
   "metadata": {},
   "source": [
    "- Instead of naive splitting, LangChain can split code using language-aware separators like:\n",
    "\n",
    "  - `\\nclass`, `\\ndef` , `\\n\\tdef` \n",
    "\n",
    "- This ensures that each chunk is a logical code unit‚Äîsuch as an entire function or class‚Äîrather than arbitrary lines.\n",
    "\n",
    "- Especially beneficial for code analysis or generation, as it maintains semantic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdffdb",
   "metadata": {},
   "source": [
    "## **‚≠ê02:Advanced Splitting Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b7e87",
   "metadata": {},
   "source": [
    "![img_2](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0502.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a87ff",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Limitations of Basic Splitting**\n",
    "\n",
    "- **Lack of Context Awareness:** Simple character-based splitting might break a function or paragraph in unnatural places, reducing model performance.\n",
    "\n",
    "- **Mismatch with Model Processing:** Since LLMs process tokens, character limits may not align with model capabilities, leading to token overflow or inefficient use of input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1d551",
   "metadata": {},
   "source": [
    "### **‚≠ïToken-Based Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1d162",
   "metadata": {},
   "source": [
    "- Splits are calculated by token count, which aligns with how LLMs consume input.\n",
    "- This ensures each chunk fits within the model‚Äôs token limit and avoids truncation.\n",
    "- Prevents loss of meaning due to mid-token splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "example_string = \"Mary had a little lamb, it's fleece was white as snow.\"\n",
    "\n",
    "# Get encoding for model\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "# Initialize the TokenTextSplitter\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=encoding.name,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=2\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = splitter.split_text(example_string)\n",
    "\n",
    "# Count tokens in each chunk and print them\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = len(encoding.encode(chunk))\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {token_count}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1efefb",
   "metadata": {},
   "source": [
    "`cl100k_base` is the tokenizer encoding used for models like:\n",
    "\n",
    "- gpt-4\n",
    "- gpt-4-32k\n",
    "- gpt-3.5-turbo\n",
    "- gpt-3.5-turbo-16k\n",
    "- and now also used as a fallback when a model like gpt-4o-mini isn't directly supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "example_string = \"Mary had a little lamb, its fleece was white as snow.\"\n",
    "\n",
    "# Get encoding for the model\n",
    "# Use the 'cl100k_base' encoding for GPT-3.5 and GPT-4 models\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Set up token-based text splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    encoding_name=encoding.name,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Wrap the string in a Document object and split into chunks\n",
    "documents = [Document(page_content=example_string)]\n",
    "chunks = token_splitter.split_documents(documents)\n",
    "\n",
    "# Display the token count in each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {len(encoding.encode(chunk.page_content))}\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda16844",
   "metadata": {},
   "source": [
    "### **‚≠ïSemantic Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eade434",
   "metadata": {},
   "source": [
    "- Uses embedding models to understand the content and split based on semantic boundaries (logical breakpoints in meaning).\n",
    "\n",
    "- Employs gradient thresholding to decide where one idea ends and another begins.\n",
    "\n",
    "- Produces coherent, context-rich chunks that enhance downstream task accuracy (like answering or summarizing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc1819",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_community.document_transformers import SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Instantiate an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(api_key=\"<OPENAI_API_TOKEN>\", model='text-embedding-3-small')\n",
    "\n",
    "# Create the semantic text splitter with desired parameters\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embedding_model, breakpoint_threshold_type=\"gradient\", breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "print(chunks[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f49e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI  \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Google embedding model used to convert text into high-dimensional vectors\n",
    "# This model helps in understanding the meaning of text for semantic processing\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Create an instance of SemanticChunker to split text based on semantic changes (meaningful segments)\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,                         # Pass the embedding model\n",
    "    breakpoint_threshold_type=\"gradient\",          # Method to detect split points based on semantic gradient\n",
    "    breakpoint_threshold_amount=0.8                # Sensitivity of chunk splitting (higher = fewer splits)\n",
    ")\n",
    "\n",
    "# Split the input documents into semantically coherent chunks\n",
    "chunks = semantic_splitter.split_documents(python_data)\n",
    "\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec763bcc",
   "metadata": {},
   "source": [
    "## **‚≠ê03: Optimizing document retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c1d492",
   "metadata": {},
   "source": [
    "![img_3](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0503.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d32dd",
   "metadata": {},
   "source": [
    "### üîç **Dense vs. Sparse Retrieval in RAG Pipelines**\n",
    "\n",
    "\n",
    "![img_4](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0504.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c849e",
   "metadata": {},
   "source": [
    "When building Retrieval-Augmented Generation (RAG) systems‚Äîlike those in LangChain‚Äîyou typically choose between **dense** and **sparse** retrieval methods.\n",
    "\n",
    "- üß™ **Dense Retrieval**\n",
    "  -  Uses neural networks (e.g., transformers) to encode documents and queries into **dense vectors**‚Äîcompact numerical representations that capture meaning.\n",
    "  -  Relevance is measured via **vector similarity** (like cosine similarity or dot product).\n",
    "  - `Pros.` Vs `Cons.`:\n",
    "     - **‚úÖ Pros:**\n",
    "       - Captures **semantic meaning**‚Äîgood with synonyms, paraphrasing, and abstract queries.\n",
    "       - Powerful for **open-domain** or fuzzy information retrieval.\n",
    "     - **‚ö†Ô∏è Cons:**\n",
    "       -  Requires **expensive training** and GPU-based inference.\n",
    "       -  Harder to **interpret** why a document was retrieved.\n",
    "\n",
    "\n",
    "-  üìö **Sparse Retrieval**\n",
    "   - Based on **keyword matching** using traditional IR methods.\n",
    "   - Works with **bag-of-words** models‚Äîeach word is treated separately and sparsely.\n",
    "   - **Common Techniques**:\n",
    "      -  **TF-IDF** (*Term Frequency‚ÄìInverse Document Frequency*):\n",
    "         - Measures how important a word is to a document.\n",
    "         - > If a term appears often in one document but rarely across others, it gets a higher score.\n",
    "      - **BM25** (*Best Matching 25*):\n",
    "        - An advanced ranking function in the Okapi family.\n",
    "        - > It refines TF-IDF by adjusting for **term frequency saturation** and **document length**.\n",
    "     - `Pros.` Vs `Cons.`:\n",
    "       - **‚úÖ Pros:**\n",
    "         -  **Fast**, resource-efficient, and easy to **interpret**.\n",
    "         -  Great for **rare terms** and exact keyword matches.\n",
    "       - **‚ö†Ô∏è Cons:**\n",
    "         -  Struggles with **synonyms** or **semantic similarity**.\n",
    "         -  Can miss documents that are relevant but use **different wording**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d4beab",
   "metadata": {},
   "source": [
    "### üß† **TF-IDF vs. BM25: Quick Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea47dfc",
   "metadata": {},
   "source": [
    "| Feature           | TF-IDF                                 | BM25                                    |\n",
    "| ----------------- | -------------------------------------- | --------------------------------------- |\n",
    "| Scoring Basis     | Term frequency √ó inverse document freq | Improved term weighting with saturation |\n",
    "| Handles Long Docs | ‚ùå No                                   | ‚úÖ Yes                                   |\n",
    "| Customizable      | Limited                                | ‚úÖ Adjustable with `k1` and `b` params   |\n",
    "| Used In           | Classic search engines, baseline NLP   | Modern IR, LangChain RAG pipelines      |\n",
    "\n",
    "\n",
    "\n",
    "- üõ†Ô∏è **In LangChain Pipelines**\n",
    "  - **`BM25` is often preferred** over `TF-IDF` because it:\n",
    "    - Handles **longer documents** better.\n",
    "    - Reduces over-penalization for **repeated keywords**.\n",
    "    - Generally provides more **balanced scoring**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6ee219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant Documents:\n",
      "Python was created by Guido van Rossum and released in 1991.\n",
      "\n",
      "Response from Gemini LLM:\n",
      "In a RAG (Retrieval Augmented Generation) application, LLM hallucination, the tendency of LLMs to generate incorrect or nonsensical information, can significantly impact its accuracy and reliability in several ways:\n",
      "\n",
      "1. **Fabricated Information:**  If the LLM hallucinates facts not present in the retrieved documents, the final answer will be wrong. For example, if asked \"What libraries does Python use for AI and ML besides PyTorch?\", a hallucinating LLM might invent libraries that don't\n"
     ]
    }
   ],
   "source": [
    "# --- Required Imports ---\n",
    "from langchain_community.retrievers import BM25Retriever    # For keyword-based document retrieval\n",
    "from langchain_core.runnables import RunnablePassthrough    # Passes question directly through in the chain\n",
    "from langchain_core.prompts import PromptTemplate           # Used to format input to the LLM\n",
    "from langchain_core.output_parsers import StrOutputParser   # Extracts string outputs from LLM responses\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI   # Gemini wrapper for LLM inference\n",
    "from langchain_core.documents import Document               # Structure for text chunks used in retrieval\n",
    "\n",
    "from dotenv import load_dotenv  # Loads env variables like API keys\n",
    "load_dotenv()\n",
    "\n",
    "# --- Step 1: Input Text Chunks for Retrieval ---\n",
    "chunks = [\n",
    "    \"Python was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Python is a popular language for machine learning (ML).\",\n",
    "    \"The PyTorch library is a popular Python library for AI and ML.\",\n",
    "    \"Python is also used for web development, data analysis, and automation.\"\n",
    "]\n",
    "\n",
    "# --- Step 2: Create BM25 Retriever from Text Chunks ---\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks, k=3)  # `k` defines how many top results to return\n",
    "\n",
    "# --- Step 3: Test Simple Keyword-Based Retrieval ---\n",
    "results = bm25_retriever.invoke(\"Who created Python?\")\n",
    "print(\"Most relevant Documents:\")\n",
    "print(results[0].page_content)\n",
    "\n",
    "# --- Step 4: Convert Raw Strings to LangChain Document Objects ---\n",
    "documents = [Document(page_content=text) for text in chunks]\n",
    "\n",
    "# --- Step 5: Create a More Structured Retriever Using Documents ---\n",
    "retriever = BM25Retriever.from_documents(documents=documents, k=5)  # More flexible for later RAG chains\n",
    "\n",
    "# --- Step 6: Configure Gemini Flash Language Model ---\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",          # Lightweight but fast Gemini model\n",
    "    max_output_tokens=100,            # Limit response length\n",
    "    temperature=0.3                   # Lower temp = more deterministic response\n",
    ")\n",
    "\n",
    "# --- Step 7: Define RAG Prompt Template ---\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are an expert assistant. Using the following context:\\n\\n{context}\\n\\nAnswer the question:\\n{question}\"\n",
    ")\n",
    "\n",
    "# --- Step 8: Build Full Retrieval-Augmented Generation Chain ---\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Step 1: Retrieve relevant docs\n",
    "    | prompt                                                   # Step 2: Format input for LLM\n",
    "    | llm                                                      # Step 3: Call Gemini LLM\n",
    "    | StrOutputParser()                                        # Step 4: Clean string output\n",
    ")\n",
    "\n",
    "# --- Step 9: Run the Chain with a Sample Query ---\n",
    "question = \"How can LLM hallucination impact a RAG application?\"\n",
    "response = chain.invoke(question)\n",
    "print(\"\\nResponse from Gemini LLM:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c39e79",
   "metadata": {},
   "source": [
    "## **‚≠ê04: Introduction to RAG evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3f115",
   "metadata": {},
   "source": [
    "![img_5](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0505.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c21fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: {'reasoning': 'Grade: INCORRECT', 'value': 'INCORRECT', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1: Imports and Setup\n",
    "# ============================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langsmith.evaluation import LangChainStringEvaluator \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your Gemini API key from the environment\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Make sure it's set in your .env file.\")\n",
    "\n",
    "# ============================\n",
    "# 2: Input Data\n",
    "# ============================\n",
    "query = \"What are the main components of RAG architecture?\"\n",
    "predicted_answer = \"Training and encoding\"\n",
    "ref_answer = \"Retrieval and Generation\"\n",
    "\n",
    "# ============================\n",
    "# 3: Prompt Template\n",
    "# ============================\n",
    "prompt_template = \"\"\"You are an expert professor specialized in grading students' answers.\n",
    "You are grading the following question: {query}\n",
    "Here is the real answer: {answer}\n",
    "You are grading the following predicted answer: {result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4: LLM Setup\n",
    "# ============================\n",
    "eval_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    max_output_tokens=100,\n",
    "    temperature=0.3,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5: Evaluator Setup\n",
    "# ============================\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "    \"qa\",  # <--- Name of the evaluator type; others include \"criteria\", \"embedding_distance\", etc. -> question answering - evaluation\n",
    "    config={ \n",
    "        \"llm\": eval_llm,\n",
    "        \"prompt\": prompt,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 6: Run Evaluation\n",
    "# ============================\n",
    "score = qa_evaluator.evaluator.evaluate_strings(prediction=predicted_answer,\n",
    "                                                reference=ref_answer,\n",
    "                                                input=query\n",
    "                                                ) \n",
    "\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d55bea",
   "metadata": {},
   "source": [
    "### üîç What Is the RAGAS Framework?\n",
    "\n",
    "**RAGAS** (Retrieval-Augmented Generation Assessment Score) is a **framework** that evaluates both **retrieval** and **generation quality** in RAG pipelines. It provides **automatic evaluation** without needing human-annotated answers.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ RAGAS Evaluation Metrics (From the Image)\n",
    "\n",
    "#### üß† Generation Metrics:\n",
    "\n",
    "1. **Faithfulness**:\n",
    "\n",
    "   * Measures if the generated answer **truthfully reflects** the context.\n",
    "   * Formula:\n",
    "\n",
    "     $$\n",
    "     \\text{Faithfulness} = \\frac{\\text{No. of claims made that can be inferred from context}}{\\text{Total no. of claims}}\n",
    "     $$\n",
    "   * Normalized between **0 and 1** (closer to 1 = more faithful).\n",
    "   * Detects **hallucination** in LLM outputs.\n",
    "\n",
    "2. **Answer Relevancy**:\n",
    "\n",
    "   * How relevant is the generated answer to the original **query**?\n",
    "\n",
    "#### üîç Retrieval Metrics:\n",
    "\n",
    "3. **Context Precision**:\n",
    "\n",
    "   * Measures the **signal-to-noise** ratio in the retrieved context.\n",
    "   * High precision = mostly relevant documents.\n",
    "\n",
    "4. **Context Recall**:\n",
    "\n",
    "   * Measures how much of the **necessary context** was retrieved.\n",
    "   * Can the retriever fetch **all relevant** information?\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Code Example Using `ragas` in Python\n",
    "\n",
    "To use these RAGAS metrics in code, you typically integrate it with `LangChain`, `ragas`, and optionally `Haystack` or `FAISS` for retrieval.\n",
    "\n",
    "\n",
    "### üí° Summary of Each Metric:\n",
    "\n",
    "| Metric                | Type       | Measures                                       | Goal                |\n",
    "| --------------------- | ---------- | ---------------------------------------------- | ------------------- |\n",
    "| **Faithfulness**      | Generation | If answer is supported by retrieved context    | Avoid hallucination |\n",
    "| **Answer Relevancy**  | Generation | If answer is relevant to the original question | Stay on-topic       |\n",
    "| **Context Precision** | Retrieval  | % of retrieved context that is relevant        | Reduce noise        |\n",
    "| **Context Recall**    | Retrieval  | % of required info retrieved from corpus       | Maximize coverage   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.0\n",
      "Context Precision Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import faithfulness, context_precision\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize Gemini model and embeddings\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Faithfulness Evaluation\n",
    "faithfulness_chain = EvaluatorChain(\n",
    "    metric=faithfulness,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "eval_result = faithfulness_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with LLMs?\", \n",
    "    \"answer\": \"The RAG model improves question answering by combining the retrieval of documents...\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages...\", \n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources, allowing the...\",\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"Faithfulness Score:\", eval_result['faithfulness'])\n",
    "\n",
    "# Context Precision Evaluation\n",
    "context_precision_chain = EvaluatorChain(\n",
    "    metric=context_precision,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "context_precision_result = context_precision_chain({\n",
    "    \"question\": \"How does the RAG model improve question answering with large language models?\",\n",
    "    \"ground_truth\": \"The RAG model improves question answering by combining the retrieval of...\",\n",
    "    \"contexts\": [ \n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving...\", \n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources...\",\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"Context Precision Score:\", context_precision_result['context_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "877e79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Faithfulness Evaluation ---\n",
      "Faithfulness Score: 0.0000\n",
      "\n",
      "--- Context Precision Evaluation ---\n",
      "Context Precision Score: 0.0000\n",
      "\n",
      "--- Answer Relevance Evaluation ---\n",
      "Answer Relevance Score: 0.8271\n",
      "\n",
      "--- Conciseness Evaluation ---\n",
      "Conciseness Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import faithfulness, context_precision, answer_relevancy\n",
    "from ragas.metrics._aspect_critic import AspectCritic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Load API Key ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Initialize Gemini Model & Embeddings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0  # deterministic\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Define Conciseness Metric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "conciseness = AspectCritic(\n",
    "    name=\"conciseness\",\n",
    "    definition=(\n",
    "        \"Does the submission convey information or ideas clearly and efficiently, \"\n",
    "        \"without unnecessary or redundant details?\"\n",
    "    ),\n",
    "    strictness=3\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1. Faithfulness Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n--- Faithfulness Evaluation ---\")\n",
    "faithfulness_chain = EvaluatorChain(\n",
    "    metric=faithfulness,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "faithfulness_eval_data = {\n",
    "    \"question\": \"How does the RAG model improve question answering with LLMs?\",\n",
    "    \"answer\":   \"The RAG model improves question answering by combining the retrieval of documents, which provides external knowledge, with the generation capabilities of large language models. This allows the LLM to provide more accurate and up-to-date answers.\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages from a knowledge base based on the user's query.\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources, allowing the language model to ground its responses in factual information beyond its training data.\",\n",
    "        \"Traditional LLMs might hallucinate or provide outdated information; RAG mitigates this by providing a current and relevant context for generation.\"\n",
    "    ]\n",
    "}\n",
    "res_faith = faithfulness_chain(faithfulness_eval_data)\n",
    "print(f\"Faithfulness Score: {res_faith['faithfulness']:.4f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2. Context Precision Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n--- Context Precision Evaluation ---\")\n",
    "context_chain = EvaluatorChain(\n",
    "    metric=context_precision,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "context_precision_eval_data = {\n",
    "    \"question\": \"How does the RAG model improve question answering with large language models?\",\n",
    "    \"ground_truth\": \"The RAG model improves question answering by dynamically retrieving relevant information from a vast knowledge base and then using this information to inform the large language model's response, leading to more accurate and factual answers.\",\n",
    "    \"contexts\": [\n",
    "        \"The RAG model integrates document retrieval with LLMs by first retrieving relevant passages from a knowledge base based on the user's query.\",\n",
    "        \"By incorporating retrieval mechanisms, RAG leverages external knowledge sources, allowing the language model to ground its responses in factual information beyond its training data.\",\n",
    "        \"Traditional LLMs might hallucinate or provide outdated information; RAG mitigates this by providing a current and relevant context for generation.\",\n",
    "        \"The RAG approach enhances the ability of LLMs to answer complex questions by accessing information that was not part of their initial training set.\"\n",
    "    ]\n",
    "}\n",
    "res_ctx = context_chain(context_precision_eval_data)\n",
    "print(f\"Context Precision Score: {res_ctx['context_precision']:.4f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3. Answer Relevance Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n--- Answer Relevance Evaluation ---\")\n",
    "relevancy_chain = EvaluatorChain(\n",
    "    metric=answer_relevancy,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "answer_relevancy_eval_data = {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"answer\":   \"Paris is the capital of France, known for its iconic Eiffel Tower and rich history.\"\n",
    "}\n",
    "res_rel = relevancy_chain(answer_relevancy_eval_data)\n",
    "print(f\"Answer Relevance Score: {res_rel['answer_relevancy']:.4f}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4. Conciseness Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n--- Conciseness Evaluation ---\")\n",
    "conciseness_chain = EvaluatorChain(\n",
    "    metric=conciseness,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "conciseness_eval_data = {\n",
    "    \"question\": \"Describe the main function of a CPU.\",\n",
    "    \"answer\":   \"The central processing unit, often abbreviated as CPU, is essentially the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control, and input/output (I/O) operations specified by the instructions. It's often called the 'brain' of the computer.\"\n",
    "}\n",
    "res_conc = conciseness_chain(conciseness_eval_data)\n",
    "print(f\"Conciseness Score: {res_conc['conciseness']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1fdc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.0\n",
      "Context Precision Score: 0.0\n",
      "Answer Relevance Score: 0.8305495956386126\n",
      "Conciseness Score: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import faithfulness, context_precision, answer_relevancy\n",
    "from ragas.metrics._aspect_critic import AspectCritic\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def load_api_key(env_var: str = \"GOOGLE_API_KEY\") -> str:\n",
    "    load_dotenv()\n",
    "    key = os.getenv(env_var)\n",
    "    if not key:\n",
    "        raise EnvironmentError(f\"{env_var} not found in .env file.\")\n",
    "    return key\n",
    "\n",
    "def initialize_models(api_key: str):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0\n",
    "    )\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=api_key\n",
    "    )\n",
    "    return llm, embeddings\n",
    "\n",
    "# Optional: define conciseness if you ever need it\n",
    "def build_conciseness_metric():\n",
    "    return AspectCritic(\n",
    "        name=\"conciseness\",\n",
    "        definition=(\n",
    "            \"Does the submission convey information or ideas clearly and efficiently, \"\n",
    "            \"without unnecessary or redundant details?\"\n",
    "        ),\n",
    "        strictness=3\n",
    "    )\n",
    "\n",
    "def run_evaluation(metric, llm, embeddings, inputs: dict, display_name: str, output_key: str):\n",
    "    chain = EvaluatorChain(metric=metric, llm=llm, embeddings=embeddings)\n",
    "    result = chain.invoke(inputs)\n",
    "    score = result.get(output_key)\n",
    "    print(f\"{display_name} Score:\", score if score is not None else \"No score returned\")\n",
    "    return score\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Task‚ÄêSpecific Evaluations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def evaluate_faithfulness(llm, embeddings):\n",
    "    inputs = {\n",
    "        \"question\": \"What are the causes of climate change?\",\n",
    "        \"answer\":   \"Climate change is caused mainly by increased use of electric vehicles.\",\n",
    "        \"contexts\": [\n",
    "            \"Climate change is primarily driven by greenhouse gas emissions from burning fossil fuels like coal and oil.\",\n",
    "            \"Deforestation and industrial pollution are also major contributors to global warming.\"\n",
    "        ]\n",
    "    }\n",
    "    return run_evaluation(\n",
    "        metric=faithfulness,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        inputs=inputs,\n",
    "        display_name=\"Faithfulness\",\n",
    "        output_key=\"faithfulness\"\n",
    "    )\n",
    "\n",
    "def evaluate_context_precision(llm, embeddings):\n",
    "    inputs = {\n",
    "        \"question\": \"What are the causes of climate change?\",\n",
    "        \"ground_truth\": \"Climate change is caused by greenhouse gas emissions, deforestation, and industrial activity.\",\n",
    "        \"contexts\": [\n",
    "            \"Greenhouse gases from fossil fuels trap heat in the atmosphere, causing the planet to warm.\",\n",
    "            \"Deforestation reduces the Earth‚Äôs capacity to absorb CO2, contributing to global warming.\",\n",
    "            \"Eating healthy foods can prevent heart disease and diabetes.\"\n",
    "        ]\n",
    "    }\n",
    "    return run_evaluation(\n",
    "        metric=context_precision,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        inputs=inputs,\n",
    "        display_name=\"Context Precision\",\n",
    "        output_key=\"context_precision\"\n",
    "    )\n",
    "\n",
    "def evaluate_answer_relevancy(llm, embeddings):\n",
    "    inputs = {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\":   \"Paris is the capital of France, known for its iconic Eiffel Tower and rich history.\"\n",
    "    }\n",
    "    return run_evaluation(\n",
    "        metric=answer_relevancy,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        inputs=inputs,\n",
    "        display_name=\"Answer Relevance\",\n",
    "        output_key=\"answer_relevancy\"\n",
    "    )\n",
    "\n",
    "def evaluate_conciseness(llm, embeddings):\n",
    "    conciseness = build_conciseness_metric()\n",
    "    inputs = {\n",
    "        \"question\": \"Describe the main function of a CPU.\",\n",
    "        \"answer\":   (\n",
    "            \"The central processing unit, often abbreviated as CPU, is essentially \"\n",
    "            \"the electronic circuitry within a computer that carries out the instructions \"\n",
    "            \"of a computer program by performing the basic arithmetic, logical, control, \"\n",
    "            \"and input/output (I/O) operations specified by the instructions. It's often \"\n",
    "            \"called the 'brain' of the computer.\"\n",
    "        )\n",
    "    }\n",
    "    return run_evaluation(\n",
    "        metric=conciseness,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        inputs=inputs,\n",
    "        display_name=\"Conciseness\",\n",
    "        output_key=\"conciseness\"\n",
    "    )\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Main Entry Point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def main():\n",
    "    api_key = load_api_key()\n",
    "    llm, embeddings = initialize_models(api_key)\n",
    "\n",
    "    evaluate_faithfulness(llm, embeddings)\n",
    "    evaluate_context_precision(llm, embeddings)\n",
    "    evaluate_answer_relevancy(llm, embeddings)\n",
    "    evaluate_conciseness(llm, embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64112413",
   "metadata": {},
   "source": [
    "# üß© ***Oth-code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Load environment variables (expects GOOGLE_API_KEY in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Step 2: Define the path to the Python file\n",
    "PATH = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\pyfile.py\"\n",
    "\n",
    "# Step 3: Load the Python file as LangChain Documents\n",
    "loader = PythonLoader(file_path=PATH)\n",
    "python_data = loader.load()  # Returns a list of Document objects\n",
    "\n",
    "# Step 4: Initialize Google embedding model for semantic chunking\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Step 5: Create a SemanticChunker instance\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\",  # Use gradient-based breakpoints\n",
    "    breakpoint_threshold_amount=0.8        # Threshold for chunk separation\n",
    ")\n",
    "\n",
    "# Step 6: Perform semantic chunking on the loaded documents\n",
    "chunks = semantic_splitter.split_documents(python_data)\n",
    "\n",
    "# Step 7: Print out all chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n{'-'*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
