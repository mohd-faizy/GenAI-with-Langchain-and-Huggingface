{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c371890f",
      "metadata": {},
      "source": [
        "# **üî∑üî∑Chain & Agentsüî∑üî∑**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f7b39f",
      "metadata": {
        "id": "57f7b39f"
      },
      "source": [
        "## **‚≠ê01: Sequential Chains**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad92fd58",
      "metadata": {},
      "source": [
        "\n",
        "- LangChain allows developers to build chains of prompts and models. A sequential chain connects multiple components where the output of one becomes the input of the next.\n",
        "\n",
        "- This is useful when you want to perform multi-step operations, such as planning a trip itinerary.\n",
        "\n",
        "- LangChain allows developers to build chains of prompts and models. A sequential chain connects multiple components where the output of one becomes the input of the next.\n",
        "\n",
        "- This is useful when you want to perform multi-step operations, such as planning a trip itinerary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e2aee5",
      "metadata": {},
      "source": [
        "\n",
        "### ‚≠ï AI-Powered Travel Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9670d24",
      "metadata": {
        "id": "e9670d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given you only have one day, here's a jam-packed itinerary focusing on three key experiences offering a blend of history, culture, and a taste of Delhi's energy:\n",
            "\n",
            "**Morning (Historical Grandeur):**\n",
            "\n",
            "* **8:00 AM - 11:00 AM: Red Fort (Lal Qila):** Start your day early at the magnificent Red Fort.  Allow at least 3 hours to explore this UNESCO World Heritage site thoroughly.  Consider hiring a guide for a richer experience (optional, but highly recommended).\n",
            "\n",
            "**Afternoon (Cultural Immersion & Relaxation):**\n",
            "\n",
            "* **11:30 AM - 1:30 PM:  Rickshaw Ride through Old Delhi & Jama Masjid:** Take a thrilling rickshaw ride through the bustling streets of Old Delhi to reach Jama Masjid.  Negotiate the rickshaw fare beforehand.  Spend some time admiring the architecture of Jama Masjid (remember respectful attire).\n",
            "\n",
            "* **1:30 PM - 2:30 PM: Lunch:** Enjoy a delicious and authentic Delhi street food lunch near Jama Masjid.  There are plenty of options, but choose carefully considering hygiene.\n",
            "\n",
            "* **2:30 PM - 4:30 PM: Humayun's Tomb:**  Escape the Old Delhi chaos with a visit to the serene Humayun's Tomb.  Its stunning architecture and peaceful atmosphere provide a welcome contrast.\n",
            "\n",
            "\n",
            "**Evening (Modern Delhi & Departure):**\n",
            "\n",
            "* **4:30 PM - 6:30 PM:  Khan Market:**  Head to Khan Market for some upscale shopping and a refreshing break.  You can grab a coffee or tea and browse the boutiques.\n",
            "\n",
            "* **6:30 PM onwards:  Dinner & Departure:**  Enjoy a final delicious Delhi dinner before heading to the airport or your accommodation for departure.  Consider trying a restaurant in Khan Market or another area convenient to your transportation.\n",
            "\n",
            "\n",
            "**Important Notes:**\n",
            "\n",
            "* **Transportation:** Utilize Delhi's Metro wherever possible to save time and navigate efficiently.  Rickshaws are best for Old Delhi's narrow lanes.\n",
            "* **Dress Code:** Dress modestly, especially when visiting religious sites like Jama Masjid.\n",
            "* **Bargaining:**  Bargaining is expected in markets like Chandni Chowk (if you choose to visit it instead of Khan Market).\n",
            "* **Hydration:** Carry water, especially during warmer months.\n",
            "* **Flexibility:** This itinerary is ambitious. Be prepared to adjust based on your energy levels and interests.\n",
            "\n",
            "\n",
            "This itinerary prioritizes maximizing your single day in Delhi with a powerful combination of iconic historical sites, a cultural immersion experience, and a taste of modern Delhi's vibrancy.  Remember to check opening times and plan travel times between locations accordingly.\n"
          ]
        }
      ],
      "source": [
        "# Import required components\n",
        "from langchain.prompts import PromptTemplate                      # For formatting prompts with variables\n",
        "from langchain_core.output_parsers import StrOutputParser         # Extracts plain text output from LLM responses for easier processing\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI         # Gemini model via LangChain\n",
        "from langchain_core.runnables import RunnableMap                  # # Maps multiple inputs to corresponding outputs in a chain for parallel execution\n",
        "from dotenv import load_dotenv                                    # To load API keys from .env\n",
        "\n",
        "# Load environment variables (like Gemini API key)\n",
        "load_dotenv()\n",
        "\n",
        "# Prompt to get activity suggestions based on destination\n",
        "destination_prompt = PromptTemplate(\n",
        "    input_variables=[\"destination\"],\n",
        "    template=\"I am planning a trip to {destination}. Can you suggest some activities to do there?\"\n",
        ")\n",
        "\n",
        "# Prompt to generate itinerary from suggested activities\n",
        "activities_prompt = PromptTemplate(\n",
        "    input_variables=[\"activities\"],\n",
        "    template=\"I only have one day, so can you create an itinerary from your top three activities: {activities}.\"\n",
        ")\n",
        "\n",
        "# Initialize Gemini model with desired settings\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",          # Specify the model to use\n",
        "    # max_output_tokens=100,           # Optional: limit response length\n",
        "    temperature=0.7                    # Controls creativity of output\n",
        ")\n",
        "\n",
        "# Parser to extract clean string output from LLM\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Step 1: Generate activity list from destination\n",
        "step1 = destination_prompt | llm | parser\n",
        "\n",
        "# Step 2: Feed activities into itinerary prompt, then get final response\n",
        "seq_chain = RunnableMap({\"activities\": step1}) | activities_prompt | llm | parser\n",
        "\n",
        "# Run the chain with an example input\n",
        "output = seq_chain.invoke({\"destination\": \"New Delhi\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3506e347",
      "metadata": {},
      "source": [
        "### ‚≠ï Building Smart AI-Powered Learning Assistants (Personalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485ce9fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Personalized Machine Learning Study Plan (10 hours/week)\n",
            "\n",
            "This plan prioritizes hands-on projects and video lectures, leveraging the user's intermediate Python, PyTorch/TensorFlow, and math background.  The goal is portfolio building for a data science job.\n",
            "\n",
            "**Phase 1: Solidify Foundations & Practical Skills (Weeks 1-4)**\n",
            "\n",
            "* **Module 1: Regression & Classification (Week 1-2):**\n",
            "    * **Task 1 (2 hours):** Review linear algebra & calculus concepts relevant to ML (Khan Academy or 3Blue1Brown for focused review).\n",
            "    * **Task 2 (4 hours):** Complete DataCamp's introductory machine learning course focusing on regression and classification.  Focus on practical implementation in Python (Scikit-learn).\n",
            "    * **Task 3 (4 hours):**  Build a simple regression project (e.g., house price prediction using a publicly available dataset).  Use Scikit-learn, document your code thoroughly, and analyze results.\n",
            "\n",
            "* **Module 2:  Deep Learning Introduction (Week 3-4):**\n",
            "    * **Task 1 (3 hours):** Watch fast.ai's introductory deep learning videos (focus on practical coding aspects).\n",
            "    * **Task 2 (4 hours):**  Work through fast.ai's introductory notebooks, focusing on PyTorch or TensorFlow (choose one based on preference).\n",
            "    * **Task 3 (3 hours):** Build a simple image classification project using a pre-trained model (e.g., transfer learning with ResNet on CIFAR-10).  Track your experiments and results.\n",
            "\n",
            "\n",
            "**Phase 2:  Advanced Topics & Portfolio Expansion (Weeks 5-8)**\n",
            "\n",
            "* **Module 3:  Neural Network Architectures (Week 5-6):**\n",
            "    * **Task 1 (3 hours):** Watch lectures on specific neural network architectures (CNNs, RNNs) from resources like Stanford CS229 lectures (relevant sections).  Focus on understanding the architectures and their applications.\n",
            "    * **Task 2 (7 hours):** Build a project involving a more complex architecture.  Examples:  Image classification with a CNN from scratch (MNIST or a similar dataset), or a sentiment analysis project using RNNs (IMDB reviews).\n",
            "\n",
            "\n",
            "* **Module 4:  Advanced Techniques & Deployment (Week 7-8):**\n",
            "    * **Task 1 (4 hours):** Explore advanced techniques like regularization, hyperparameter tuning, and model evaluation in depth. Use resources like articles from Towards Data Science.\n",
            "    * **Task 2 (6 hours):** Choose one project from Phase 1 or 2 and deploy it using a simple framework (e.g., Flask, Streamlit). Create a basic web interface for user interaction.\n",
            "\n",
            "\n",
            "**Phase 3: Specialization & Portfolio Refinement (Weeks 9-12 and beyond)**\n",
            "\n",
            "* **Module 5: Specialization (Weeks 9-12):**\n",
            "    * **Task 1 (10 hours/week):** Choose a specialization area (e.g., Computer Vision, NLP, Time Series Analysis).  Select a relevant project from Kaggle or a similar platform.  Focus on delivering a high-quality, well-documented project.  Use relevant libraries and techniques learned throughout the plan.\n",
            "\n",
            "\n",
            "**Ongoing Activities:**\n",
            "\n",
            "* **Regularly review your code:**  Refactor and improve your code throughout the plan.\n",
            "* **Document your projects thoroughly:**  Create a GitHub repository to showcase your work.\n",
            "* **Network with other data scientists:**  Attend online meetups or join relevant communities.\n",
            "* **Contribute to open-source projects (optional):** This is a great way to build skills and demonstrate your abilities.\n",
            "\n",
            "\n",
            "**Resource Allocation:**\n",
            "\n",
            "* **DataCamp:** Primarily for structured learning in Phases 1 and 2.\n",
            "* **fast.ai:** Primarily for practical deep learning in Phases 1 and 2.\n",
            "* **Stanford CS229 (select lectures):** For deeper understanding of specific architectures in Phase 2.\n",
            "* **Towards Data Science & Analytics Vidhya:** For articles on advanced techniques and staying updated.\n",
            "* **Kaggle:** For finding projects and datasets.\n",
            "\n",
            "\n",
            "This plan is a suggestion and can be adjusted based on your progress and interests.  Remember to focus on building a strong portfolio that showcases your skills and abilities.  Consistent effort and a focus on practical application will be key to your success.\n"
          ]
        }
      ],
      "source": [
        "# Import required components\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser \n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnableMap\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (like Gemini API key)\n",
        "load_dotenv()\n",
        "\n",
        "# Prompt to get learning resources based on topic\n",
        "learning_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"I want to learn about {topic}. Can you suggest some resources like articles, videos, or courses?\"\n",
        ")\n",
        "\n",
        "# Prompt to generate a personalized study plan using resources + user info\n",
        "study_plan_prompt = PromptTemplate(\n",
        "    input_variables=[\n",
        "        \"resources\",\n",
        "        \"programming_experience\",\n",
        "        \"math_background\",\n",
        "        \"learning_style\",\n",
        "        \"learning_goals\",\n",
        "        \"time_commitment\"\n",
        "    ],\n",
        "    template= \"\"\"\n",
        "    I have the following resources: {resources}\n",
        "    \n",
        "    Create a personalized study plan using these resources based on this user's information:\n",
        "    \n",
        "        1. Programming experience: {programming_experience}\n",
        "        2. Math background: {math_background}\n",
        "        3. Preferred learning style: {learning_style}\n",
        "        4. Learning goals: {learning_goals}\n",
        "        5. Time commitment per week: {time_commitment} hours\n",
        "\n",
        "        Make the plan structured and practical. Include weeks/modules, brief tasks, and align them with the user's preferences and pace.\n",
        "        \"\"\"\n",
        "        )\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Output parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Step 1: Generate resource list from topic\n",
        "step1 = learning_prompt | llm | parser\n",
        "\n",
        "# Step 2: Personalized study plan using resources and user profile\n",
        "seq_chain = RunnableMap({\n",
        "    \"resources\": step1,\n",
        "    \"programming_experience\": lambda x: x[\"programming_experience\"],\n",
        "    \"math_background\": lambda x: x[\"math_background\"],\n",
        "    \"learning_style\": lambda x: x[\"learning_style\"],\n",
        "    \"learning_goals\": lambda x: x[\"learning_goals\"],\n",
        "    \"time_commitment\": lambda x: x[\"time_commitment\"],\n",
        "}) | study_plan_prompt | llm | parser\n",
        "\n",
        "# Example input from user\n",
        "user_input = {\n",
        "    \"topic\": \"Machine Learning\",\n",
        "    \"programming_experience\": \"Intermediate (Python, Pytorch, TensorFlow)\",\n",
        "    \"math_background\": \"Intermediate (Calculus, Linear Algebra)\",\n",
        "    \"learning_style\": \"Hands-on projects and video lectures\",\n",
        "    \"learning_goals\": \"Build a portfolio to get a job in data science\",\n",
        "    \"time_commitment\": \"10\"\n",
        "}\n",
        "\n",
        "# Run the chain\n",
        "output = seq_chain.invoke(user_input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635ecaad",
      "metadata": {
        "id": "635ecaad"
      },
      "source": [
        "## **‚≠ê02: LangChain Agents and ReAct**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2edee47",
      "metadata": {},
      "source": [
        "* **Agents in LangChain**:\n",
        "\n",
        "  * Use LLMs to make decisions and perform actions by calling external tools.\n",
        "  * Enable dynamic workflows where the model can choose the next step based on the task.\n",
        "\n",
        "* **ReAct Pattern** (Reasoning + Acting):\n",
        "\n",
        "  * A framework where the LLM thinks through a problem step by step.\n",
        "  * Chooses a relevant tool to act with.\n",
        "  * Observes the tool's output.\n",
        "  * Continues reasoning based on the observed result.\n",
        "\n",
        "* **Flow of ReAct Agent**:\n",
        "\n",
        "  1. **Thought**: Model decides what to do next.\n",
        "  2. **Act**: Model selects and calls the appropriate tool with input.\n",
        "  3. **Observe**: Model receives and processes the tool's output.\n",
        "  4. **Repeat or respond**: Model may continue thinking and acting or provide the final answer.\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "```\n",
        "# Thought: I should call Weather() to find the weather in Kingston, Jamaica.\n",
        "# Act: Weather(\"Kingston, Jamaica\")\n",
        "# Observe: The weather is mostly sunny with temperatures of 82¬∞F.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e942b60",
      "metadata": {
        "id": "6e942b60"
      },
      "source": [
        "## **‚≠ê03: LangGraph**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d664a50",
      "metadata": {},
      "source": [
        "- üîÑ **LangGraph** is a framework built on top of LangChain.\n",
        "- üß† It helps you create **stateful AI agents** ‚Äî ones that remember and adapt as they go.\n",
        "- üîß Based on **graphs**, where each node is a tool or step in the agent's logic.\n",
        "- üîÅ Supports **loops and branching**, so the agent can revisit steps or make decisions.\n",
        "- üß± Makes building **complex workflows** with language models easier and more structured.\n",
        "- ü§ù Works well with tools, memory, and custom logic for smarter agents.\n",
        "- ‚ö° Good for tasks like **chatbots, planning agents, and multi-step reasoning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a233f6cb",
      "metadata": {},
      "source": [
        "### ‚≠ï Building a Gemini-Based Agent for Math Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e07134b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to perform the calculations in the parentheses first, then add the results.  I will use the calculator to help.\n",
            "Action: Calculator\n",
            "Action Input: (45 / 9) + (7 * 3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 26.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 26\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Final Answer: {'input': 'What is the result of (45 / 9) + (7 * 3)?', 'output': '26'}\n",
            "Agent Response: 26\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "# Load environment and setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             temperature=0.7)\n",
        "\n",
        "# Load math tool\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "\n",
        "# Create agent using Zero-shot ReAct (works well with Gemini)\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Ask a math question\n",
        "response = agent.invoke(\"What is the result of (45 / 9) + (7 * 3)?\")\n",
        "print(\"\\nFinal Answer:\", response)\n",
        "print(\"Agent Response:\", response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e94ce0f",
      "metadata": {},
      "source": [
        "### ‚≠ï `wikipedia` based AI Query System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55479c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Agent Answer:\n",
            "India's population is over 1.4 billion people, making it the most populous country in the world.  More precise figures vary slightly depending on the source and year, but it's currently around 1.428 billion according to the UN.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import Tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# LangChain-Community helpers\n",
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
        "\n",
        "# 1) Initialize Gemini\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# 2) Build a Wikipedia-query tool by wrapping WikipediaAPIWrapper ‚Üí WikipediaQueryRun\n",
        "wiki_wrapper = WikipediaAPIWrapper()\n",
        "wiki_runner = WikipediaQueryRun(api_wrapper=wiki_wrapper)\n",
        "\n",
        "wiki_tool = Tool(\n",
        "    name=\"wikipedia_search\",\n",
        "    func=wiki_runner.run,\n",
        "    description=(\n",
        "        \"Use this tool whenever you need factual information, context, or background from Wikipedia. \"\n",
        "        \"Input should be a concise topic or question‚Äîe.g. 'New York City population', 'Apollo 11 mission', \"\n",
        "        \"'Python programming language history', or 'Marie Curie biography'.\\n\\n\"\n",
        "        \"The output is an excerpt (or summary) from the corresponding Wikipedia article. \"\n",
        "        \"You can rely on it to gather definitions, dates, statistics, and high-level overviews. \"\n",
        "        \"If the initial excerpt doesn‚Äôt fully answer a multi-part query, you may run a follow-up search \"\n",
        "        \"with a more specific phrase.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 3) Create a ReAct-style agent that knows about ‚Äúwikipedia_search‚Äù\n",
        "agent = create_react_agent(llm, [wiki_tool])\n",
        "\n",
        "# 4) Ask a question\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"human\", \"How many people live in India?\")]\n",
        "    # \"messages\": [(\"human\", \"How many people live in India as of 2025?\")] # This query require other tools like internet search to answer this accurately\n",
        "})\n",
        "\n",
        "print(\"Final Agent Answer:\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81349200",
      "metadata": {},
      "source": [
        "### ‚≠ï Multi-Source AI Query System(`wikipedia + DuckDuckGo`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "65e656e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Agent Answer:\n",
            "Based on the internet search, estimates for India's population in 2025 vary.  One estimate puts it at approximately 1.45 billion, making it the world's most populous country.  Another source gives a figure of 1,462,732,387 as of May 31, 2025.  It's important to note that these are estimates, and the exact number may differ slightly depending on the source and methodology used.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import Tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# LangChain‚ÄêCommunity Wikipedia helpers\n",
        "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
        "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun\n",
        "\n",
        "# DuckDuckGo search (no API key required)\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# 1) Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# 2a) Build a Wikipedia‚Äêquery tool\n",
        "wiki_wrapper = WikipediaAPIWrapper()\n",
        "wiki_runner = WikipediaQueryRun(api_wrapper=wiki_wrapper)\n",
        "\n",
        "wiki_tool = Tool(\n",
        "    name=\"wikipedia_search\",\n",
        "    func=wiki_runner.run,\n",
        "    description=(\n",
        "        \"Use this tool whenever you need factual background or historical context from Wikipedia. \"\n",
        "        \"For example, you might ask: 'India population 2020 census' or 'India demographic history'. \"\n",
        "        \"It returns a Wikipedia excerpt or summary for your query.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 2b) Build a DuckDuckGo Internet‚Äêsearch tool\n",
        "ddg_wrapper = DuckDuckGoSearchRun()  # performs a live DuckDuckGo query\n",
        "ddg_tool = Tool(\n",
        "    name=\"internet_search\",\n",
        "    func=ddg_wrapper.run,\n",
        "    description=(\n",
        "        \"Use this tool to fetch up‚Äêto‚Äêdate, real‚Äêtime information from the web (via DuckDuckGo). \"\n",
        "        \"Input should be a query like 'India population 2025 estimate'. \"\n",
        "        \"It returns the top search snippets, which often include population figures or news reports.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 3) Create a ReAct‚Äêstyle agent that knows about both tools\n",
        "agent = create_react_agent(llm, [ddg_tool, wiki_tool])\n",
        "\n",
        "# 4) Ask the agent for India‚Äôs 2025 population\n",
        "response = agent.invoke({\n",
        "    \"messages\": [\n",
        "        (\"human\", \"How many people live in India as of 2025?\")\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Final Agent Answer:\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f803e2b9",
      "metadata": {
        "id": "f803e2b9"
      },
      "source": [
        "## **‚≠ê04: Creating Custom Tools**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b5147c",
      "metadata": {},
      "source": [
        "LangChain allows you to define and integrate custom tools.\n",
        "\n",
        "These tools can be simple Python functions wrapped for agent use.\n",
        "\n",
        "```python\n",
        "# Define a financial reporting function\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
        "\n",
        "    net_income = revenue - expenses\n",
        "\n",
        "    report = f\"Financial Report for {company_name}:\"\n",
        "    report += f\"Revenue: ${revenue}\"\n",
        "    report += f\"Expenses: ${expenses}\"\n",
        "    report += f\"Net Income: ${net_income}\"\n",
        "    return report\n",
        "\n",
        "# Test the function\n",
        "print(financial_report(company_name=\"LemonadeStand\", revenue=100, expenses=50))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93db1ac",
      "metadata": {
        "id": "c93db1ac"
      },
      "source": [
        "### ‚≠ïWrapping Function as a Tool\n",
        "\n",
        "\n",
        "To use the function with an agent, wrap it with `@tool` decorator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c106ecba",
      "metadata": {},
      "source": [
        "```python\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
        "\n",
        "    net_income = revenue - expenses\n",
        "\n",
        "    report = f\"Financial Report for {company_name}:\"\n",
        "    report += f\"Revenue: ${revenue}\"\n",
        "    report += f\"Expenses: ${expenses}\"\n",
        "    report += f\"Net Income: ${net_income}\"\n",
        "    return report\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40892d0b",
      "metadata": {
        "id": "40892d0b"
      },
      "source": [
        "### ‚≠ï Integrating Custom Tool with Agent\n",
        "\n",
        "\n",
        "You can pass your custom tool to the ReAct agent and use it in a natural language query.\n",
        "\n",
        "```python\n",
        "agent = create_react_agent(llm, [financial_report])\n",
        "\n",
        "messages = agent.invoke({\n",
        "    \"messages\": [(\"human\", \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\")]\n",
        "})\n",
        "print(messages)\n",
        "\n",
        "# Print the agent's final message\n",
        "print(messages['messages'][-1].content)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270f287e",
      "metadata": {},
      "source": [
        "### ‚≠ï Financial report tool with Gemini LLM and LangGraph agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac09a70",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent full response: {'messages': [HumanMessage(content='Can you calculate the area of a rectangle with length 10 meters and width 5 meters?', additional_kwargs={}, response_metadata={}, id='37fbbdec-6568-46f8-bd08-1f89688ae0de'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'calculate_area', 'arguments': '{\"length\": 10.0, \"width\": 5.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--0facb4f5-8f54-48c8-afd5-4b96f4d07737-0', tool_calls=[{'name': 'calculate_area', 'args': {'length': 10.0, 'width': 5.0}, 'id': 'c97a715f-71e7-42e1-bf8b-f3c96f5b745a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 42, 'output_tokens': 7, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='50.0', name='calculate_area', id='ed2202f0-4b32-43b8-a3ae-0eb2fa04aa36', tool_call_id='c97a715f-71e7-42e1-bf8b-f3c96f5b745a'), AIMessage(content='The area of the rectangle is 50 square meters.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--2bc113c3-d75b-49e4-a97d-1cd8e221e485-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})]}\n",
            "\n",
            "Final Agent Answer:\n",
            "The area of the rectangle is 50 square meters.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI \n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Define a simple tool to calculate area\n",
        "@tool\n",
        "def calculate_area(length: float, width: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the area of a rectangle given its length and width.\n",
        "    \"\"\"\n",
        "    return length * width\n",
        "\n",
        "# Initialize Gemini LLM \n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Create the ReAct agent with the calculate_area tool\n",
        "agent = create_react_agent(llm, [calculate_area])\n",
        "\n",
        "# Define a user message that describes what they want\n",
        "user_message = \"Can you calculate the area of a rectangle with length 10 meters and width 5 meters?\"\n",
        "\n",
        "# Call the agent with the human message\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"human\", user_message)]\n",
        "})\n",
        "\n",
        "# Print the full response (optional debug)\n",
        "print(\"Agent full response:\", response)\n",
        "\n",
        "# Print the final answer only\n",
        "print(\"\\nFinal Agent Answer:\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80a0375",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent full response: {'messages': [HumanMessage(content='TechStack generated made $10 million with $8 million of costs. Generate a financial report.', additional_kwargs={}, response_metadata={}, id='99edd5f9-2ec1-4bbf-960d-999bde80e223'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'financial_report', 'arguments': '{\"revenue\": 10000000.0, \"expenses\": 8000000.0, \"company_name\": \"TechStack\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--abb3b8e7-d4a2-4595-bcbb-c8623c4ff5b3-0', tool_calls=[{'name': 'financial_report', 'args': {'revenue': 10000000.0, 'expenses': 8000000.0, 'company_name': 'TechStack'}, 'id': '1e81cad7-66a6-4cdb-bd69-a40f75744fc3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 90, 'output_tokens': 12, 'total_tokens': 102, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Financial Report for TechStack:\\nRevenue: $10,000,000\\nExpenses: $8,000,000\\nNet Income: $2,000,000\\nStatus: Profitable\\n', name='financial_report', id='d7bcda54-b398-4fc1-937a-6d79cb279907', tool_call_id='1e81cad7-66a6-4cdb-bd69-a40f75744fc3'), AIMessage(content=\"TechStack's financial report shows a revenue of $10,000,000, expenses of $8,000,000, and a net income of $2,000,000.  The company is profitable.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--7d606ed8-4fd7-415b-acbc-f89cca559f86-0', usage_metadata={'input_tokens': 159, 'output_tokens': 58, 'total_tokens': 217, 'input_token_details': {'cache_read': 0}})]}\n",
            "\n",
            "Final Agent Answer:\n",
            "TechStack's financial report shows a revenue of $10,000,000, expenses of $8,000,000, and a net income of $2,000,000.  The company is profitable.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Define the financial return report\n",
        "# The @tool decorator registers this function as a tool for the LangChain agent\n",
        "@tool\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \n",
        "    net_income = revenue - expenses\n",
        "    \n",
        "    report = f\"Financial Report for {company_name}:\\n\"\n",
        "    report += f\"Revenue: ${revenue:,}\\n\"\n",
        "    report += f\"Expenses: ${expenses:,}\\n\"\n",
        "    report += f\"Net Income: ${net_income:,}\\n\"\n",
        "    \n",
        "    if net_income > 0:\n",
        "        report += \"Status: Profitable\\n\"\n",
        "    elif net_income < 0:\n",
        "        report += \"Status: Loss\\n\"\n",
        "    else:\n",
        "        report += \"Status: Break-even\\n\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Initialize Gemini LLM \n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             temperature=0)\n",
        "\n",
        "# Create the ReAct agent with the financial_report tool\n",
        "agent = create_react_agent(llm, [financial_report])\n",
        "\n",
        "# Define input message\n",
        "user_message = \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\"\n",
        "\n",
        "# Call the agent with the human message\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"human\", user_message)]\n",
        "})\n",
        "\n",
        "# Print the full response (including reasoning steps if verbose is enabled)\n",
        "print(\"Agent full response:\", response)\n",
        "\n",
        "# Print the final answer content only\n",
        "print(\"\\nFinal Agent Answer:\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
