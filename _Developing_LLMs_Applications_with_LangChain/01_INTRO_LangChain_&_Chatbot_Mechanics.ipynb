{"cells":[{"cell_type":"markdown","id":"c6becfe6","metadata":{"id":"c6becfe6"},"source":["# 📘 LangChain Chapter 1: In-Depth IPython Notebook"]},{"cell_type":"markdown","id":"91d9a4d3","metadata":{"id":"91d9a4d3"},"source":["\n","**Author: Faizy**  \n","**Topic: Developing LLM Applications with LangChain**\n"]},{"cell_type":"markdown","metadata":{"id":"c44c2ae6"},"source":["## 🧱 Introduction to LangChain Ecosystem\n","LangChain is a powerful framework designed to simplify the process of developing applications powered by large language models (LLMs). Its ecosystem provides tools to manage prompts, chains of calls, memory, agents, and integrations with external data sources or APIs. The core idea is to treat LLMs not just as isolated responders but as components in more complex workflows.\n","\n","Use cases include:\n","- Building chatbots with memory and context.\n","- Automating reasoning tasks by chaining prompt responses.\n","- Integrating with databases or search tools for retrieval-augmented generation (RAG)."],"id":"c44c2ae6"},{"cell_type":"markdown","metadata":{"id":"f6f492bf"},"source":["## 🔌 LangChain Integrations\n","LangChain supports various integrations to expand its capabilities. These include:\n","- **LLM Providers**: OpenAI (GPT-4, GPT-4o-mini), Hugging Face, Cohere, Anthropic.\n","- **Vector Databases**: FAISS, Pinecone, Weaviate for semantic search and context retrieval.\n","- **Toolkits and Agents**: Enable LLMs to interact with calculators, search engines, or APIs.\n","\n","Integrations allow users to build robust AI workflows that go beyond text generation, including retrieval, augmentation, and execution."],"id":"f6f492bf"},{"cell_type":"markdown","metadata":{"id":"32102265"},"source":["## 🔧 Building LLM Apps the LangChain Way\n","The LangChain development approach emphasizes modularity and composability. Applications are broken into:\n","- **Prompts**: The textual instructions or templates given to LLMs.\n","- **Chains**: Sequences of operations such as formatting input, calling a model, parsing output.\n","- **Agents**: LLMs with tool access that make decisions about which actions to take.\n","- **Memory**: Used to store and recall previous interactions.\n","\n","By using LangChain Expression Language (LCEL), developers can connect components using the `|` (pipe) operator, making the flow clear and testable."],"id":"32102265"},{"cell_type":"markdown","source":["## 🌳The Langchain Ecosystem"],"metadata":{"id":"8qC3eUqMg8WP"},"id":"8qC3eUqMg8WP"},{"cell_type":"markdown","source":["```python\n","from langchain.chat_models import ChatOpenAI\n","\n","# Define the LLM\n","llm = ChatOpenAI(model=\"gpt-4o-mini\",\n","                 api_key=\"<OPENAI_API_TOKEN>\")\n","\n","# Predict the words following the text in question\n","prompt = 'Three reasons for using LangChain for LLM application development.'\n","response = llm.invoke(prompt)\n","\n","print(response.content)\n","```"],"metadata":{"id":"Gfn18TXCg_Ul"},"id":"Gfn18TXCg_Ul"},{"cell_type":"code","source":["# Import the class for defining Hugging Face pipelines\n","from langchain_huggingface import HuggingFacePipeline"],"metadata":{"id":"ofL5MTewbqw9"},"execution_count":null,"outputs":[],"id":"ofL5MTewbqw9"},{"cell_type":"code","source":["# 1. DistilGPT-2 (~82MB) - Very fast and lightweight\n","llm_distilgpt2 = HuggingFacePipeline.from_model_id(\n","    model_id=\"distilgpt2\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n","    )\n","\n","prompt = \"Hugging Face is\"\n","response = llm_distilgpt2 .invoke(prompt)\n","print(response)"],"metadata":{"id":"u1VZ8cyOdPdc"},"execution_count":null,"outputs":[],"id":"u1VZ8cyOdPdc"},{"cell_type":"code","source":["# 2. GPT-2 Small (~124MB) - Classic small model\n","llm_gpt2 = HuggingFacePipeline.from_model_id(\n","    model_id=\"gpt2\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n",")\n","\n","prompt = \"Hugging Face is\"\n","response = llm_gpt2.invoke(prompt)\n","print(response)"],"metadata":{"id":"CeMf1qysdhCz"},"execution_count":null,"outputs":[],"id":"CeMf1qysdhCz"},{"cell_type":"code","source":["# 3. TinyLlama (~550MB) - Small but capable\n","llm_tinyllama = HuggingFacePipeline.from_model_id(\n","    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n","    )\n","\n","prompt = \"Hugging Face is\"\n","response = llm_tinyllama.invoke(prompt)\n","print(response)"],"metadata":{"id":"5SmNeVSkdsqs"},"execution_count":null,"outputs":[],"id":"5SmNeVSkdsqs"},{"cell_type":"code","source":["# 4. Pythia-160M (~320MB) - EleutherAI's smallest model\n","llm_pythia = HuggingFacePipeline.from_model_id(\n","    model_id=\"EleutherAI/pythia-160m\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n",")\n","\n","prompt = \"Hugging Face is\"\n","response = llm_pythia.invoke(prompt)\n","print(response)"],"metadata":{"id":"h7n54HEzd5Xt"},"execution_count":null,"outputs":[],"id":"h7n54HEzd5Xt"},{"cell_type":"code","source":["# 5. OPT-125M (~250MB) - Meta's small model\n","llm_opt = HuggingFacePipeline.from_model_id(\n","    model_id=\"facebook/opt-125m\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n","    )\n","\n","prompt = \"Hugging Face is\"\n","response = llm_opt.invoke(prompt)\n","print(response)"],"metadata":{"id":"HMUTNXW0eBTD"},"execution_count":null,"outputs":[],"id":"HMUTNXW0eBTD"},{"cell_type":"code","source":["# 6. DialoGPT-small (~117MB) - Good for conversational tasks\n","llm_dialogpt = HuggingFacePipeline.from_model_id(\n","    model_id=\"microsoft/DialoGPT-small\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n","    )\n","\n","prompt = \"Hugging Face is\"\n","response = llm_dialogpt.invoke(prompt)\n","print(response)"],"metadata":{"id":"ZOAh163ieH2E"},"execution_count":null,"outputs":[],"id":"ZOAh163ieH2E"},{"cell_type":"code","source":["# 7. Phi-1.5 (~1.4GB) - Microsoft's efficient model (use with caution on free tier)\n","llm_phi = HuggingFacePipeline.from_model_id(\n","    model_id=\"microsoft/phi-1_5\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 20}\n",")\n","\n","# prompt = \"Hugging Face is\"\n","prompt = \"what is the capital of india?\"\n","response = llm_phi.invoke(prompt)\n","print(response)"],"metadata":{"id":"C8S4cw9rePlk"},"execution_count":null,"outputs":[],"id":"C8S4cw9rePlk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXQorxAsSFsV"},"outputs":[],"source":["# Import the class for defining Hugging Face pipelines\n","from langchain_huggingface import HuggingFacePipeline\n","\n","# Define the LLM from the Hugging Face model ID\n","llm = HuggingFacePipeline.from_model_id(\n","    model_id=\"crumb/nano-mistral\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\n","        \"max_new_tokens\": 20\n","        }\n","    )\n","\n","prompt = \"Hugging Face is\"\n","response = llm.invoke(prompt)\n","print(response)"],"id":"zXQorxAsSFsV"},{"cell_type":"markdown","source":["```python\n","llm = HuggingFacePipeline.from_model_id(\n","    model_id=\"crumb/nano-mistral\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\n","        \"max_new_tokens\": 20,       # Limits the length of generated text\n","        \"temperature\": 0.7,         # Controls randomness (higher values increase creativity)\n","        \"top_k\": 50,                # Limits sampling to top-k most probable tokens\n","        \"top_p\": 0.9,               # Uses nucleus sampling for diverse responses\n","        \"repetition_penalty\": 1.2,  # Reduces repetitive outputs\n","        \"do_sample\": True,          # Enables sampling instead of greedy decoding\n","        \"num_return_sequences\": 3,  # Generates multiple variations of output\n","        \"early_stopping\": True      # Stops generation when an end condition is met\n","    }\n",")\n","```"],"metadata":{"id":"7GvP-2eSft6D"},"id":"7GvP-2eSft6D"},{"cell_type":"markdown","id":"d72002e0","metadata":{"id":"d72002e0"},"source":["## 🤖 Prompting OpenAI Models\n","\n","LangChain simplifies interfacing with OpenAI's language models. The `ChatOpenAI` wrapper allows for:\n","- Model configuration (e.g., GPT-4o-mini).\n","- Controlling response generation using parameters like temperature and max_tokens.\n","- Simple `.invoke()` calls to generate responses.\n","\n","This abstraction makes it easy to switch models or change behavior without altering core logic."]},{"cell_type":"code","execution_count":null,"id":"42def881","metadata":{"id":"42def881"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\",\n","    api_key=\"your_openai_api_key_here\"\n",")\n","\n","response = llm.invoke(\"What is LangChain?\")\n","print(response)\n"]},{"cell_type":"markdown","id":"2c5df3a0","metadata":{"id":"2c5df3a0"},"source":["## 🤗 Prompting Hugging Face Models\n","\n","Hugging Face offers open-source models that can be used through LangChain with the `HuggingFacePipeline`. It allows:\n","- Model loading via model_id (e.g., LLaMA-3).\n","- Specifying generation tasks (e.g., text-generation).\n","- Tuning outputs with arguments like `max_new_tokens`.\n","\n","This is valuable for offline use cases or where you need fine-grained control over the model and infrastructure."]},{"cell_type":"code","execution_count":null,"id":"2c0c95fe","metadata":{"id":"2c0c95fe"},"outputs":[],"source":["from langchain_huggingface import HuggingFacePipeline\n","\n","llm = HuggingFacePipeline.from_model_id(\n","    model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n","    task=\"text-generation\",\n","    pipeline_kwargs={\"max_new_tokens\": 100}\n",")\n","\n","response = llm.invoke(\"What is Hugging Face?\")\n","print(response)"]},{"cell_type":"markdown","id":"08958c05","metadata":{"id":"08958c05"},"source":["## 🧾 Prompt Templates\n","Prompt templates provide structure and clarity when building prompts. Instead of hardcoding input strings, templates use variables that are filled at runtime.\n","\n","Benefits:\n","- Easy to update and debug.\n","- Can be reused across chains or components.\n","- Makes dynamic prompt construction systematic.\n","\n","Templates may include system messages, instructions, context, and examples depending on the use case."]},{"cell_type":"code","execution_count":null,"id":"c591b7f8","metadata":{"id":"c591b7f8"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate\n","\n","template = \"Explain this concept simply and concisely: {concept}\"\n","prompt_template = PromptTemplate.from_template(template)\n","\n","prompt = prompt_template.invoke({\"concept\": \"Prompting LLMs\"})\n","print(prompt.text)\n","\n","# Composing with an LLM\n","llm_chain = prompt_template | llm\n","concept = \"Prompting LLMs\"\n","print(llm_chain.invoke({\"concept\": concept}))"]},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n","\n","prompt = PromptTemplate.from_template(\n","    template=template\n","    )\n","\n","llm = ChatOpenAI(\n","    model = \"gpt-4o-mini\",\n","    api_key = \"OPENAI_API_TOKEN\"\n",")\n","\n","llm_chain = prompt | llm\n","\n","question = \"How does Langchain make the LLM application development easier\"\n","print(llm_chain.invoke({\"question\": question}))"],"metadata":{"id":"Z8SsUgZtWPcg"},"id":"Z8SsUgZtWPcg","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f0de616d","metadata":{"id":"f0de616d"},"source":["## 🗣️ Chat Models with Roles\n","LangChain supports chat-based interactions using role-based messaging:\n","- **System**: Sets the tone and purpose of the conversation.\n","- **Human**: Represents user inputs.\n","- **AI**: Shows model responses.\n","\n","This format allows for rich multi-turn conversations. ChatPromptTemplate helps define this dialogue structure in a maintainable format."]},{"cell_type":"code","execution_count":null,"id":"3212ade8","metadata":{"id":"3212ade8"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a calculator that responds with math.\"),\n","    (\"human\", \"Answer this math question: What is two plus two?\"),\n","    (\"ai\", \"2+2=4\"),\n","    (\"human\", \"Answer this math question: {math}\")\n","])\n","\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"your_openai_api_key_here\")\n","llm_chain = template | llm\n","\n","response = llm_chain.invoke({\"math\": \"What is five times five?\"})\n","print(response.content)"]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Create a chat prompt template\n","prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a geography expert that returns the colors present in a country's flag.\"),\n","        (\"human\", \"France\"),\n","        (\"ai\", \"blue, white, red\"),\n","        (\"human\", \"{country}\")\n","    ]\n",")\n","\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>')\n","\n","\n","# Chain the prompt template and model, and invoke the chain\n","llm_chain = prompt_template | llm\n","\n","country = \"Japan\"\n","response = llm_chain.invoke({\"country\": country})\n","print(response.content)"],"metadata":{"id":"H7A0xToFYnW8"},"id":"H7A0xToFYnW8","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8a0364e9","metadata":{"id":"8a0364e9"},"source":["## 🔂 Few-Shot Prompting\n","Few-shot prompting allows LLMs to generalize by showing examples in the prompt itself. LangChain facilitates this with `FewShotPromptTemplate`, which can:\n","- Format multiple question-answer pairs.\n","- Append suffixes to guide the model.\n","- Accept examples from dataframes or lists.\n","\n","This method is ideal when fine-tuning is not possible but the task needs clear contextual signals."]},{"cell_type":"code","execution_count":null,"id":"71b25bd2","metadata":{"id":"71b25bd2"},"outputs":[],"source":["from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n","\n","examples = [\n","    {\"question\": \"Does Henry Campbell have any pets?\",\n","     \"answer\": \"Henry Campbell has a dog called Pluto.\"}\n","]\n","\n","example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n","\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    suffix=\"Question: {input}\",\n","    input_variables=[\"input\"]\n",")\n","\n","prompt = few_shot_prompt.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})\n","print(prompt.text)"]},{"cell_type":"code","source":["from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n","\n","\n","# Create the examples list of dicts\n","examples = [\n","    {\n","    \"question\": \"What is the capital city of Japan?\",\n","    \"answer\": \"Tokyo\"\n","        },\n","    {\n","    \"question\": \"Who developed the theory of relativity?\",\n","    \"answer\": \"Albert Einstein\"\n","        },\n","    {\n","    \"question\": \"What is the chemical symbol for gold?\",\n","    \"answer\": \"Au\"\n","    }\n","]\n","\n","# Complete the prompt for formatting answers\n","example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n","\n","# create the few-shot prompt\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    suffix=\"Question: {input}\",\n","    input_variables=[\"input\"]\n",")\n","\n","prompt = few_shot_prompt.invoke({\"input\": \"What is the chemical symbol for Silver?\"})\n","print(prompt.text)"],"metadata":{"id":"kXE_U7JW3yng"},"id":"kXE_U7JW3yng","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"425cfcf5","metadata":{"id":"425cfcf5"},"source":["## 🔗 Integrating Few-Shot with Chain\n","By combining prompt templates and LLMs in a pipeline, LangChain lets you build complete applications. For example:\n","- A few-shot prompt template receives input.\n","- It gets formatted with relevant examples.\n","- The model generates an output.\n","- All of this can be connected into a single callable `llm_chain`.\n","\n","This modular pipeline makes testing, evaluation, and reuse easier across projects."]},{"cell_type":"code","execution_count":null,"id":"1728fe92","metadata":{"id":"1728fe92"},"outputs":[],"source":["llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"your_openai_api_key_here\")\n","llm_chain = few_shot_prompt | llm\n","\n","response = llm_chain.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})\n","print(response.content)"]},{"cell_type":"markdown","id":"8690ed37","metadata":{"id":"8690ed37"},"source":["## ✅ Summary"]},{"cell_type":"code","execution_count":null,"id":"e06108df","metadata":{"id":"e06108df"},"outputs":[],"source":["# Try a custom question with the existing few-shot prompt chain\n","your_input = \"Explain reinforcement learning in simple terms.\"\n","response = llm_chain.invoke({\"input\": your_input})\n","print(response.content)"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["8qC3eUqMg8WP"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}