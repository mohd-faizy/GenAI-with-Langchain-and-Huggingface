{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3626325",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) with LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdcaa2",
   "metadata": {},
   "source": [
    "## ‚≠ïIntroduction to RAG\n",
    "\n",
    "### LLM Limitation: Knowledge Constraints\n",
    "Large Language Models (LLMs) are limited by the data they were trained on. They cannot dynamically pull in real-time or external knowledge.\n",
    "\n",
    "### What is Retrieval-Augmented Generation?\n",
    "RAG integrates external data sources with LLMs to overcome this limitation. It retrieves relevant documents or information based on user queries and uses that as context for LLMs to generate responses.\n",
    "\n",
    "### Standard RAG Workflow\n",
    "1. **User Query Input**\n",
    "2. **Retriever fetches relevant documents** from vector store\n",
    "3. **Context + Query is passed to the LLM**\n",
    "4. **LLM generates answer** using retrieved context\n",
    "\n",
    "### Preparing Data for Retrieval\n",
    "To use RAG effectively, the documents must be ingested, split into manageable chunks, embedded, and stored in a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d0b0d",
   "metadata": {},
   "source": [
    "## ‚≠ê Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e233e4",
   "metadata": {},
   "source": [
    "LangChain provides loaders for various file formats.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    DirectoryLoader,\n",
    "    PyPDFLoader,\n",
    "    PDFPlumberLoader,\n",
    "    PyMuPDFLoader,\n",
    "    PDFMinerLoader,\n",
    "    WebBaseLoader,\n",
    "    UnstructuredURLLoader,\n",
    "    RecursiveURLLoader,\n",
    "    SitemapLoader,\n",
    "    S3DirectoryLoader,\n",
    "    AzureBlobStorageLoader,\n",
    "    GoogleDriveLoader,\n",
    "    ArxivLoader,\n",
    "    YoutubeAudioLoader,\n",
    "    NotionDirectoryLoader\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  country: united states\n",
      "confederation: concacaf\n",
      "population_share: 4.5\n",
      "tv_audience_share: 4.3\n",
      "gdp_weighted_share: 11.3 \n",
      "\n",
      "Metadata: {'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\fifa_countries_audience.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "path_to_csv =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\fifa_countries_audience.csv\"\n",
    "# Load the CSV file using the CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path= path_to_csv)\n",
    "documents = csv_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2c09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  Retrieval Argument Generation: Enhancing Language Model \n",
      " Capabilities Through External Knowledge Integration \n",
      " 1. Introduction to Retrieval Argument Generation (RAG) \n",
      " Retrieval-Augmented Generation (RAG) represents a paradigm shift in how large \n",
      " language models (LLMs) operate, moving beyond the constraints of their pre-trained \n",
      " knowledge by incorporating information from external, authoritative knowledge bases \n",
      " during the response generation process.  1  This  technique fundamentally optimizes the \n",
      " output of LLMs, ensuring that the generated content is not solely reliant on the \n",
      " model's internal parameters but is also grounded in a broader, often more current and \n",
      " specific, set of information.  1  In the realm of natural  language processing (NLP), RAG \n",
      " serves as a powerful tool to enhance text generation by seamlessly integrating data \n",
      " from diverse knowledge repositories, including databases, digital asset libraries, and \n",
      " comprehensive document repositories.  3  This architectural  pattern within generative AI \n",
      " is specifically designed to elevate the accuracy and relevance of LLM responses by \n",
      " dynamically retrieving pertinent external data precisely when a user issues a prompt.  4 \n",
      " At its core, RAG is an advanced artificial intelligence (AI) technique that masterfully \n",
      " combines the strengths of information retrieval and text generation.  5  This synergistic \n",
      " approach empowers AI models to access and retrieve relevant information from a \n",
      " multitude of knowledge sources and subsequently incorporate this retrieved \n",
      " information directly into the text they generate.  5  Functioning  as an AI framework, \n",
      " RAG's primary aim is to ground LLMs on the most accurate and up-to-date \n",
      " information available within an external knowledge base.  7  This not only improves the \n",
      " factual correctness of the generated content but also provides users with valuable \n",
      " insight into the generative process undertaken by the LLM.  7  By modifying the \n",
      " standard interaction with an LLM, RAG ensures that the model's responses are \n",
      " formulated with direct reference to a specified set of documents, effectively \n",
      " supplementing the information gleaned from its initial training data.  8  This capability is \n",
      " particularly significant as it allows LLMs to leverage domain-specific and recently \n",
      " updated information without requiring a complete model retraining.  8  As a technique, \n",
      " RAG is instrumental in enhancing the overall accuracy and reliability of generative AI \n",
      " models by equipping them with the ability to draw upon specific and relevant data \n",
      " sources.  9 \n",
      " From a machine learning perspective, Retrieval Augmented Generation is a \n",
      " sophisticated technique that harmoniously blends retrieval-based methodologies with \n",
      " generative models.  10  Its application is particularly  prominent within Natural Language \n",
      " Processing (NLP), where it serves to significantly enhance the capabilities of large \n",
      "\n",
      "Metadata: {'producer': 'Skia/PDF m135', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0', 'creationdate': '2025-04-15T19:09:13+00:00', 'title': 'RAG: Definition and Applications - Google Docs', 'moddate': '2025-04-15T19:09:13+00:00', 'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\RAG.pdf', 'total_pages': 46, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path_to_pdf =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(file_path= path_to_pdf)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "path_to_html = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\white_house_executive_order_nov_2023.html\"\n",
    "\n",
    "html_loader = UnstructuredHTMLLoader(file_path=path_to_html, encoding='utf-8')\n",
    "documents = html_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b43091",
   "metadata": {},
   "source": [
    "## ‚≠êText Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d4b9a",
   "metadata": {},
   "source": [
    "Split large documents into smaller chunks for effective embedding and retrieval.\n",
    "\n",
    "```python \n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    "    HTMLTextSplitter,\n",
    "    LatexTextSplitter,\n",
    "    JSONTextSplitter\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c01930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 323, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is a fascinating field.\\n    It involves algorithms and models that can learn from data.\\n    These models can then make predictions or decisions without \\n    being explicitly programmed to perform the task.\\n    This capability is increasingly valuable in \\n    various industries, from finance to healthcare.', 'There are many types of machine learning, \\n    including supervised, unsupervised, and reinforcement learning.\\n    Each type has its own \\n    strengths and applications.']\n",
      "[323, 169]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"Machine learning is a fascinating field.\n",
    "    It involves algorithms and models that can learn from data.\n",
    "    These models can then make predictions or decisions without \n",
    "    being explicitly programmed to perform the task.\n",
    "    This capability is increasingly valuable in \n",
    "    various industries, from finance to healthcare.\n",
    "\n",
    "    There are many types of machine learning, \n",
    "    including supervised, unsupervised, and reinforcement learning.\n",
    "    Each type has its own \n",
    "    strengths and applications.\"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30e8ae",
   "metadata": {},
   "source": [
    "- `\"\\n\\n\"` (Double Newline) ‚Äì> First, the text is split at paragraph breaks (double newlines), keeping sections intact.\n",
    "- `\"\\n\"` (Single Newline) ‚Äì> If chunks are still too large, the splitter moves to sentence-level splitting.\n",
    "- `\" \"` (Space) ‚Äì> If the previous splits are insufficient, it breaks at word boundaries.\n",
    "- `\"\"` (Empty String) ‚Äì> As a last resort, it splits character-by-character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62f77dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is a fascinating field.', 'It involves algorithms and models that can learn from data.', 'These models can then make predictions or decisions without', 'being explicitly programmed to perform the task.', 'This capability is increasingly valuable in', 'various industries, from finance to healthcare.', 'There are many types of machine learning,', 'including supervised, unsupervised, and reinforcement learning.\\n    Each type has its own', 'strengths and applications.']\n",
      "[40, 59, 59, 48, 43, 47, 41, 89, 27]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path_to_pdf =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=path_to_pdf)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(chunks)\n",
    "print([len(chunk.page_content) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b9f1",
   "metadata": {},
   "source": [
    "## ‚≠êEmbedding and Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7f78a",
   "metadata": {},
   "source": [
    "Embedding represents chunks in vector form to enable similarity search. LangChain supports OpenAI and ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c6bcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model (Google's embedding model)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Assume `chunks` is a list of documents (strings or LangChain Document objects)\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ce6fa",
   "metadata": {},
   "source": [
    "## ‚≠êBuilding LCEL Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62272276",
   "metadata": {},
   "source": [
    "LangChain Expression Language (LCEL) allows declarative pipeline construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6344df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0339092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, say that you don't know.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3a07244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Gemini Flash (chat model)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Replace with your desired model variant\n",
    "    max_output_tokens=50,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a439e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Gemini Flash (chat model)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Replace with your desired model variant\n",
    "    max_output_tokens=50,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f125f4",
   "metadata": {},
   "source": [
    "# üß© Full RAG Pipeline (Google + Chroma + PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8669b",
   "metadata": {},
   "source": [
    "Full working code for a complete Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "\n",
    "‚úÖ PyPDFLoader to load a PDF\n",
    "\n",
    "‚úÖ RecursiveCharacterTextSplitter to split text into chunks\n",
    "\n",
    "‚úÖ GoogleGenerativeAIEmbeddings for embedding text\n",
    "\n",
    "‚úÖ Chroma vector store to store and retrieve chunks\n",
    "\n",
    "‚úÖ Gemini 1.5 Flash as the LLM\n",
    "\n",
    "‚úÖ A prompt + LangChain chain to handle queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[945, 943, 935, 614, 978, 916, 981, 788, 940, 986, 949, 425, 993, 978, 991, 377, 939, 986, 936, 286, 919, 970, 978, 522, 976, 976, 939, 353, 971, 930, 973, 383, 976, 935, 963, 420, 954, 949, 921, 529, 959, 938, 921, 376, 999, 986, 976, 301, 963, 942, 952, 499, 958, 941, 956, 385, 942, 958, 945, 339, 986, 951, 924, 345, 988, 930, 951, 273, 923, 990, 993, 212, 946, 996, 949, 399, 968, 944, 960, 392, 928, 970, 937, 511, 933, 941, 986, 213, 973, 924, 963, 386, 977, 943, 935, 447, 992, 921, 978, 194, 931, 959, 975, 416, 977, 932, 966, 357, 945, 929, 992, 397, 949, 928, 987, 386, 956, 983, 963, 254, 959, 929, 937, 255, 999, 975, 951, 264, 987, 978, 947, 377, 966, 969, 976, 248, 987, 938, 964, 333, 917, 984, 972, 933, 981, 942, 539, 916, 957, 929, 409, 871, 981, 669, 984, 918, 974, 538, 947, 947, 920, 417, 994, 941, 961, 341, 997, 963, 996, 518, 969, 991, 993, 946, 929, 883]\n",
      "\n",
      "üìÑ Answer:\n",
      "Based on the provided text, Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models by integrating external knowledge into the response generation process.  It's particularly valuable when access to up-to-date and factual information is critical.  The text contrasts RAG with prompt engineering, noting that RAG uses an underlying parameter or knowledge base, while prompt engineering focuses on guiding the model's inherent abilities.  The two techniques are often used synergistically in advanced AI applications.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load environment variables (make sure GOOGLE_API_KEY is in your .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load PDF and split into chunks\n",
    "# -------------------------------\n",
    "path_to_pdf = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=path_to_pdf)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into overlapping chunks (1000 characters, with 200 overlap)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Optional: Check chunk sizes\n",
    "print([len(chunk.page_content) for chunk in chunks])\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create embeddings + vector store\n",
    "# -------------------------------\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create retriever from vector store\n",
    "# -------------------------------\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Set up prompt template\n",
    "# -------------------------------\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say you don't know ‚Äî don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Set up Gemini LLM\n",
    "# -------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    max_output_tokens=512,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Build the RAG chain\n",
    "# -------------------------------\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Ask a question\n",
    "# -------------------------------\n",
    "question = \"What is retrieval-augmented generation (RAG)?\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(\"\\nüìÑ Answer:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
