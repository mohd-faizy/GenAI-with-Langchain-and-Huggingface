{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3626325",
   "metadata": {},
   "source": [
    "# **üî∑üî∑Retrieval-Augmented Generation (RAG) with LangChainüî∑üî∑**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdcaa2",
   "metadata": {},
   "source": [
    "## **‚≠ê01: Introduction to RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306919a2",
   "metadata": {},
   "source": [
    "### LLM Limitation: Knowledge Constraints\n",
    "Large Language Models (LLMs) are limited by the data they were trained on. They cannot dynamically pull in real-time or external knowledge.\n",
    "\n",
    "### What is Retrieval-Augmented Generation?\n",
    "RAG integrates external data sources with LLMs to overcome this limitation. It retrieves relevant documents or information based on user queries and uses that as context for LLMs to generate responses.\n",
    "\n",
    "### Standard RAG Workflow\n",
    "1. **User Query Input**\n",
    "2. **Retriever fetches relevant documents** from vector store\n",
    "3. **Context + Query is passed to the LLM**\n",
    "4. **LLM generates answer** using retrieved context\n",
    "\n",
    "### Preparing Data for Retrieval\n",
    "To use RAG effectively, the documents must be ingested, split into manageable chunks, embedded, and stored in a vector database.\n",
    "\n",
    "![img_2](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0402.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d0b0d",
   "metadata": {},
   "source": [
    "## **‚≠ê02: Document Loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e233e4",
   "metadata": {},
   "source": [
    "LangChain provides loaders for various file formats.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    DirectoryLoader,\n",
    "    PyPDFLoader,\n",
    "    PDFPlumberLoader,\n",
    "    PyMuPDFLoader,\n",
    "    PDFMinerLoader,\n",
    "    WebBaseLoader,\n",
    "    UnstructuredURLLoader,\n",
    "    RecursiveURLLoader,\n",
    "    SitemapLoader,\n",
    "    S3DirectoryLoader,\n",
    "    AzureBlobStorageLoader,\n",
    "    GoogleDriveLoader,\n",
    "    ArxivLoader,\n",
    "    YoutubeAudioLoader,\n",
    "    NotionDirectoryLoader\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  country: united states\n",
      "confederation: concacaf\n",
      "population_share: 4.5\n",
      "tv_audience_share: 4.3\n",
      "gdp_weighted_share: 11.3 \n",
      "\n",
      "Metadata: {'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\fifa_countries_audience.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "path_to_csv =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\fifa_countries_audience.csv\"\n",
    "# Load the CSV file using the CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path= path_to_csv)\n",
    "documents = csv_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2c09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:  Retrieval Argument Generation: Enhancing Language Model \n",
      " Capabilities Through External Knowledge Integration \n",
      " 1. Introduction to Retrieval Argument Generation (RAG) \n",
      " Retrieval-Augmented Generation (RAG) represents a paradigm shift in how large \n",
      " language models (LLMs) operate, moving beyond the constraints of their pre-trained \n",
      " knowledge by incorporating information from external, authoritative knowledge bases \n",
      " during the response generation process.  1  This  technique fundamentally optimizes the \n",
      " output of LLMs, ensuring that the generated content is not solely reliant on the \n",
      " model's internal parameters but is also grounded in a broader, often more current and \n",
      " specific, set of information.  1  In the realm of natural  language processing (NLP), RAG \n",
      " serves as a powerful tool to enhance text generation by seamlessly integrating data \n",
      " from diverse knowledge repositories, including databases, digital asset libraries, and \n",
      " comprehensive document repositories.  3  This architectural  pattern within generative AI \n",
      " is specifically designed to elevate the accuracy and relevance of LLM responses by \n",
      " dynamically retrieving pertinent external data precisely when a user issues a prompt.  4 \n",
      " At its core, RAG is an advanced artificial intelligence (AI) technique that masterfully \n",
      " combines the strengths of information retrieval and text generation.  5  This synergistic \n",
      " approach empowers AI models to access and retrieve relevant information from a \n",
      " multitude of knowledge sources and subsequently incorporate this retrieved \n",
      " information directly into the text they generate.  5  Functioning  as an AI framework, \n",
      " RAG's primary aim is to ground LLMs on the most accurate and up-to-date \n",
      " information available within an external knowledge base.  7  This not only improves the \n",
      " factual correctness of the generated content but also provides users with valuable \n",
      " insight into the generative process undertaken by the LLM.  7  By modifying the \n",
      " standard interaction with an LLM, RAG ensures that the model's responses are \n",
      " formulated with direct reference to a specified set of documents, effectively \n",
      " supplementing the information gleaned from its initial training data.  8  This capability is \n",
      " particularly significant as it allows LLMs to leverage domain-specific and recently \n",
      " updated information without requiring a complete model retraining.  8  As a technique, \n",
      " RAG is instrumental in enhancing the overall accuracy and reliability of generative AI \n",
      " models by equipping them with the ability to draw upon specific and relevant data \n",
      " sources.  9 \n",
      " From a machine learning perspective, Retrieval Augmented Generation is a \n",
      " sophisticated technique that harmoniously blends retrieval-based methodologies with \n",
      " generative models.  10  Its application is particularly  prominent within Natural Language \n",
      " Processing (NLP), where it serves to significantly enhance the capabilities of large \n",
      "\n",
      "Metadata: {'producer': 'Skia/PDF m135', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0', 'creationdate': '2025-04-15T19:09:13+00:00', 'title': 'RAG: Definition and Applications - Google Docs', 'moddate': '2025-04-15T19:09:13+00:00', 'source': 'E:\\\\01_Github_Repo\\\\GenAI-with-Langchain-and-Huggingface\\\\_Developing_LLMs_Applications_with_LangChain\\\\_data\\\\RAG.pdf', 'total_pages': 46, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path_to_pdf =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(file_path= path_to_pdf)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "path_to_html = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\white_house_executive_order_nov_2023.html\"\n",
    "\n",
    "html_loader = UnstructuredHTMLLoader(file_path=path_to_html, encoding='utf-8')\n",
    "documents = html_loader.load()\n",
    "\n",
    "print(\"Content: \", documents[0].page_content, \"\\n\")\n",
    "print(\"Metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b43091",
   "metadata": {},
   "source": [
    "## **‚≠ê03: Text Splitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d4b9a",
   "metadata": {},
   "source": [
    "Split large documents into smaller chunks for effective embedding and retrieval.\n",
    "\n",
    "```python \n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTextSplitter,\n",
    "    SpacyTextSplitter,\n",
    "    NLTKTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    "    HTMLTextSplitter,\n",
    "    LatexTextSplitter,\n",
    "    JSONTextSplitter\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c897d9",
   "metadata": {},
   "source": [
    "![img_3](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0403.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c01930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 323, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is a fascinating field.\\n    It involves algorithms and models that can learn from data.\\n    These models can then make predictions or decisions without \\n    being explicitly programmed to perform the task.\\n    This capability is increasingly valuable in \\n    various industries, from finance to healthcare.', 'There are many types of machine learning, \\n    including supervised, unsupervised, and reinforcement learning.\\n    Each type has its own \\n    strengths and applications.']\n",
      "[323, 169]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"Machine learning is a fascinating field.\n",
    "    It involves algorithms and models that can learn from data.\n",
    "    These models can then make predictions or decisions without \n",
    "    being explicitly programmed to perform the task.\n",
    "    This capability is increasingly valuable in \n",
    "    various industries, from finance to healthcare.\n",
    "\n",
    "    There are many types of machine learning, \n",
    "    including supervised, unsupervised, and reinforcement learning.\n",
    "    Each type has its own \n",
    "    strengths and applications.\"\"\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30e8ae",
   "metadata": {},
   "source": [
    "- `\"\\n\\n\"` (Double Newline) ‚Äì> First, the text is split at paragraph breaks (double newlines), keeping sections intact.\n",
    "- `\"\\n\"` (Single Newline) ‚Äì> If chunks are still too large, the splitter moves to sentence-level splitting.\n",
    "- `\" \"` (Space) ‚Äì> If the previous splits are insufficient, it breaks at word boundaries.\n",
    "- `\"\"` (Empty String) ‚Äì> As a last resort, it splits character-by-character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f77dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is a fascinating field.', 'It involves algorithms and models that can learn from data.', 'These models can then make predictions or decisions without', 'being explicitly programmed to perform the task.', 'This capability is increasingly valuable in', 'various industries, from finance to healthcare.', 'There are many types of machine learning,', 'including supervised, unsupervised, and reinforcement learning.\\n    Each type has its own', 'strengths and applications.']\n",
      "[40, 59, 59, 48, 43, 47, 41, 89, 27]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10\n",
    "    )\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path_to_pdf =  r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=path_to_pdf)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(chunks)\n",
    "print([len(chunk.page_content) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b9f1",
   "metadata": {},
   "source": [
    "## **‚≠ê04: Embedding and Storage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7f78a",
   "metadata": {},
   "source": [
    "Embedding represents chunks in vector form to enable similarity search. LangChain supports OpenAI and ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fd93a",
   "metadata": {},
   "source": [
    "![img_1](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0401.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c6bcf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model (Google's embedding model)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Assume `chunks` is a list of documents (strings or LangChain Document objects)\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ce6fa",
   "metadata": {},
   "source": [
    "## **‚≠ê05: Building LCEL Retrieval Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62272276",
   "metadata": {},
   "source": [
    "LangChain Expression Language (LCEL) allows declarative pipeline construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1505acf0",
   "metadata": {},
   "source": [
    "![img_4](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0404.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the vector store into a retriever\n",
    "# Uses similarity search\n",
    "# Returns the top 2 most relevant documents\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099c633",
   "metadata": {},
   "source": [
    "- This line creates a **retriever object** from a `vector_store`.\n",
    "- This is common in **LangChain** or **vector database** workflows for **retrieval-based applications** like RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "---\n",
    "\n",
    "- üîπ What is `vector_store.as_retriever()`?\n",
    "  - `as_retriever()` is a method that converts a vector store (e.g., `FAISS`, `Chroma`, `Pinecone`, etc.) into a **retriever object**. \n",
    "  - A retriever is used to **fetch relevant documents** based on a query vector ‚Äî typically derived from user input.\n",
    "\n",
    "---\n",
    "\n",
    "- üîπ `search_type=\"similarity\"`\n",
    "  - This defines the **type of search** the retriever will perform.\n",
    "\n",
    "  - **Common `search_type` values (varies by implementation):**\n",
    "    | Search Type                    | Description                                                                                              |\n",
    "    | ------------------------------ | -------------------------------------------------------------------------------------------------------- |\n",
    "    | `\"similarity\"`                 | Retrieves documents most similar to the query vector using cosine similarity or another distance metric. |\n",
    "    | `\"mmr\"`                        | Maximal Marginal Relevance ‚Äî balances similarity and diversity in retrieved results.                     |\n",
    "    | `\"similarity_score_threshold\"` | Only returns results with a similarity score above a given threshold.                                    |\n",
    "    | `\"exact\"` or `\"filtered\"`      | Returns results that exactly match a condition. (Not available in all vector stores.)                    |\n",
    "\n",
    "    - The actual available options may depend on which vector store you‚Äôre using (e.g., `FAISS`, `Pinecone`, `Chroma`, `Weaviate`, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "- üîπ `search_kwargs={\"k\": 2}`\n",
    "  - This is a dictionary of **additional parameters** passed to the search method. Here:\n",
    "    -  `k` means **\"return the top-k most relevant results\"**\n",
    "    -  So `k=2` means it will return the **2 most similar documents** based on the query.\n",
    "    - Other possible `search_kwargs` (depending on the vector store):\n",
    "\n",
    "    | Key               | Description                                                               |\n",
    "    | ----------------- | ------------------------------------------------------------------------- |\n",
    "    | `k`               | Number of results to return.                                              |\n",
    "    | `score_threshold` | Only return documents with a similarity score above this threshold.       |\n",
    "    | `fetch_k`         | Total number of vectors to consider before filtering (used with MMR).     |\n",
    "    | `lambda_mult`     | Controls trade-off between similarity and diversity in MMR.               |\n",
    "    | `filter`          | Apply metadata filters (e.g., only retrieve documents with `topic=\"AI\"`). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0339092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, say that you don't know.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a439e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough   # Passes input through without modification\n",
    "from langchain_core.output_parsers import StrOutputParser  # Parses the LLM output into a string\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # Imports Gemini chat model wrapper\n",
    "\n",
    "# Initialize Gemini Flash (chat model) with specified parameters\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",         # Use the Gemini 1.5 Flash model\n",
    "    max_output_tokens=50,             # Limit output to 50 tokens\n",
    "    temperature=0.3                   # Low temperature for more deterministic output\n",
    ")\n",
    "\n",
    "# Create a chain: injects context and question into prompt, sends to LLM, then parses the response\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Prepare input dict with context and raw question\n",
    "    | prompt                                                   # Format input using a prompt template\n",
    "    | llm                                                      # Generate response using the Gemini model\n",
    "    | StrOutputParser()                                        # Extract and return the final string output\n",
    ")\n",
    "\n",
    "# Invoke the chain on the inputs provided\n",
    "print(chain.invoke({\n",
    "    \"context\": \"The first image of a black hole was captured by the Event Horizon Telescope in 2019.\",\n",
    "    \"question\": \"When was the first image of a black hole captured?\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f125f4",
   "metadata": {},
   "source": [
    "# üß© ***Full RAG Pipeline*** (`Google` + `Chroma` + `PDF`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8669b",
   "metadata": {},
   "source": [
    "Full working code for a complete Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "\n",
    "‚úÖ PyPDFLoader to load a PDF\n",
    "\n",
    "‚úÖ RecursiveCharacterTextSplitter to split text into chunks\n",
    "\n",
    "‚úÖ GoogleGenerativeAIEmbeddings for embedding text\n",
    "\n",
    "‚úÖ Chroma vector store to store and retrieve chunks\n",
    "\n",
    "‚úÖ Gemini 1.5 Flash as the LLM\n",
    "\n",
    "‚úÖ A prompt + LangChain chain to handle queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a99d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 176\n",
      "[945, 943, 935, 614, 978, 916, 981, 788, 940, 986, 949, 425, 993, 978, 991, 377, 939, 986, 936, 286, 919, 970, 978, 522, 976, 976, 939, 353, 971, 930, 973, 383, 976, 935, 963, 420, 954, 949, 921, 529, 959, 938, 921, 376, 999, 986, 976, 301, 963, 942, 952, 499, 958, 941, 956, 385, 942, 958, 945, 339, 986, 951, 924, 345, 988, 930, 951, 273, 923, 990, 993, 212, 946, 996, 949, 399, 968, 944, 960, 392, 928, 970, 937, 511, 933, 941, 986, 213, 973, 924, 963, 386, 977, 943, 935, 447, 992, 921, 978, 194, 931, 959, 975, 416, 977, 932, 966, 357, 945, 929, 992, 397, 949, 928, 987, 386, 956, 983, 963, 254, 959, 929, 937, 255, 999, 975, 951, 264, 987, 978, 947, 377, 966, 969, 976, 248, 987, 938, 964, 333, 917, 984, 972, 933, 981, 942, 539, 916, 957, 929, 409, 871, 981, 669, 984, 918, 974, 538, 947, 947, 920, 417, 994, 941, 961, 341, 997, 963, 996, 518, 969, 991, 993, 946, 929, 883]\n",
      "\n",
      "üìÑ Answer:\n",
      "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models by integrating external knowledge into the response generation process.  It is particularly valuable when access to up-to-date and factual information is critical.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# üì¶ Import required libraries\n",
    "# -------------------------------\n",
    "from dotenv import load_dotenv  # Loads environment variables from .env file\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.document_loaders import PyPDFLoader         # For loading PDF files\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # To split large text into manageable chunks\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI  # Embeddings & LLM from Google Gemini\n",
    "from langchain_chroma import Chroma                                  # Vector store for storing and retrieving embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate                # To format prompts to LLMs\n",
    "from langchain_core.runnables import RunnablePassthrough             # Utility for passing inputs unchanged in chain\n",
    "from langchain_core.output_parsers import StrOutputParser            # Converts LLM output to plain string\n",
    "\n",
    "# -------------------------------\n",
    "# üåê Load environment variables\n",
    "# -------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------------\n",
    "# üìÑ Step 1: Load and split PDF\n",
    "# -------------------------------\n",
    "path_to_pdf = r\"E:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\_Developing_LLMs_Applications_with_LangChain\\_data\\RAG.pdf\"\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(file_path=path_to_pdf)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the text into chunks (1000 characters each with 200 characters overlap)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Define how to split text\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    "    )\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Print number and size of chunks\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print([len(chunk.page_content) for chunk in chunks])\n",
    "\n",
    "# -------------------------------\n",
    "# üß† Step 2: Create embeddings & vector store\n",
    "# -------------------------------\n",
    "# Initialize Google embedding model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Store document chunks in a Chroma vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# üîç Step 3: Create retriever from vector store\n",
    "# -------------------------------\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",      # Use similarity search\n",
    "    search_kwargs={\"k\": 2}         # Return top 2 similar chunks\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# üìù Step 4: Create a prompt template\n",
    "# -------------------------------\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say you don't know ‚Äî don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# ü§ñ Step 5: Initialize Gemini LLM\n",
    "# -------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    max_output_tokens=512,\n",
    "    temperature=0.3  # Controls randomness in output (lower = more deterministic)\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# üîó Step 6: Build the RAG chain\n",
    "# -------------------------------\n",
    "# Chain execution: Question ‚Üí Pass-through ‚Üí Prompt ‚Üí LLM ‚Üí Output Parser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# ‚ùì Step 7: Ask a question\n",
    "# -------------------------------\n",
    "question = \"What is retrieval-augmented generation (RAG)?\"\n",
    "\n",
    "# Run the RAG chain with the input question\n",
    "response = chain.invoke(question)\n",
    "\n",
    "# -------------------------------\n",
    "# üì§ Output the response\n",
    "# -------------------------------\n",
    "print(\"\\nüìÑ Answer:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
