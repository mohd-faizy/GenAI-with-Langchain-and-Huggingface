{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "57f7b39f",
      "metadata": {
        "id": "57f7b39f"
      },
      "source": [
        "# **01: Sequential Chains**\n",
        "\n",
        "\n",
        "LangChain allows developers to build chains of prompts and models. A sequential chain connects multiple components where the output of one becomes the input of the next.\n",
        "\n",
        "This is useful when you want to perform multi-step operations, such as planning a trip itinerary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e2aee5",
      "metadata": {},
      "source": [
        "\n",
        "## AI-Powered Travel Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9670d24",
      "metadata": {
        "id": "e9670d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given you only have one day, here's a jam-packed itinerary focusing on three key experiences offering a blend of history, culture, and a taste of Delhi's energy:\n",
            "\n",
            "**Morning (Historical Grandeur):**\n",
            "\n",
            "* **8:00 AM - 11:00 AM: Red Fort (Lal Qila):** Start your day early at the magnificent Red Fort.  Allow at least 3 hours to explore this UNESCO World Heritage site thoroughly.  Consider hiring a guide for a richer experience (optional, but highly recommended).\n",
            "\n",
            "**Afternoon (Cultural Immersion & Relaxation):**\n",
            "\n",
            "* **11:30 AM - 1:30 PM:  Rickshaw Ride through Old Delhi & Jama Masjid:** Take a thrilling rickshaw ride through the bustling streets of Old Delhi to reach Jama Masjid.  Negotiate the rickshaw fare beforehand.  Spend some time admiring the architecture of Jama Masjid (remember respectful attire).\n",
            "\n",
            "* **1:30 PM - 2:30 PM: Lunch:** Enjoy a delicious and authentic Delhi street food lunch near Jama Masjid.  There are plenty of options, but choose carefully considering hygiene.\n",
            "\n",
            "* **2:30 PM - 4:30 PM: Humayun's Tomb:**  Escape the Old Delhi chaos with a visit to the serene Humayun's Tomb.  Its stunning architecture and peaceful atmosphere provide a welcome contrast.\n",
            "\n",
            "\n",
            "**Evening (Modern Delhi & Departure):**\n",
            "\n",
            "* **4:30 PM - 6:30 PM:  Khan Market:**  Head to Khan Market for some upscale shopping and a refreshing break.  You can grab a coffee or tea and browse the boutiques.\n",
            "\n",
            "* **6:30 PM onwards:  Dinner & Departure:**  Enjoy a final delicious Delhi dinner before heading to the airport or your accommodation for departure.  Consider trying a restaurant in Khan Market or another area convenient to your transportation.\n",
            "\n",
            "\n",
            "**Important Notes:**\n",
            "\n",
            "* **Transportation:** Utilize Delhi's Metro wherever possible to save time and navigate efficiently.  Rickshaws are best for Old Delhi's narrow lanes.\n",
            "* **Dress Code:** Dress modestly, especially when visiting religious sites like Jama Masjid.\n",
            "* **Bargaining:**  Bargaining is expected in markets like Chandni Chowk (if you choose to visit it instead of Khan Market).\n",
            "* **Hydration:** Carry water, especially during warmer months.\n",
            "* **Flexibility:** This itinerary is ambitious. Be prepared to adjust based on your energy levels and interests.\n",
            "\n",
            "\n",
            "This itinerary prioritizes maximizing your single day in Delhi with a powerful combination of iconic historical sites, a cultural immersion experience, and a taste of modern Delhi's vibrancy.  Remember to check opening times and plan travel times between locations accordingly.\n"
          ]
        }
      ],
      "source": [
        "# Import required components\n",
        "from langchain.prompts import PromptTemplate                      # For formatting prompts with variables\n",
        "from langchain_core.output_parsers import StrOutputParser         # To convert LLM output to plain strings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI         # Gemini model via LangChain\n",
        "from langchain_core.runnables import RunnableMap                  # To run intermediate steps in chains\n",
        "from dotenv import load_dotenv                                    # To load API keys from .env\n",
        "\n",
        "# Load environment variables (like Gemini API key)\n",
        "load_dotenv()\n",
        "\n",
        "# Prompt to get activity suggestions based on destination\n",
        "destination_prompt = PromptTemplate(\n",
        "    input_variables=[\"destination\"],\n",
        "    template=\"I am planning a trip to {destination}. Can you suggest some activities to do there?\"\n",
        ")\n",
        "\n",
        "# Prompt to generate itinerary from suggested activities\n",
        "activities_prompt = PromptTemplate(\n",
        "    input_variables=[\"activities\"],\n",
        "    template=\"I only have one day, so can you create an itinerary from your top three activities: {activities}.\"\n",
        ")\n",
        "\n",
        "# Initialize Gemini model with desired settings\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",          # Specify the model to use\n",
        "    # max_output_tokens=100,           # Optional: limit response length\n",
        "    temperature=0.7                    # Controls creativity of output\n",
        ")\n",
        "\n",
        "# Parser to extract clean string output from LLM\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Step 1: Generate activity list from destination\n",
        "step1 = destination_prompt | llm | parser\n",
        "\n",
        "# Step 2: Feed activities into itinerary prompt, then get final response\n",
        "seq_chain = RunnableMap({\n",
        "    \"activities\": step1               # Maps 'activities' input from step1 to next prompt\n",
        "}) | activities_prompt | llm | parser\n",
        "\n",
        "# Run the chain with an example input\n",
        "output = seq_chain.invoke({\"destination\": \"New Delhi\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3506e347",
      "metadata": {},
      "source": [
        "## Building Smart AI-Powered Learning Assistants (Personalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485ce9fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Personalized Machine Learning Study Plan (10 hours/week)\n",
            "\n",
            "This plan prioritizes hands-on projects and video lectures, leveraging the user's intermediate Python, PyTorch/TensorFlow, and math background.  The goal is portfolio building for a data science job.\n",
            "\n",
            "**Phase 1: Solidify Foundations & Practical Skills (Weeks 1-4)**\n",
            "\n",
            "* **Module 1: Regression & Classification (Week 1-2):**\n",
            "    * **Task 1 (2 hours):** Review linear algebra & calculus concepts relevant to ML (Khan Academy or 3Blue1Brown for focused review).\n",
            "    * **Task 2 (4 hours):** Complete DataCamp's introductory machine learning course focusing on regression and classification.  Focus on practical implementation in Python (Scikit-learn).\n",
            "    * **Task 3 (4 hours):**  Build a simple regression project (e.g., house price prediction using a publicly available dataset).  Use Scikit-learn, document your code thoroughly, and analyze results.\n",
            "\n",
            "* **Module 2:  Deep Learning Introduction (Week 3-4):**\n",
            "    * **Task 1 (3 hours):** Watch fast.ai's introductory deep learning videos (focus on practical coding aspects).\n",
            "    * **Task 2 (4 hours):**  Work through fast.ai's introductory notebooks, focusing on PyTorch or TensorFlow (choose one based on preference).\n",
            "    * **Task 3 (3 hours):** Build a simple image classification project using a pre-trained model (e.g., transfer learning with ResNet on CIFAR-10).  Track your experiments and results.\n",
            "\n",
            "\n",
            "**Phase 2:  Advanced Topics & Portfolio Expansion (Weeks 5-8)**\n",
            "\n",
            "* **Module 3:  Neural Network Architectures (Week 5-6):**\n",
            "    * **Task 1 (3 hours):** Watch lectures on specific neural network architectures (CNNs, RNNs) from resources like Stanford CS229 lectures (relevant sections).  Focus on understanding the architectures and their applications.\n",
            "    * **Task 2 (7 hours):** Build a project involving a more complex architecture.  Examples:  Image classification with a CNN from scratch (MNIST or a similar dataset), or a sentiment analysis project using RNNs (IMDB reviews).\n",
            "\n",
            "\n",
            "* **Module 4:  Advanced Techniques & Deployment (Week 7-8):**\n",
            "    * **Task 1 (4 hours):** Explore advanced techniques like regularization, hyperparameter tuning, and model evaluation in depth. Use resources like articles from Towards Data Science.\n",
            "    * **Task 2 (6 hours):** Choose one project from Phase 1 or 2 and deploy it using a simple framework (e.g., Flask, Streamlit). Create a basic web interface for user interaction.\n",
            "\n",
            "\n",
            "**Phase 3: Specialization & Portfolio Refinement (Weeks 9-12 and beyond)**\n",
            "\n",
            "* **Module 5: Specialization (Weeks 9-12):**\n",
            "    * **Task 1 (10 hours/week):** Choose a specialization area (e.g., Computer Vision, NLP, Time Series Analysis).  Select a relevant project from Kaggle or a similar platform.  Focus on delivering a high-quality, well-documented project.  Use relevant libraries and techniques learned throughout the plan.\n",
            "\n",
            "\n",
            "**Ongoing Activities:**\n",
            "\n",
            "* **Regularly review your code:**  Refactor and improve your code throughout the plan.\n",
            "* **Document your projects thoroughly:**  Create a GitHub repository to showcase your work.\n",
            "* **Network with other data scientists:**  Attend online meetups or join relevant communities.\n",
            "* **Contribute to open-source projects (optional):** This is a great way to build skills and demonstrate your abilities.\n",
            "\n",
            "\n",
            "**Resource Allocation:**\n",
            "\n",
            "* **DataCamp:** Primarily for structured learning in Phases 1 and 2.\n",
            "* **fast.ai:** Primarily for practical deep learning in Phases 1 and 2.\n",
            "* **Stanford CS229 (select lectures):** For deeper understanding of specific architectures in Phase 2.\n",
            "* **Towards Data Science & Analytics Vidhya:** For articles on advanced techniques and staying updated.\n",
            "* **Kaggle:** For finding projects and datasets.\n",
            "\n",
            "\n",
            "This plan is a suggestion and can be adjusted based on your progress and interests.  Remember to focus on building a strong portfolio that showcases your skills and abilities.  Consistent effort and a focus on practical application will be key to your success.\n"
          ]
        }
      ],
      "source": [
        "# Import required components\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnableMap\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (like Gemini API key)\n",
        "load_dotenv()\n",
        "\n",
        "# Prompt to get learning resources based on topic\n",
        "learning_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"I want to learn about {topic}. Can you suggest some resources like articles, videos, or courses?\"\n",
        ")\n",
        "\n",
        "# Prompt to generate a personalized study plan using resources + user info\n",
        "study_plan_prompt = PromptTemplate(\n",
        "    input_variables=[\n",
        "        \"resources\",\n",
        "        \"programming_experience\",\n",
        "        \"math_background\",\n",
        "        \"learning_style\",\n",
        "        \"learning_goals\",\n",
        "        \"time_commitment\"\n",
        "    ],\n",
        "    template=\"\"\"\n",
        "I have the following resources: {resources}\n",
        "\n",
        "Create a personalized study plan using these resources based on this user's information:\n",
        "\n",
        "1. Programming experience: {programming_experience}\n",
        "2. Math background: {math_background}\n",
        "3. Preferred learning style: {learning_style}\n",
        "4. Learning goals: {learning_goals}\n",
        "5. Time commitment per week: {time_commitment} hours\n",
        "\n",
        "Make the plan structured and practical. Include weeks/modules, brief tasks, and align them with the user's preferences and pace.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Output parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Step 1: Generate resource list from topic\n",
        "step1 = learning_prompt | llm | parser\n",
        "\n",
        "# Step 2: Personalized study plan using resources and user profile\n",
        "seq_chain = RunnableMap({\n",
        "    \"resources\": step1,\n",
        "    \"programming_experience\": lambda x: x[\"programming_experience\"],\n",
        "    \"math_background\": lambda x: x[\"math_background\"],\n",
        "    \"learning_style\": lambda x: x[\"learning_style\"],\n",
        "    \"learning_goals\": lambda x: x[\"learning_goals\"],\n",
        "    \"time_commitment\": lambda x: x[\"time_commitment\"],\n",
        "}) | study_plan_prompt | llm | parser\n",
        "\n",
        "# Example input from user\n",
        "user_input = {\n",
        "    \"topic\": \"Machine Learning\",\n",
        "    \"programming_experience\": \"Intermediate (Python, Pytorch, TensorFlow)\",\n",
        "    \"math_background\": \"Intermediate (Calculus, Linear Algebra)\",\n",
        "    \"learning_style\": \"Hands-on projects and video lectures\",\n",
        "    \"learning_goals\": \"Build a portfolio to get a job in data science\",\n",
        "    \"time_commitment\": \"10\"\n",
        "}\n",
        "\n",
        "# Run the chain\n",
        "output = seq_chain.invoke(user_input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635ecaad",
      "metadata": {
        "id": "635ecaad"
      },
      "source": [
        "# 02: LangChain Agents and ReAct\n",
        "\n",
        "* **Agents in LangChain**:\n",
        "\n",
        "  * Use LLMs to make decisions and perform actions by calling external tools.\n",
        "  * Enable dynamic workflows where the model can choose the next step based on the task.\n",
        "\n",
        "* **ReAct Pattern** (Reasoning + Acting):\n",
        "\n",
        "  * A framework where the LLM thinks through a problem step by step.\n",
        "  * Chooses a relevant tool to act with.\n",
        "  * Observes the tool's output.\n",
        "  * Continues reasoning based on the observed result.\n",
        "\n",
        "* **Flow of ReAct Agent**:\n",
        "\n",
        "  1. **Thought**: Model decides what to do next.\n",
        "  2. **Act**: Model selects and calls the appropriate tool with input.\n",
        "  3. **Observe**: Model receives and processes the tool's output.\n",
        "  4. **Repeat or respond**: Model may continue thinking and acting or provide the final answer.\n",
        "\n",
        "* **Example**:\n",
        "\n",
        "```\n",
        "# Thought: I should call Weather() to find the weather in Kingston, Jamaica.\n",
        "# Act: Weather(\"Kingston, Jamaica\")\n",
        "# Observe: The weather is mostly sunny with temperatures of 82Â°F.\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e942b60",
      "metadata": {
        "id": "6e942b60"
      },
      "source": [
        "# 03: LangGraph\n",
        "\n",
        "\n",
        "* ðŸ”„ **LangGraph** is a framework built on top of LangChain.\n",
        "* ðŸ§  It helps you create **stateful AI agents** â€” ones that remember and adapt as they go.\n",
        "* ðŸ”§ Based on **graphs**, where each node is a tool or step in the agent's logic.\n",
        "* ðŸ” Supports **loops and branching**, so the agent can revisit steps or make decisions.\n",
        "* ðŸ§± Makes building **complex workflows** with language models easier and more structured.\n",
        "* ðŸ¤ Works well with tools, memory, and custom logic for smarter agents.\n",
        "* âš¡ Good for tasks like **chatbots, planning agents, and multi-step reasoning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a233f6cb",
      "metadata": {},
      "source": [
        "## Building a Gemini-Based ReAct Agent for Math Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07134b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to perform the calculations in the parentheses first, then add the results.  I will use the calculator to help.\n",
            "Action: Calculator\n",
            "Action Input: (45 / 9) + (7 * 3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 26.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 26\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Final Answer: {'input': 'What is the result of (45 / 9) + (7 * 3)?', 'output': '26'}\n",
            "Agent Response: 26\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "# Load environment and setup\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             temperature=0.7)\n",
        "\n",
        "# Load math tool\n",
        "tools = load_tools([\"llm-math\"], llm=llm)\n",
        "\n",
        "# Create agent using Zero-shot ReAct (works well with Gemini)\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Ask a math question\n",
        "response = agent.invoke(\"What is the result of (45 / 9) + (7 * 3)?\")\n",
        "print(\"\\nFinal Answer:\", response)\n",
        "print(\"Agent Response:\", response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f803e2b9",
      "metadata": {
        "id": "f803e2b9"
      },
      "source": [
        "# 04: Creating Custom Tools\n",
        "\n",
        "\n",
        "LangChain allows you to define and integrate custom tools.\n",
        "\n",
        "These tools can be simple Python functions wrapped for agent use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b5147c",
      "metadata": {},
      "source": [
        "```python\n",
        "# Define a financial reporting function\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
        "\n",
        "    net_income = revenue - expenses\n",
        "\n",
        "    report = f\"Financial Report for {company_name}:\"\n",
        "    report += f\"Revenue: ${revenue}\"\n",
        "    report += f\"Expenses: ${expenses}\"\n",
        "    report += f\"Net Income: ${net_income}\"\n",
        "    return report\n",
        "\n",
        "# Test the function\n",
        "print(financial_report(company_name=\"LemonadeStand\", revenue=100, expenses=50))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93db1ac",
      "metadata": {
        "id": "c93db1ac"
      },
      "source": [
        "# 05: Wrapping Function as a Tool\n",
        "\n",
        "\n",
        "To use the function with an agent, wrap it with `@tool` decorator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c106ecba",
      "metadata": {},
      "source": [
        "```python\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
        "\n",
        "    net_income = revenue - expenses\n",
        "\n",
        "    report = f\"Financial Report for {company_name}:\"\n",
        "    report += f\"Revenue: ${revenue}\"\n",
        "    report += f\"Expenses: ${expenses}\"\n",
        "    report += f\"Net Income: ${net_income}\"\n",
        "    return report\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40892d0b",
      "metadata": {
        "id": "40892d0b"
      },
      "source": [
        "# 06: Integrating Custom Tool with Agent\n",
        "\n",
        "\n",
        "You can pass your custom tool to the ReAct agent and use it in a natural language query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5b40a8",
      "metadata": {},
      "source": [
        "```python\n",
        "agent = create_react_agent(llm, [financial_report])\n",
        "\n",
        "messages = agent.invoke({\n",
        "    \"messages\": [(\"human\", \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\")]\n",
        "})\n",
        "print(messages)\n",
        "\n",
        "# Print the agent's final message\n",
        "print(messages['messages'][-1].content)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270f287e",
      "metadata": {},
      "source": [
        "## Financial report tool with Gemini LLM and LangGraph agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80a0375",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent full response: {'messages': [HumanMessage(content='TechStack generated made $10 million with $8 million of costs. Generate a financial report.', additional_kwargs={}, response_metadata={}, id='99edd5f9-2ec1-4bbf-960d-999bde80e223'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'financial_report', 'arguments': '{\"revenue\": 10000000.0, \"expenses\": 8000000.0, \"company_name\": \"TechStack\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--abb3b8e7-d4a2-4595-bcbb-c8623c4ff5b3-0', tool_calls=[{'name': 'financial_report', 'args': {'revenue': 10000000.0, 'expenses': 8000000.0, 'company_name': 'TechStack'}, 'id': '1e81cad7-66a6-4cdb-bd69-a40f75744fc3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 90, 'output_tokens': 12, 'total_tokens': 102, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Financial Report for TechStack:\\nRevenue: $10,000,000\\nExpenses: $8,000,000\\nNet Income: $2,000,000\\nStatus: Profitable\\n', name='financial_report', id='d7bcda54-b398-4fc1-937a-6d79cb279907', tool_call_id='1e81cad7-66a6-4cdb-bd69-a40f75744fc3'), AIMessage(content=\"TechStack's financial report shows a revenue of $10,000,000, expenses of $8,000,000, and a net income of $2,000,000.  The company is profitable.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--7d606ed8-4fd7-415b-acbc-f89cca559f86-0', usage_metadata={'input_tokens': 159, 'output_tokens': 58, 'total_tokens': 217, 'input_token_details': {'cache_read': 0}})]}\n",
            "\n",
            "Final Agent Answer:\n",
            "TechStack's financial report shows a revenue of $10,000,000, expenses of $8,000,000, and a net income of $2,000,000.  The company is profitable.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Define the financial return report\n",
        "\n",
        "# Define the financial report tool\n",
        "@tool\n",
        "def financial_report(company_name: str, revenue: int, expenses: int) -> str:\n",
        "    \"\"\"Generate a financial report for a company that calculates net income.\n",
        "    \n",
        "    Args:\n",
        "        company_name: Name of the company\n",
        "        revenue: Total revenue in dollars\n",
        "        expenses: Total expenses in dollars\n",
        "    \n",
        "    Returns:\n",
        "        A formatted financial report string\n",
        "    \"\"\"\n",
        "    net_income = revenue - expenses\n",
        "    \n",
        "    report = f\"Financial Report for {company_name}:\\n\"\n",
        "    report += f\"Revenue: ${revenue:,}\\n\"\n",
        "    report += f\"Expenses: ${expenses:,}\\n\"\n",
        "    report += f\"Net Income: ${net_income:,}\\n\"\n",
        "    \n",
        "    if net_income > 0:\n",
        "        report += \"Status: Profitable\\n\"\n",
        "    elif net_income < 0:\n",
        "        report += \"Status: Loss\\n\"\n",
        "    else:\n",
        "        report += \"Status: Break-even\\n\"\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Initialize Gemini LLM (make sure GOOGLE_API_KEY is in your environment)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             temperature=0)\n",
        "\n",
        "# Create the ReAct agent with the financial_report tool\n",
        "agent = create_react_agent(llm, [financial_report])\n",
        "\n",
        "# Define input message\n",
        "user_message = \"TechStack generated made $10 million with $8 million of costs. Generate a financial report.\"\n",
        "\n",
        "# Call the agent with the human message\n",
        "response = agent.invoke({\n",
        "    \"messages\": [(\"human\", user_message)]\n",
        "})\n",
        "\n",
        "# Print the full response (including reasoning steps if verbose is enabled)\n",
        "print(\"Agent full response:\", response)\n",
        "\n",
        "# Print the final answer content only\n",
        "print(\"\\nFinal Agent Answer:\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
