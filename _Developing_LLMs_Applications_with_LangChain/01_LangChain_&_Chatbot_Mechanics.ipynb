{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6becfe6",
      "metadata": {
        "id": "c6becfe6"
      },
      "source": [
        "# **üî∑üî∑LangChainüî∑üî∑** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a921468",
      "metadata": {},
      "source": [
        "![img_5](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_Developing_LLMs_Applications_with_LangChain/_img/0305.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44c2ae6",
      "metadata": {
        "id": "c44c2ae6"
      },
      "source": [
        "## üß± Introduction to LangChain Ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e45c3e52",
      "metadata": {},
      "source": [
        "LangChain is a powerful framework designed to simplify the process of developing applications powered by large language models (LLMs). Its ecosystem provides tools to manage prompts, chains of calls, memory, agents, and integrations with external data sources or APIs. The core idea is to treat LLMs not just as isolated responders but as components in more complex workflows.\n",
        "\n",
        "Use cases include:\n",
        "- Building chatbots with memory and context.\n",
        "- Automating reasoning tasks by chaining prompt responses.\n",
        "- Integrating with databases or search tools for retrieval-augmented generation (RAG)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f492bf",
      "metadata": {
        "id": "f6f492bf"
      },
      "source": [
        "## üîå LangChain Integrations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6a812f",
      "metadata": {},
      "source": [
        "LangChain supports various integrations to expand its capabilities. These include:\n",
        "- **LLM Providers**: OpenAI (GPT-4, GPT-4o-mini), Hugging Face, Cohere, Anthropic.\n",
        "- **Vector Databases**: FAISS, Pinecone, Weaviate for semantic search and context retrieval.\n",
        "- **Toolkits and Agents**: Enable LLMs to interact with calculators, search engines, or APIs.\n",
        "\n",
        "Integrations allow users to build robust AI workflows that go beyond text generation, including retrieval, augmentation, and execution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32102265",
      "metadata": {
        "id": "32102265"
      },
      "source": [
        "## üîß Building LLM Apps the LangChain Way"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a749f55",
      "metadata": {},
      "source": [
        "The LangChain development approach emphasizes modularity and composability. Applications are broken into:\n",
        "- **Prompts**: The textual instructions or templates given to LLMs.\n",
        "- **Chains**: Sequences of operations such as formatting input, calling a model, parsing output.\n",
        "- **Agents**: LLMs with tool access that make decisions about which actions to take.\n",
        "- **Memory**: Used to store and recall previous interactions.\n",
        "\n",
        "By using LangChain Expression Language (LCEL), developers can connect components using the `|` (pipe) operator, making the flow clear and testable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8qC3eUqMg8WP",
      "metadata": {
        "id": "8qC3eUqMg8WP"
      },
      "source": [
        "## üå≥The Langchain Ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gfn18TXCg_Ul",
      "metadata": {
        "id": "Gfn18TXCg_Ul"
      },
      "source": [
        "```python\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Define the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 api_key=\"<OPENAI_API_TOKEN>\")\n",
        "\n",
        "# Predict the words following the text in question\n",
        "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ofL5MTewbqw9",
      "metadata": {
        "id": "ofL5MTewbqw9"
      },
      "outputs": [],
      "source": [
        "# Import the class for defining Hugging Face pipelines\n",
        "from langchain_huggingface import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1VZ8cyOdPdc",
      "metadata": {
        "id": "u1VZ8cyOdPdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\01_Github_Repo\\GenAI-with-Langchain-and-Huggingface\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mohdf\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "# 1. DistilGPT-2 (~82MB) - Very fast and lightweight\n",
        "llm_distilgpt2 = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"distilgpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        "    )\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_distilgpt2 .invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CeMf1qysdhCz",
      "metadata": {
        "id": "CeMf1qysdhCz"
      },
      "outputs": [],
      "source": [
        "# 2. GPT-2 Small (~124MB) - Classic small model\n",
        "llm_gpt2 = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        ")\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_gpt2.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5SmNeVSkdsqs",
      "metadata": {
        "id": "5SmNeVSkdsqs"
      },
      "outputs": [],
      "source": [
        "# 3. TinyLlama (~550MB) - Small but capable\n",
        "llm_tinyllama = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        "    )\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_tinyllama.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h7n54HEzd5Xt",
      "metadata": {
        "id": "h7n54HEzd5Xt"
      },
      "outputs": [],
      "source": [
        "# 4. Pythia-160M (~320MB) - EleutherAI's smallest model\n",
        "llm_pythia = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"EleutherAI/pythia-160m\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        ")\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_pythia.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HMUTNXW0eBTD",
      "metadata": {
        "id": "HMUTNXW0eBTD"
      },
      "outputs": [],
      "source": [
        "# 5. OPT-125M (~250MB) - Meta's small model\n",
        "llm_opt = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"facebook/opt-125m\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        "    )\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_opt.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOAh163ieH2E",
      "metadata": {
        "id": "ZOAh163ieH2E"
      },
      "outputs": [],
      "source": [
        "# 6. DialoGPT-small (~117MB) - Good for conversational tasks\n",
        "llm_dialogpt = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"microsoft/DialoGPT-small\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        "    )\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm_dialogpt.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C8S4cw9rePlk",
      "metadata": {
        "id": "C8S4cw9rePlk"
      },
      "outputs": [],
      "source": [
        "# 7. Phi-1.5 (~1.4GB) - Microsoft's efficient model (use with caution on free tier)\n",
        "llm_phi = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"microsoft/phi-1_5\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
        ")\n",
        "\n",
        "# prompt = \"Hugging Face is\"\n",
        "prompt = \"what is the capital of india?\"\n",
        "response = llm_phi.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zXQorxAsSFsV",
      "metadata": {
        "id": "zXQorxAsSFsV"
      },
      "outputs": [],
      "source": [
        "# Import the class for defining Hugging Face pipelines\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Define the LLM from the Hugging Face model ID\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"crumb/nano-mistral\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 20\n",
        "        }\n",
        "    )\n",
        "\n",
        "prompt = \"Hugging Face is\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7GvP-2eSft6D",
      "metadata": {
        "id": "7GvP-2eSft6D"
      },
      "source": [
        "```python\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"crumb/nano-mistral\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\n",
        "        \"max_new_tokens\": 20,       # Limits the length of generated text\n",
        "        \"temperature\": 0.7,         # Controls randomness (higher values increase creativity)\n",
        "        \"top_k\": 50,                # Limits sampling to top-k most probable tokens\n",
        "        \"top_p\": 0.9,               # Uses nucleus sampling for diverse responses\n",
        "        \"repetition_penalty\": 1.2,  # Reduces repetitive outputs\n",
        "        \"do_sample\": True,          # Enables sampling instead of greedy decoding\n",
        "        \"num_return_sequences\": 3,  # Generates multiple variations of output\n",
        "        \"early_stopping\": True      # Stops generation when an end condition is met\n",
        "    }\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72002e0",
      "metadata": {
        "id": "d72002e0"
      },
      "source": [
        "## ü§ñ Prompting OpenAI Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c838d62",
      "metadata": {},
      "source": [
        "LangChain simplifies interfacing with OpenAI's language models. The `ChatGoogleGenerativeAI` wrapper allows for:\n",
        "- Model configuration (e.g., GPT-4o-mini).\n",
        "- Controlling response generation using parameters like temperature and max_tokens.\n",
        "- Simple `.invoke()` calls to generate responses.\n",
        "\n",
        "This abstraction makes it easy to switch models or change behavior without altering core logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42def881",
      "metadata": {
        "id": "42def881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain is a framework for developing applications powered by large language models (LLMs).  It'\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the ChatGoogleGenerativeAI model with configuration settings\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",          # Using Google's free-tier model\n",
        "    # api_key=\"${GOOGLE_API_KEY}\",     # API key for authentication (if needed, uncomment this)\n",
        "    max_output_tokens=20,              # Limit the number of tokens in the generated response\n",
        "    temperature=0.7                    # Controls randomness; lower values make output more deterministic\n",
        ")\n",
        "\n",
        "# Invoke the language model with a sample query\n",
        "response = llm.invoke(\"What is LangChain?\")\n",
        "\n",
        "# Print the AI-generated response content\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5df3a0",
      "metadata": {
        "id": "2c5df3a0"
      },
      "source": [
        "## ü§ó Prompting Hugging Face Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad17e46c",
      "metadata": {},
      "source": [
        "Hugging Face offers open-source models that can be used through LangChain with the `HuggingFacePipeline`. It allows:\n",
        "- Model loading via model_id (e.g., LLaMA-3).\n",
        "- Specifying generation tasks (e.g., text-generation).\n",
        "- Tuning outputs with arguments like `max_new_tokens`.\n",
        "\n",
        "This is valuable for offline use cases or where you need fine-grained control over the model and infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0c95fe",
      "metadata": {
        "id": "2c0c95fe"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (make sure .env file contains necessary keys)\n",
        "load_dotenv()\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"max_new_tokens\": 100}\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What is Hugging Face?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08958c05",
      "metadata": {
        "id": "08958c05"
      },
      "source": [
        "## üßæ Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9ea26e",
      "metadata": {},
      "source": [
        "Prompt templates provide structure and clarity when building prompts. Instead of hardcoding input strings, templates use variables that are filled at runtime.\n",
        "\n",
        "Benefits:\n",
        "- Easy to update and debug.\n",
        "- Can be reused across chains or components.\n",
        "- Makes dynamic prompt construction systematic.\n",
        "\n",
        "Templates may include system messages, instructions, context, and examples depending on the use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c591b7f8",
      "metadata": {
        "id": "c591b7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain this concept simply and concisely: Prompting LLMs\n",
            "Prompting LLMs is giving instructions or questions to a large language model (like ChatGPT) to get a specific response.  It's like asking a question, but you carefully craft your question to get the best answer.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Create a prompt template\n",
        "template = \"Explain this concept simply and concisely: {concept}\"\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format the prompt\n",
        "prompt = prompt_template.invoke({\"concept\": \"Prompting LLMs\"})\n",
        "print(prompt.text)\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",          \n",
        "    # api_key=\"${GOOGLE_API_KEY}\",     \n",
        "    max_output_tokens=50,              \n",
        "    temperature=0.7\n",
        "    )\n",
        "\n",
        "# Run the model with the prompt\n",
        "llm_chain = prompt_template | llm\n",
        "concept = \"Prompting LLMs\"\n",
        "\n",
        "response = llm_chain.invoke({\"concept\": concept})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c694a04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain simplifies LLM application development by providing modular components for common tasks like prompt engineering, memory management, and chain building.  This allows developers to focus on the application logic rather than reinventing the wheel for fundamental LLM interactions.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Create prompt template\n",
        "template = \"You are an artificial intelligence assistant, answer the question in few sentences. {question}\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Initialize Gemini Flash model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    # api_key=\"${GOOGLE_API_KEY}\",\n",
        "    max_output_tokens=50,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Chain prompt with model\n",
        "llm_chain = prompt | llm\n",
        "\n",
        "# Ask a question\n",
        "question = \"How does Langchain make the LLM application development easier?\"\n",
        "response = llm_chain.invoke({\"question\": question})\n",
        "\n",
        "# Print model response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z8SsUgZtWPcg",
      "metadata": {
        "id": "Z8SsUgZtWPcg"
      },
      "source": [
        "## üîòUsing OpenAI LLM model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71448d19",
      "metadata": {},
      "source": [
        "\n",
        "```python\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=template\n",
        "    )\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    api_key = \"OPENAI_API_TOKEN\"\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm\n",
        "\n",
        "question = \"How does Langchain make the LLM application development easier\"\n",
        "print(llm_chain.invoke({\"question\": question}))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0de616d",
      "metadata": {
        "id": "f0de616d"
      },
      "source": [
        "## üó£Ô∏è Chat Models with Roles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54335e8",
      "metadata": {},
      "source": [
        "LangChain supports chat-based interactions using role-based messaging:\n",
        "- **System**: Sets the tone and purpose of the conversation.\n",
        "- **Human**: Represents user inputs.\n",
        "- **AI**: Shows model responses.\n",
        "\n",
        "This format allows for rich multi-turn conversations. ChatPromptTemplate helps define this dialogue structure in a maintainable format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3212ade8",
      "metadata": {
        "id": "3212ade8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 x 5 = 25\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Define multi-turn chat prompt\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a calculator that responds with math.\"),\n",
        "    (\"human\", \"Answer this math question: What is two plus two?\"),\n",
        "    (\"ai\", \"2+2=4\"),\n",
        "    (\"human\", \"Answer this math question: {math}\")\n",
        "])\n",
        "\n",
        "# Initialize Gemini Flash model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    # api_key=\"${GOOGLE_API_KEY}\",\n",
        "    max_output_tokens=50,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# Create chain with prompt and model\n",
        "llm_chain = template | llm\n",
        "\n",
        "# Run the chain with a new math question\n",
        "response = llm_chain.invoke({\"math\": \"What is five times five?\"})\n",
        "\n",
        "# Print the model's answer\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H7A0xToFYnW8",
      "metadata": {
        "id": "H7A0xToFYnW8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Red and white\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Define chat prompt for flag color identification\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a geography expert that returns the colors present in a country's flag.\"),\n",
        "        (\"human\", \"France\"),\n",
        "        (\"ai\", \"blue, white, red\"),\n",
        "        (\"human\", \"{country}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize Gemini Flash model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    # api_key=\"${GOOGLE_API_KEY}\",\n",
        "    max_output_tokens=50,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Create and invoke the chain\n",
        "llm_chain = prompt_template | llm\n",
        "country = \"Japan\"\n",
        "response = llm_chain.invoke({\"country\": country})\n",
        "\n",
        "# Print the model's response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0364e9",
      "metadata": {
        "id": "8a0364e9"
      },
      "source": [
        "## üîÇ Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64c3198",
      "metadata": {},
      "source": [
        "Few-shot prompting allows LLMs to generalize by showing examples in the prompt itself. LangChain facilitates this with `FewShotPromptTemplate`, which can:\n",
        "- Format multiple question-answer pairs.\n",
        "- Append suffixes to guide the model.\n",
        "- Accept examples from dataframes or lists.\n",
        "\n",
        "This method is ideal when fine-tuning is not possible but the task needs clear contextual signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b25bd2",
      "metadata": {
        "id": "71b25bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Does Henry Campbell have any pets?\n",
            "Henry Campbell has a dog called Pluto.\n",
            "\n",
            "Question: What is the name of Henry Campbell's dog?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"question\": \"Does Henry Campbell have any pets?\",\n",
        "     \"answer\": \"Henry Campbell has a dog called Pluto.\"}\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "prompt = few_shot_prompt.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})\n",
        "print(prompt.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kXE_U7JW3yng",
      "metadata": {
        "id": "kXE_U7JW3yng"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the capital city of Japan?\n",
            "Tokyo\n",
            "\n",
            "Question: Who developed the theory of relativity?\n",
            "Albert Einstein\n",
            "\n",
            "Question: What is the chemical symbol for gold?\n",
            "Au\n",
            "\n",
            "Question: What is the chemical symbol for Silver?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "\n",
        "# Create the examples list of dicts\n",
        "examples = [\n",
        "    {\n",
        "    \"question\": \"What is the capital city of Japan?\",\n",
        "    \"answer\": \"Tokyo\"\n",
        "        },\n",
        "    {\n",
        "    \"question\": \"Who developed the theory of relativity?\",\n",
        "    \"answer\": \"Albert Einstein\"\n",
        "        },\n",
        "    {\n",
        "    \"question\": \"What is the chemical symbol for gold?\",\n",
        "    \"answer\": \"Au\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Complete the prompt for formatting answers\n",
        "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
        "\n",
        "# create the few-shot prompt\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "prompt = few_shot_prompt.invoke({\"input\": \"What is the chemical symbol for Silver?\"})\n",
        "print(prompt.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425cfcf5",
      "metadata": {
        "id": "425cfcf5"
      },
      "source": [
        "## üîó Integrating Few-Shot with Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67cb510f",
      "metadata": {},
      "source": [
        "By combining prompt templates and LLMs in a pipeline, LangChain lets you build complete applications. For example:\n",
        "- A few-shot prompt template receives input.\n",
        "- It gets formatted with relevant examples.\n",
        "- The model generates an output.\n",
        "- All of this can be connected into a single callable `llm_chain`.\n",
        "\n",
        "This modular pipeline makes testing, evaluation, and reuse easier across projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aa5724ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            " Question: What is the capital of France?\n",
            "Answer: The capital of France is Paris.\n",
            "\n",
            "Question: What is the capital of Japan?\n",
            "Response:\n",
            " Answer: The capital of Japan is Tokyo.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (make sure .env has GOOGLE_API_KEY)\n",
        "load_dotenv()\n",
        "\n",
        "# New few-shot examples\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"The capital of France is Paris.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Prompt template for formatting the examples\n",
        "example_prompt = PromptTemplate.from_template(\"Question: {question}\\nAnswer: {answer}\")\n",
        "\n",
        "# FewShotPromptTemplate with suffix for the new question\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "# Print the formatted prompt to verify\n",
        "formatted_prompt = few_shot_prompt.format(input=\"What is the capital of Japan?\")\n",
        "print(\"Prompt:\\n\", formatted_prompt)\n",
        "\n",
        "# Set up the Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=100\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "chain = few_shot_prompt | llm\n",
        "\n",
        "# Invoke the chain with a new question\n",
        "response = chain.invoke({\"input\": \"What is the capital of Japan?\"})\n",
        "\n",
        "# Print the final response\n",
        "print(\"Response:\\n\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1728fe92",
      "metadata": {
        "id": "1728fe92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ag\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Create the examples list of dicts\n",
        "examples = [\n",
        "    {\n",
        "    \"question\": \"What is the capital city of Japan?\",\n",
        "    \"answer\": \"Tokyo\"\n",
        "        },\n",
        "    {\n",
        "    \"question\": \"Who developed the theory of relativity?\",\n",
        "    \"answer\": \"Albert Einstein\"\n",
        "        },\n",
        "    {\n",
        "    \"question\": \"What is the chemical symbol for gold?\",\n",
        "    \"answer\": \"Au\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Complete the prompt for formatting answers\n",
        "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
        "\n",
        "# create the few-shot prompt\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                             temperature=0.2,\n",
        "                            #  api_key=\"your_google_api_key_here\",\n",
        "                             max_output_tokens=100)    \n",
        "\n",
        "llm_chain = few_shot_prompt | llm\n",
        "\n",
        "response = llm_chain.invoke({\"input\": \"What is the chemical symbol for Silver?\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8690ed37",
      "metadata": {
        "id": "8690ed37"
      },
      "source": [
        "## ‚úÖ Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e06108df",
      "metadata": {
        "id": "e06108df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imagine teaching a dog a trick.  You don't tell it exactly *how* to do it, but you give it treats (rewards) when it gets closer to the desired behavior.  Reinforcement learning is similar.  It's a type of machine learning where a computer program (the \"dog\") learns to perform a task by trying different things and receiving rewards or penalties based on its actions.  The program learns to maximize its rewards over time by figuring out which actions lead to\n"
          ]
        }
      ],
      "source": [
        "# Try a custom question with the existing few-shot prompt chain\n",
        "your_input = \"Explain reinforcement learning in simple terms.\"\n",
        "response = llm_chain.invoke({\"input\": your_input})\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8qC3eUqMg8WP"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
