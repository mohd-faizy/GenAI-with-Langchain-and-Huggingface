{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce520789",
   "metadata": {},
   "source": [
    "# üß± **Major LangChain Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e4af2",
   "metadata": {},
   "source": [
    "![author](https://img.shields.io/badge/author-mohd--faizy-red)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f297c196",
   "metadata": {},
   "source": [
    "## **üó∫Ô∏è01 - imports mindmap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0aee6",
   "metadata": {},
   "source": [
    "![imports-maps-png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/00_LangChain_imports/_img/Lang-imports.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088253a9",
   "metadata": {},
   "source": [
    "## **üèóÔ∏è02 - Package Structure Changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d8d17",
   "metadata": {},
   "source": [
    "**IMPORTANT**: LangChain has been restructured into multiple packages. The old monolithic `langchain` package is now split into:\n",
    "\n",
    "- `langchain-core`: Core abstractions and base classes\n",
    "- `langchain-community`: Third-party integrations\n",
    "- `langchain-openai`: OpenAI-specific integrations\n",
    "- `langchain-anthropic`: Anthropic-specific integrations\n",
    "- `langchain-google`: Google-specific integrations\n",
    "- `langchain`: Main package (now lighter, orchestrates others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ca1f8",
   "metadata": {},
   "source": [
    "```python\n",
    "# Main package (optional, includes common chains and agents)\n",
    "pip install langchain\n",
    "\n",
    "# Core package (always needed)\n",
    "pip install langchain-core\n",
    "\n",
    "# Core LangChain ecosystem\n",
    "pip install  langchain-community \n",
    "\n",
    "# Advanced agent orchestration\n",
    "pip install langgraph\n",
    "\n",
    "# Production monitoring\n",
    "pip install langsmith\n",
    "\n",
    "# Deployment\n",
    "pip install langserve uvicorn fastapi\n",
    "\n",
    "# Advanced retrieval\n",
    "pip install faiss-cpu  # or faiss-gpu for large scale\n",
    "\n",
    "# Provider-specific packages (install as needed)\n",
    "pip install langchain-openai\n",
    "pip install langchain-anthropic\n",
    "pip install langchain-google\n",
    "pip install langchain-community\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27090c0",
   "metadata": {},
   "source": [
    "## **üß†03 - LLMs (Language Models)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d86ca",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚ùå OLD WAY (deprecated)\n",
    "from langchain.llms import OpenAI, Cohere, Anthropic, HuggingFaceHub\n",
    "\n",
    "# ‚úÖ NEW WAY (current)\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_anthropic import AnthropicLLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llms moved to community\n",
    "from langchain_community.llms import Cohere, HuggingFaceHub, HuggingFacePipeline \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32408c14",
   "metadata": {},
   "source": [
    "### What these are\n",
    "\n",
    "These classes provide **interfaces to LLMs**. LangChain wraps various third-party APIs or local pipelines, allowing you to treat them all in a standardized way.\n",
    "\n",
    "Each LLM implements the `Runnable` interface with `.invoke()`, `.stream()`, `.batch()`, and `.ainvoke()` methods.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Basic completions (e.g., Q&A, summarization)\n",
    "- Prompt engineering\n",
    "- Backend of a chain or agent\n",
    "\n",
    "### Differences\n",
    "\n",
    "| Class | Backend | Package | Online? | Good For |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `OpenAI` | OpenAI API | `langchain-openai` | Yes | GPT-4, GPT-4o, GPT-3.5 |\n",
    "| `AnthropicLLM` | Claude API | `langchain-anthropic` | Yes | Claude 2 models only |\n",
    "| `Cohere` | Cohere API | `langchain-community` | Yes | Command models |\n",
    "| `HuggingFaceHub` | HuggingFace cloud | `langchain-community` | Yes | Open source models |\n",
    "| `HuggingFacePipeline` | Local HuggingFace | `langchain-community` | No | Offline inference |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a20e1",
   "metadata": {},
   "source": [
    "## **ü§ñ04 - Chat Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd198e0",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚ùå OLD WAY (deprecated)\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic, ChatGooglePalm\n",
    "\n",
    "# ‚úÖ NEW WAY (current)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_community.chat_models import ChatCohere\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7f27a",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "\n",
    "These are used when interacting with **chat-style models** (multi-message inputs), like ChatGPT or Claude. They support **structured input** using message classes.\n",
    "\n",
    "### How They Differ from LLMs\n",
    "\n",
    "- LLMs take `str` as input.\n",
    "- Chat models take a **list of `BaseMessage` objects**.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4\")\n",
    "result = chat.invoke([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Tell me a joke\")\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "### Updated Chat Models\n",
    "\n",
    "| Class | Backend | Package | Models Available |\n",
    "| --- | --- | --- | --- |\n",
    "| `ChatOpenAI` | OpenAI API | `langchain-openai` | GPT-4, GPT-4o, GPT-3.5-turbo |\n",
    "| `ChatAnthropic` | Claude API | `langchain-anthropic` | Claude 3 Opus, Sonnet, Haiku |\n",
    "| `ChatGoogleGenerativeAI` | Google AI | `langchain-google` | Gemini Pro, Gemini Pro Vision |\n",
    "| `ChatCohere` | Cohere API | `langchain-community` | Command R, Command R+ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4\")\n",
    "result = chat.invoke([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Tell me a joke\")\n",
    "])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d1a14",
   "metadata": {},
   "source": [
    "## **üîò05 - Embedding models in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a890671",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Usage\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # or \"text-embedding-3-large\"\n",
    "    dimensions=256    \n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Hugging Face Embeddings\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Usage\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Other Popular Embedding Models\n",
    "\n",
    "```python\n",
    "# Cohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Anthropic (Claude)\n",
    "from langchain_anthropic import AnthropicEmbeddings\n",
    "\n",
    "# Google\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Ollama (local models)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Azure OpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "```\n",
    "\n",
    "### FAISS Integration (common with embeddings)\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "```\n",
    "\n",
    "### Chroma Integration\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_texts(texts, embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5d7e4",
   "metadata": {},
   "source": [
    "## **üì¶06 -  Chains and LCEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2bcb0",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚ùå OLD WAY (some deprecated)\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "# ‚úÖ NEW WAY (current)\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain import hub  # For prompt templates\n",
    "\n",
    "```\n",
    "\n",
    "### What Chains Are\n",
    "\n",
    "Modern LangChain emphasizes the **LCEL (LangChain Expression Language)** over traditional chains. You can compose operations using the `|` operator.\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "| Pattern | Purpose | Example |\n",
    "| --- | --- | --- |\n",
    "| **LCEL Chains** | Modern composition | `prompt | model | parser` |\n",
    "| **Retrieval Chains** | RAG workflows | `create_retrieval_chain()` |\n",
    "| **History-Aware** | Multi-turn RAG | `create_history_aware_retriever()` |\n",
    "| **Runnable** | Base interface | All components implement this |\n",
    "\n",
    "### Modern Example\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL chain composition\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"topic\": \"AI\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11fe32",
   "metadata": {},
   "source": [
    "## **üìÑ07 -  Prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14b069",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (mostly unchanged)\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "# Hub for sharing prompts\n",
    "from langchain import hub\n",
    "\n",
    "```\n",
    "\n",
    "### Why Prompts Matter\n",
    "\n",
    "LLMs are **prompt-driven**. Prompt classes let you define reusable templates with variables, making it easier to pass dynamic content.\n",
    "\n",
    "### Prompt Classes\n",
    "\n",
    "| Class | Purpose | Use Case |\n",
    "| --- | --- | --- |\n",
    "| `PromptTemplate` | Format single-string input | Simple LLM prompts |\n",
    "| `ChatPromptTemplate` | Compose multi-message chat input | Chat model prompts |\n",
    "| `MessagesPlaceholder` | Insert dynamic message lists | Chat history, tools |\n",
    "| `SystemMessagePromptTemplate` | Template for system role | Chat instructions |\n",
    "| `HumanMessagePromptTemplate` | Template for user message | User input |\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abff25",
   "metadata": {},
   "source": [
    "## **üß±08 - Documents and Loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c96630",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (moved to community)\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    WebBaseLoader,\n",
    "    PyPDFLoader,  # More reliable than PDFLoader\n",
    "    DirectoryLoader,\n",
    "    JSONLoader,\n",
    "    CSVLoader\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To ingest external content (docs, sites, PDFs) into a structured `Document` object with metadata (source, type, etc.).\n",
    "\n",
    "### When Used\n",
    "\n",
    "- In Retrieval-Augmented Generation (RAG)\n",
    "- In search-based agents or tools\n",
    "\n",
    "### Updated Loaders\n",
    "\n",
    "| Loader | Source | Package |\n",
    "| --- | --- | --- |\n",
    "| `TextLoader` | Plain text files | `langchain-community` |\n",
    "| `WebBaseLoader` | URLs or webpages | `langchain-community` |\n",
    "| `PyPDFLoader` | PDFs (more reliable) | `langchain-community` |\n",
    "| `DirectoryLoader` | Batch-load files | `langchain-community` |\n",
    "| `JSONLoader` | JSON files | `langchain-community` |\n",
    "| `CSVLoader` | CSV files | `langchain-community` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65441cb4",
   "metadata": {},
   "source": [
    "## **üîé09 Retrievers and VectorStores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1776c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "# ‚úÖ CURRENT (moved to community and specific packages)\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Turn `Document` objects into **semantic search-ready vectors**, then retrieve based on user input.\n",
    "\n",
    "### Flow\n",
    "\n",
    "```\n",
    "[Raw text] ‚Üí [Documents] ‚Üí [Embeddings] ‚Üí [Vector DB] ‚Üí [Retriever]\n",
    "\n",
    "```\n",
    "\n",
    "### Updated VectorStores\n",
    "\n",
    "| Store | Type | Package | Pros |\n",
    "| --- | --- | --- | --- |\n",
    "| `FAISS` | Local | `langchain-community` | Fast, free, offline |\n",
    "| `Chroma` | Local | `langchain-community` | Persistent, good dev support |\n",
    "| `PineconeVectorStore` | Cloud | `langchain-pinecone` | Scalable, production-ready |\n",
    "\n",
    "### Updated Embeddings\n",
    "\n",
    "| Class | Package | Uses |\n",
    "| --- | --- | --- |\n",
    "| `OpenAIEmbeddings` | `langchain-openai` | text-embedding-3-small/large |\n",
    "| `HuggingFaceEmbeddings` | `langchain-community` | Offline or HuggingFace models |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37685d52",
   "metadata": {},
   "source": [
    "## **üí¨10 - Memory (for Conversations)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db54343",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (simplified approach)\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To track **chat history** so that LLMs can reason across multiple turns.\n",
    "\n",
    "### Modern Approach\n",
    "\n",
    "Instead of complex memory classes, LangChain now uses:\n",
    "\n",
    "- `RunnableWithMessageHistory` for adding memory to any chain\n",
    "- Simple message history stores\n",
    "- Built-in chat history management\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Simple in-memory store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Add memory to any chain\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac111b07",
   "metadata": {},
   "source": [
    "## **üîß11 - Tools and Agents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688d6be",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (restructured)\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_community.tools import (\n",
    "    PythonREPLTool,\n",
    "    DuckDuckGoSearchRun,\n",
    "    WikipediaQueryRun\n",
    ")\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "\n",
    "```\n",
    "\n",
    "### Tools = External abilities for LLMs\n",
    "\n",
    "- **Tools** are wrappers around functions (e.g., search, calculator).\n",
    "- **Agents** can use these tools dynamically by deciding which one to call based on the query.\n",
    "\n",
    "### Modern Tool Creation\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "```\n",
    "\n",
    "### Updated Tools\n",
    "\n",
    "| Tool | Function | Package |\n",
    "| --- | --- | --- |\n",
    "| `PythonREPLTool` | Execute Python code | `langchain-community` |\n",
    "| `DuckDuckGoSearchRun` | Web search | `langchain-community` |\n",
    "| `WikipediaQueryRun` | Wikipedia search | `langchain-community` |\n",
    "\n",
    "### Modern Agent Creation\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "tools = [multiply, DuckDuckGoSearchRun()]\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094485f8",
   "metadata": {},
   "source": [
    "## **üß™12 - Output Parsers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb58f3",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (moved to core)\n",
    "from langchain_core.output_parsers import (\n",
    "    StrOutputParser,\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "To extract **structured info** from the natural language output of LLMs.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- JSON or structured output\n",
    "- API-ready responses\n",
    "- Information extraction\n",
    "\n",
    "### Modern Example\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: int = Field(description=\"person's age\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339615a",
   "metadata": {},
   "source": [
    "## **üß¨13 - Schema / Core Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a993e5",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CURRENT (now in langchain-core)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "```\n",
    "\n",
    "### Purpose\n",
    "\n",
    "These are **foundational classes** used across all LangChain components.\n",
    "\n",
    "### Updated Types\n",
    "\n",
    "| Class | Package | Purpose |\n",
    "| --- | --- | --- |\n",
    "| `Document` | `langchain-core` | Stores text + metadata |\n",
    "| `HumanMessage`, `AIMessage`, `SystemMessage` | `langchain-core` | Chat messages |\n",
    "| `Runnable` | `langchain-core` | Base interface for all components |\n",
    "| `BaseOutputParser` | `langchain-core` | Parent class for parsers |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcfd94",
   "metadata": {},
   "source": [
    "## **üîÅ14 - How They All Work Together (Modern RAG Example)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408715cd",
   "metadata": {},
   "source": [
    "Here's a **modern RAG chatbot** using current LangChain patterns:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load and split documents\n",
    "loader = WebBaseLoader(\"https://example.com\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 2. Create vector store\n",
    "vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. Create RAG chain using LCEL\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer based on context: {context}\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4. Use the chain\n",
    "response = rag_chain.invoke(\"What is the main topic?\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6167620",
   "metadata": {},
   "source": [
    "## **üï∏Ô∏è15 - LangGraph - Advanced Agent Orchestration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc0eec",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ CRITICAL for GenAI/Agent Engineers\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "\n",
    "```\n",
    "\n",
    "### What is LangGraph\n",
    "\n",
    "LangGraph is a low-level agent orchestration framework for building controllable agents. It's designed for **cyclic workflows** that traditional chains can't handle.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Purpose | Use Case |\n",
    "| --- | --- | --- |\n",
    "| **StateGraph** | Define agent workflow as a graph | Multi-step reasoning, loops |\n",
    "| **Nodes** | Individual processing steps | Tool calls, LLM invocations |\n",
    "| **Edges** | Flow control between nodes | Conditional routing |\n",
    "| **Checkpointing** | Save/restore agent state | Human-in-the-loop, persistence |\n",
    "\n",
    "### Basic Agent Example\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def agent_node(state: MessagesState):\n",
    "    return {\"messages\": [ChatOpenAI().invoke(state[\"messages\"])]}\n",
    "\n",
    "# Create graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"agent\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"messages\": [HumanMessage(content=\"Hello\")]})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d78ee5",
   "metadata": {},
   "source": [
    "## **üß†16 - Multi-Agent Systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996eab0",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ ESSENTIAL Multi-Agent Imports\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import Literal\n",
    "\n",
    "```\n",
    "\n",
    "### Multi-Agent Patterns\n",
    "\n",
    "| Pattern | Description | Use Case |\n",
    "| --- | --- | --- |\n",
    "| **Supervisor** | One agent coordinates others | Task delegation |\n",
    "| **Hierarchical** | Nested agent structures | Complex workflows |\n",
    "| **Swarm** | Peer-to-peer collaboration | Distributed problem solving |\n",
    "| **Sequential** | Agents work in sequence | Pipeline processing |\n",
    "\n",
    "### Supervisor Pattern Example\n",
    "\n",
    "```python\n",
    "from typing import Annotated, Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "members = [\"researcher\", \"coder\", \"writer\"]\n",
    "\n",
    "def supervisor_node(state: MessagesState) -> dict:\n",
    "    # Supervisor decides which agent to call next\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # Logic to determine next agent\n",
    "    return {\"next\": \"researcher\"}  # or \"coder\", \"writer\", \"FINISH\"\n",
    "\n",
    "def researcher_node(state: MessagesState) -> dict:\n",
    "    # Research logic here\n",
    "    return {\"messages\": [HumanMessage(content=\"Research complete\")]}\n",
    "\n",
    "# Build supervisor graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"supervisor\", supervisor_node)\n",
    "graph.add_node(\"researcher\", researcher_node)\n",
    "# Add conditional routing\n",
    "graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state.get(\"next\", \"FINISH\"),\n",
    "    {\"researcher\": \"researcher\", \"FINISH\": END}\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ef8fa",
   "metadata": {},
   "source": [
    "## **üìä17 - LangSmith - Observability & Debugging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cf3ab",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# ‚úÖ PRODUCTION MONITORING (Essential for GenAI Engineers)\n",
    "from langsmith import Client, traceable\n",
    "from langchain_core.callbacks import LangChainTracer\n",
    "import os\n",
    "\n",
    "# Set up tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your_api_key\"\n",
    "\n",
    "```\n",
    "\n",
    "### What is LangSmith\n",
    "\n",
    "LangSmith is a unified observability & evals platform where teams can debug, test, and monitor AI app performance ‚Äî essential for production GenAI applications.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Purpose | Critical For |\n",
    "| --- | --- | --- |\n",
    "| **Tracing** | Track every step of agent execution | Debugging complex agents |\n",
    "| **Evaluations** | Automated testing of agent performance | Quality assurance |\n",
    "| **Datasets** | Version control for test data | Reproducible testing |\n",
    "| **Monitoring** | Real-time performance metrics | Production monitoring |\n",
    "\n",
    "### Production Tracing Example\n",
    "\n",
    "```python\n",
    "from langsmith import traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def my_agent_function(query: str) -> str:\n",
    "    \"\"\"Traced agent function for production monitoring\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    result = llm.invoke(query)\n",
    "    return result.content\n",
    "\n",
    "# Automatically traces to LangSmith\n",
    "response = my_agent_function(\"Analyze this data\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97c126",
   "metadata": {},
   "source": [
    "## **üöÄ18 - LangServe - Production Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34eaf7",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ DEPLOYMENT (Critical for Production)\n",
    "from langserve import add_routes\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "```\n",
    "\n",
    "### What is LangServe\n",
    "\n",
    "LangServe helps developers deploy LangChain runnables and chains as a REST API. It's integrated with FastAPI for production deployment.\n",
    "\n",
    "### Deployment Example\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create your chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"My AI Agent\", version=\"1.0\")\n",
    "\n",
    "# Add your chain as an endpoint\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/agent\",\n",
    "    enable_feedback_endpoint=True,\n",
    "    enable_public_trace_link_endpoint=True,\n",
    ")\n",
    "\n",
    "# Run with: uvicorn main:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c1e54",
   "metadata": {},
   "source": [
    "## **üèóÔ∏è19 - Structured Output & Function Calling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65798891",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ STRUCTURED OUTPUTS (Essential for Reliable Agents)\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Optional\n",
    "\n",
    "```\n",
    "\n",
    "### Why Structured Output Matters\n",
    "\n",
    "For many applications, models need to output in a structured format to store in databases and ensure schema compliance.\n",
    "\n",
    "### Advanced Structured Output\n",
    "\n",
    "```python\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "class TaskAnalysis(BaseModel):\n",
    "    \"\"\"Structured analysis of a task\"\"\"\n",
    "    priority: str = Field(description=\"High, Medium, or Low priority\")\n",
    "    estimated_hours: float = Field(description=\"Estimated completion time\")\n",
    "    required_skills: List[str] = Field(description=\"Skills needed\")\n",
    "    dependencies: List[str] = Field(description=\"Task dependencies\")\n",
    "\n",
    "# Use with structured output\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "structured_llm = llm.with_structured_output(TaskAnalysis)\n",
    "\n",
    "# Reliable structured responses\n",
    "result = structured_llm.invoke(\"Analyze building a web application\")\n",
    "print(f\"Priority: {result.priority}\")\n",
    "print(f\"Hours: {result.estimated_hours}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac139424",
   "metadata": {},
   "source": [
    "## **üîß20 - Advanced Tool Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04e3bd",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ ADVANCED TOOL USAGE\n",
    "from langchain_core.tools import BaseTool, StructuredTool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Type, Any\n",
    "\n",
    "```\n",
    "\n",
    "### Custom Tool with Structured Input\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class DatabaseQueryInput(BaseModel):\n",
    "    query: str = Field(description=\"SQL query to execute\")\n",
    "    table: str = Field(description=\"Target table name\")\n",
    "\n",
    "class DatabaseTool(BaseTool):\n",
    "    name = \"database_query\"\n",
    "    description = \"Execute SQL queries on the database\"\n",
    "    args_schema: Type[BaseModel] = DatabaseQueryInput\n",
    "\n",
    "    def _run(self, query: str, table: str) -> str:\n",
    "        # Your database logic here\n",
    "        return f\"Executed '{query}' on table '{table}'\"\n",
    "\n",
    "# Use in agents\n",
    "db_tool = DatabaseTool()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4f84f",
   "metadata": {},
   "source": [
    "## **üß™21 - Advanced Retrieval Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a397921",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ ADVANCED RAG (Essential for GenAI Engineers)\n",
    "from langchain.retrievers import (\n",
    "    ParentDocumentRetriever,\n",
    "    MultiQueryRetriever,\n",
    "    EnsembleRetriever\n",
    ")\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "```\n",
    "\n",
    "### Hybrid Retrieval (Vector + Keyword)\n",
    "\n",
    "```python\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create retrievers\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "vector_retriever = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "# Combine them\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weighting\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Parent Document Retrieval\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split documents at different levels\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# Store both parent and child documents\n",
    "vectorstore = FAISS.from_documents([], OpenAIEmbeddings())\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832ff4a",
   "metadata": {},
   "source": [
    "## **üîÑ22 - Streaming & Async Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3d963",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ PRODUCTION STREAMING (Critical for UX)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import asyncio\n",
    "\n",
    "```\n",
    "\n",
    "### Streaming Responses\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create streaming chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "# Stream responses\n",
    "for chunk in chain.stream({\"topic\": \"AI agents\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "```\n",
    "\n",
    "### Async Operations\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "async def async_agent_call(query: str):\n",
    "    llm = ChatOpenAI()\n",
    "    response = await llm.ainvoke(query)\n",
    "    return response.content\n",
    "\n",
    "# Batch async calls\n",
    "async def batch_queries():\n",
    "    queries = [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
    "    tasks = [async_agent_call(q) for q in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f31bbd",
   "metadata": {},
   "source": [
    "## **üè¢23 - Enterprise & Production Patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bb755",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ ENTERPRISE INTEGRATIONS\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "```\n",
    "\n",
    "### Custom Callback for Logging\n",
    "\n",
    "```python\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "class ProductionCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback for production monitoring\"\"\"\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(f\"üöÄ LLM Call Started: {len(prompts)} prompts\")\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs):\n",
    "        print(f\"‚úÖ LLM Call Completed: {response.llm_output}\")\n",
    "\n",
    "    def on_llm_error(self, error, **kwargs):\n",
    "        print(f\"‚ùå LLM Error: {error}\")\n",
    "\n",
    "# Use in production\n",
    "callbacks = [ProductionCallbackHandler()]\n",
    "config = RunnableConfig(callbacks=callbacks)\n",
    "\n",
    "```\n",
    "\n",
    "### Caching for Performance\n",
    "\n",
    "```python\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "# Set up caching to reduce API calls\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "# Now all LLM calls will be cached\n",
    "llm = ChatOpenAI()  # Automatically uses cache\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d0ba7",
   "metadata": {},
   "source": [
    "## **üîç24 - Evaluation & Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60662e",
   "metadata": {},
   "source": [
    "```python\n",
    "# ‚úÖ AGENT EVALUATION (Critical for Quality Assurance)\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "```\n",
    "\n",
    "### Automated Evaluation\n",
    "\n",
    "```python\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load evaluators\n",
    "relevance_evaluator = load_evaluator(\"criteria\", criteria=\"relevance\")\n",
    "correctness_evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
    "\n",
    "# Evaluate agent responses\n",
    "def evaluate_agent_response(query: str, response: str, expected: str):\n",
    "    relevance_score = relevance_evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input=query\n",
    "    )\n",
    "\n",
    "    correctness_score = correctness_evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        reference=expected,\n",
    "        input=query\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"relevance\": relevance_score,\n",
    "        \"correctness\": correctness_score\n",
    "    }\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
