{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7320ab9d",
   "metadata": {},
   "source": [
    "# **üîóü¶úLangChain Components**\n",
    "\n",
    "![author](https://img.shields.io/badge/author-mohd--faizy-red)\n",
    "\n",
    "\n",
    "- [LangChain Integrations](https://python.langchain.com/docs/integrations/providers/)\n",
    "- [LangChain Components](https://python.langchain.com/docs/integrations/components/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdc576",
   "metadata": {},
   "source": [
    "**LangChain has 6 core components**\n",
    "\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_langCompIMG/Lang_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e68e5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü™¥ **1. `Models` ‚Äì The Core of LangChain**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87537f",
   "metadata": {},
   "source": [
    "- [‚≠êLangChain Chat Models‚≠ê](https://python.langchain.com/docs/integrations/chat/)\n",
    "- [‚≠êLangChain Embedding Models‚≠ê](https://python.langchain.com/docs/integrations/text_embedding/)\n",
    "- [‚≠êVector Store‚≠ê](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bd158",
   "metadata": {},
   "source": [
    "### üìå **What Are LangChain Models?**\n",
    "\n",
    "- üß† The **Models component** is the **core interface** to interact with AI models (LLMs & Embedding Models).\n",
    "- üîÑ LangChain is **model-agnostic** ‚Äì you can switch between different LLM providers with minimal code changes.\n",
    "- üõ†Ô∏è Solves the **standardization problem** ‚Äì every provider (`OpenAI`, `Gemini`, `Anthropic`, etc.) has different APIs, but LangChain offers one unified interface.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatOpenAI (GPT-4)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "\n",
    "result = model.invoke(\"Now divide the result by 1.5\")\n",
    "\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatAnthropic (Google Gemnai)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",   \n",
    "    max_output_tokens=50,              \n",
    "    temperature=0.7\n",
    "    )\n",
    "\n",
    "result = model.invoke(\"Hi who are you\")\n",
    "\n",
    "print(result.content)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# üöÄ Code using Groq (Mixtral / LLaMA via Langchain)\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",  # or \"llama3-70b-8192\"\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "result = model.invoke(\"Tell me a fun fact about black holes.\")\n",
    "\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatAnthropic (Claude 3 Opus)\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "\n",
    "result = model.invoke(\"Hi who are you\")\n",
    "\n",
    "print(result.content)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "# Chat Model: Local (Ollama / Llama2)\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama2\")\n",
    "response = llm.invoke(\"What are black holes?\")\n",
    "\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **Why Are Models Important?**\n",
    "\n",
    "- ‚úÖ Most important component of LangChain ‚Äì it's where the AI ‚Äúthinks.‚Äù\n",
    "- ü§ñ Handles both **language generation** (chatbots, agents) and **vector embedding** (search, retrieval).\n",
    "- üèóÔ∏è Acts as a **foundation** for the other 5 components: Prompts, Chains, Memory, Indexes, Agents.\n",
    "\n",
    "\n",
    "### üîç **Challenges Solved by LangChain Models**\n",
    "\n",
    "1. üß± **Huge Size** of LLMs (100GB+) ‚Üí Solved via API access.\n",
    "2. üîå **Different APIs for Different Providers** ‚Üí LangChain unifies them.\n",
    "3. üîÅ **No Standardized Output/Input Handling** ‚Üí LangChain parses and handles it uniformly.\n",
    "\n",
    "\n",
    "### ü§π‚Äç‚ôÇÔ∏è **Types of Models in LangChain**\n",
    "\n",
    "1. üó£Ô∏è **Language Models (LLMs)**\n",
    "    - Input: `Text`\n",
    "    - Output: `Text`\n",
    "    - Use cases: `Chatbots`, `summarization`, `translation`, `coding`.\n",
    "    - Providers: `OpenAI`, `Claude`, `Hugging Face`, `Bedrock`, `Mistral`, `Vertex AI`, `Azure`.\n",
    "  \n",
    "2. üß≠ **Embedding Models**\n",
    "    - Input: Text\n",
    "    - Output: Vector (numerical representation)\n",
    "    - Use case: `Semantic Search`, `Vector DB`\n",
    "    - Providers: `OpenAI`, `Mistral` AI, `IBM`, `Llama`, etc.\n",
    "\n",
    "\n",
    "### üß™ **Features Supported Across Models**\n",
    "\n",
    "- üß∞ Tool calling\n",
    "- üì¶ JSON / Structured output\n",
    "- üßë‚Äçüíª Local execution\n",
    "- üì∏ Multimodal input (e.g., images + text)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e954cf",
   "metadata": {},
   "source": [
    "## **Language Mode vs Embedding Model** \n",
    "\n",
    "- **Language Models** understand and generate text, great for conversations and tasks.\n",
    "- **Embedding Models** understand semantic meaning of text as vectors, used for search and matching.\n",
    "\n",
    "\n",
    "\n",
    "### üìä **Language Model vs Embedding Model Comparison**\n",
    "\n",
    "| Feature                           | **Language Model (LLM)**                                      | **Embedding Model**                                                 |\n",
    "| --------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
    "| **Primary Purpose**               | Generate and understand natural language text                 | Convert text into numerical vector representations                  |\n",
    "| **Input**                         | Natural language prompt (e.g., a question, instruction)       | Text string (e.g., sentence, document)                              |\n",
    "| **Output**                        | Natural language text (e.g., answer, summary, code, etc.)     | Dense vector (list of floats)                                       |\n",
    "| **Use Cases**                     | Chatbots, summarization, question answering, reasoning        | Semantic search, document retrieval, clustering, similarity scoring |\n",
    "| **Examples**                      | `GPT-4`, `Claude 3`, `Gemini Pro`, `Mistral`                          | `OpenAI Embeddings`, `Sentence-BERT`, Google `models/embedding-001`     |\n",
    "| **Key LangChain Class**           | `ChatOpenAI`, `ChatAnthropic`, `ChatGoogleGenerativeAI`, etc. | `OpenAIEmbeddings`, `GoogleGenerativeAIEmbeddings`, etc.            |\n",
    "| **Typical Output Format**         | Human-readable text                                           | List of float values (vector)                                       |\n",
    "| **Interaction Pattern**           | Conversational or completion-based                            | One-shot vectorization (no dialog)                                  |\n",
    "| **Supports Reasoning/Context?**   | ‚úÖ Yes                                                         | ‚ùå No (just encodes semantics)                                       |\n",
    "| **Supports RAG / Vector Search?** | ‚úÖ Yes (when paired with vector DBs)                           | ‚úÖ Core component of vector search pipelines                         |\n",
    "| **Latency & Cost**                | Higher (especially for large models)                          | Lower (vectors are precomputed and reused)                          |\n",
    "| **Storage Requirements**          | No persistent storage required unless caching responses       | Vectors often stored in vector DBs like `FAISS`, `Pinecone`, `Weaviate`   |\n",
    "| **Composability**                 | Often part of full pipelines (e.g., RAG, agent workflows)     | Used in retrieval step of pipelines                                 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d969f",
   "metadata": {},
   "source": [
    "## üí° **Embedding Models**\n",
    "\n",
    "\n",
    "### üß† Embedding Model: OpenAI\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vector = embedder.embed_query(\"What is machine learning?\")\n",
    "\n",
    "print(vector[:5])  # First 5 values\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üß† Embedding Model: Google Gemini\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedder = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embedder.embed_query(\"Define artificial intelligence.\")\n",
    "\n",
    "print(vector[:5])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üß† Embedding Model: Groq (using OpenAI-compatible embeddings)\n",
    "\n",
    "```python\n",
    "# Groq currently runs OpenAI-compatible models, so you can reuse OpenAIEmbeddings with Groq's API key\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-groq-api-key\"\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vector = embedder.embed_query(\"How do transformers work?\")\n",
    "\n",
    "print(vector[:5])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üîß üîü Advanced: Get JSON Output from Chat Model\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "response = llm.invoke(\"Return a JSON of 3 countries and their capitals.\")\n",
    "\n",
    "# Example output: {\"France\": \"Paris\", \"Japan\": \"Tokyo\", \"India\": \"New Delhi\"}\n",
    "print(response.content)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58461927",
   "metadata": {},
   "source": [
    "### ‚úÖ **Summary**\n",
    "\n",
    "- The **Models component** provides a **standardized, pluggable way** to interact with any LLM or embedding model.\n",
    "- Enables rapid experimentation and development with **minimal vendor lock-in**.\n",
    "- Supports both **language tasks (text in ‚Üí text out)** and **vector embeddings (text in ‚Üí vector out)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb6fe5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d46b31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üó®Ô∏è **2. `Prompts` ‚Äì Crafting the Right Questions for LLMs**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b709c7",
   "metadata": {},
   "source": [
    "## üß† **Types of Prompts in LangChain**\n",
    "\n",
    "### 1Ô∏è‚É£ Dynamic & Reusable Prompts\n",
    "\n",
    "* üîß Use placeholders like `{topic}` or `{tone}` that get filled dynamically.\n",
    "* ‚úÖ Example: `\"Summarize this {topic} in a {tone} tone.\"`\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate  # ‚úÖ Import for PromptTemplate\n",
    "\n",
    "# Create a reusable prompt template\n",
    "prompt = PromptTemplate.from_template('Summarize {topic} in {emotion} tone')\n",
    "\n",
    "# Fill in the placeholders\n",
    "print(prompt.format(topic='Cricket', emotion='fun'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Role-Based Prompts\n",
    "\n",
    "* üßë‚Äç‚öïÔ∏è Use a system-level prompt like: `\"You are an experienced doctor.\"`\n",
    "* üë§ Then ask: `\"Explain symptoms of viral fever.\"`\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate  # ‚úÖ Import for ChatPromptTemplate\n",
    "\n",
    "# Define the chat prompt with system and user roles\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Hi, you are an experienced {profession}\"),\n",
    "    (\"user\", \"Tell me about {topic}\"),\n",
    "])\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    profession=\"Doctor\", topic=\"Viral Fever\"\n",
    ")\n",
    "\n",
    "# View formatted prompt messages\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Few-Shot Prompts\n",
    "\n",
    "* üéì Give **input-output examples** to teach the model before the real query.\n",
    "* üìä Example: Show how messages map to categories before asking it to classify a new one.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate  # ‚úÖ Required imports\n",
    "\n",
    "# Step 1: Define examples (few-shot demonstrations)\n",
    "examples = [\n",
    "    {\"input\": \"I was charged twice for my subscription this month.\", \"output\": \"Billing Issue\"},\n",
    "    {\"input\": \"The app crashes every time I try to log in.\", \"output\": \"Technical Problem\"},\n",
    "    {\"input\": \"Can you explain how to upgrade my plan?\", \"output\": \"General Inquiry\"},\n",
    "    {\"input\": \"I need a refund for a payment I didn't authorize.\", \"output\": \"Billing Issue\"}\n",
    "]\n",
    "\n",
    "# Step 2: Define how each example should appear\n",
    "example_template = \"\"\"\n",
    "Ticket: {input}\n",
    "Category: {output}\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Build the few-shot prompt\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate(\n",
    "        input_variables=[\"input\", \"output\"],\n",
    "        template=example_template\n",
    "    ),\n",
    "    prefix=\"Classify the following customer support tickets into one of the categories: 'Billing Issue', 'Technical Problem', or 'General Inquiry'.\\n\",\n",
    "    suffix=\"Ticket: {user_input}\\nCategory:\",\n",
    "    input_variables=[\"user_input\"]\n",
    ")\n",
    "\n",
    "# Generate a prompt with a new user input\n",
    "final_prompt = few_shot_prompt.format(user_input=\"I am unable to connect to the internet using your service.\")\n",
    "print(final_prompt)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```text\n",
    "Classify the following customer support tickets into one of the categories: 'Billing Issue', 'Technical Problem', or 'General Inquiry'.\n",
    "\n",
    "Ticket: I was charged twice for my subscription this month.\n",
    "Category: Billing Issue\n",
    "\n",
    "Ticket: The app crashes every time I try to log in.\n",
    "Category: Technical Problem\n",
    "\n",
    "Ticket: Can you explain how to upgrade my plan?\n",
    "Category: General Inquiry\n",
    "\n",
    "Ticket: I need a refund for a payment I didn't authorize.\n",
    "Category: Billing Issue\n",
    "\n",
    "Ticket: I am unable to connect to the internet using your service.\n",
    "Category:\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f4930",
   "metadata": {},
   "source": [
    "### ‚úÖ **Summary**\n",
    "\n",
    "- The **Prompts component** gives **full control** over how you talk to LLMs.\n",
    "- Enables **reusable, flexible, and structured** prompt design.\n",
    "- Makes your apps **more reliable and intelligent** by controlling how LLMs interpret input.\n",
    "- ‚ú® Essential for building smart, adaptive, and role-aware AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a5b1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚õìÔ∏è **3. `Chains` ‚Äì Build Smart Pipelines for LLM Workflows**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e462a00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üìå **What Are Chains in LangChain?**\n",
    "\n",
    "- üîó Chains are used to **link multiple steps** of an LLM app into a **single automated pipeline**.\n",
    "- ü§ñ LangChain is **named after Chains** ‚Äì that‚Äôs how fundamental they are!\n",
    "- ‚öôÔ∏è They let you build **sequential**, **parallel**, or **conditional** flows between components like LLMs, tools, and memory.\n",
    "\n",
    "\n",
    "### ‚ö° **Why Use Chains?**\n",
    "\n",
    "- üîÑ Automatically passes the **output of one step as the input** to the next.\n",
    "- üßº Avoids repetitive manual code to handle data transfer between steps.\n",
    "- üöÄ Lets you design **multi-step AI applications** that work as one smooth pipeline.\n",
    "\n",
    "\n",
    "### üõ†Ô∏è **Real-World Use Case (Sequential Chain Example)**\n",
    "\n",
    "### üîÅ English Text ‚û°Ô∏è Hindi Translation ‚û°Ô∏è Hindi Summary\n",
    "\n",
    "1. Step 1: Translate English to Hindi (LLM 1)\n",
    "2. Step 2: Summarize Hindi text (LLM 2)\n",
    "\n",
    "    ‚úÖ Chains handle this flow without manual intervention ‚Äî just input English text and get the final Hindi summary.\n",
    "\n",
    "\n",
    "### üîç **Types of Chains in LangChain**\n",
    "\n",
    "#### 1Ô∏è‚É£ **Sequential Chains**\n",
    "\n",
    "- Steps run **one after another** in order.\n",
    "- *Example*: Translate ‚Üí Summarize ‚Üí Format ‚Üí Output\n",
    "\n",
    "#### 2Ô∏è‚É£ **Parallel Chains**\n",
    "\n",
    "- üß† Run multiple LLMs **simultaneously** and combine results.\n",
    "- *Example*: Same input sent to 3 LLMs to generate different takes ‚Üí Combine in final report.\n",
    "\n",
    "#### 3Ô∏è‚É£ **Conditional Chains**\n",
    "\n",
    "- ü§î Branching logic: behavior changes based on input/response.\n",
    "- *Example*: If user feedback is negative ‚Üí Send alert to support; else ‚Üí Send thank-you note.\n",
    "\n",
    "\n",
    "## üíª **Code Examples for LangChain Chains**\n",
    "\n",
    "\n",
    "### 1Ô∏è‚É£ Basic LLMChain (1-step flow)\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = PromptTemplate.from_template(\"Translate the following English text to Hindi:\\n\\n{text}\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "result = chain.run(\"I love learning about AI.\")\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 2Ô∏è‚É£ SequentialChain: Translation ‚û°Ô∏è Summarization\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Hindi:\\n\\n{text}\")\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize this Hindi text in under 100 words:\\n\\n{text}\")\n",
    "\n",
    "translate_chain = LLMChain(llm=llm, prompt=translate_prompt, output_key=\"translated\")\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, input_key=\"translated\")\n",
    "\n",
    "full_chain = SequentialChain(\n",
    "    chains=[translate_chain, summary_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"translated\", \"text\"]\n",
    ")\n",
    "\n",
    "result = full_chain.run({\"text\": \"Artificial Intelligence is transforming the world.\"})\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 3Ô∏è‚É£ Simple Conditional Chain (If-Else Logic)\n",
    "\n",
    "```python\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "def route_feedback(inputs):\n",
    "    feedback = inputs[\"feedback\"]\n",
    "    return {\"action\": \"thank user\" if \"good\" in feedback.lower() else \"alert support\"}\n",
    "\n",
    "router = TransformChain(input_variables=[\"feedback\"], output_variables=[\"action\"], transform=route_feedback)\n",
    "\n",
    "result = router.run({\"feedback\": \"The product is not working.\"})\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4Ô∏è‚É£ Parallel Chain (Mock Conceptual Example)\n",
    "\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "prompt2 = PromptTemplate.from_template(\"Write a joke about {topic}\")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "# In practice, you could run these chains in parallel using asyncio or LangGraph (experimental)\n",
    "poem = chain1.run(\"robots\")\n",
    "joke = chain2.run(\"robots\")\n",
    "\n",
    "print(\"Poem:\\n\", poem)\n",
    "print(\"Joke:\\n\", joke)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 5Ô∏è‚É£ Chain with Memory Integration (Preview)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "prompt = PromptTemplate.from_template(\"You are a chatbot. User said: {input}\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "print(chain.run(\"Hello, how are you?\"))\n",
    "print(chain.run(\"What did I just say?\"))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### ‚úÖ **Summary**\n",
    "\n",
    "- üß© **Chains** simplify **multi-step workflows** in LLM applications.\n",
    "- üí° They abstract away manual code and let you focus on logic and flow.\n",
    "- üß† Types:\n",
    "  - **Sequential** ‚Äì one step after another\n",
    "  - **Parallel** ‚Äì multiple steps at once\n",
    "  - **Conditional** ‚Äì smart branching\n",
    "- üì¶ Combine Chains with Prompts, Memory, and Agents for powerful apps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313972c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† **4. `Memory` ‚Äì Remembering Past Conversations in LangChain**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a68afe",
   "metadata": {},
   "source": [
    "### üìå **What Is Memory in LangChain?**\n",
    "\n",
    "- üîÅ‚ö†Ô∏è Most LLMs like GPT are **stateless** ‚Äî they forget everything after each message.\n",
    "- ‚ùå If you ask:\n",
    "  - \"Who is Narendra Modi?\"\n",
    "  - Then: \"How old is he?\"\n",
    "  - ‚Üí The model doesn‚Äôt remember who *\"he\"* is.\n",
    "- üß† **Memory solves this problem** by maintaining **context across turns** in a conversation.\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Why Is Memory Important?**\n",
    "\n",
    "- üó£Ô∏è Makes **chatbots and assistants feel natural and human-like**.\n",
    "- üßæ Keeps track of what users say ‚Äî no need to repeat questions.\n",
    "- ü§ñ Essential for building **stateful AI applications** like customer service bots, AI tutors, assistants, etc.\n",
    "\n",
    "\n",
    "### üîç **Types of Memory in LangChain**\n",
    "\n",
    "| üß† Type | üìã Description | üí° Use Case |\n",
    "| --- | --- | --- |\n",
    "| **`ConversationBufferMemory`** | Stores **full chat history** | Best for short conversations |\n",
    "| **`ConversationBufferWindowMemory`** | Stores **last N messages** | Great for recent context without overloading |\n",
    "| **`ConversationSummaryMemory`** | Stores a **summary of conversation** | Ideal for long chats, saves cost |\n",
    "| **`Custom Memory`** | Store **special facts or variables** | Good for personalized assistants |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb58a26",
   "metadata": {},
   "source": [
    "## üíª **Code Examples for LangChain Memory**\n",
    "\n",
    "\n",
    "### 1Ô∏è‚É£ Basic Memory Integration with `LLMChain`\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are a helpful bot. User said: {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# Simulate conversation\n",
    "print(chain.run(\"Who is Virat Kohli?\"))\n",
    "print(chain.run(\"What team does he play for?\"))  # Remembers previous message\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 2Ô∏è‚É£ Using `ConversationBufferWindowMemory`\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)  # Remembers last 2 interactions\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "print(chain.run(\"Explain Machine Learning.\"))\n",
    "print(chain.run(\"Give an example.\"))\n",
    "print(chain.run(\"What did I just say?\"))  # Only remembers 2 last messages\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 3Ô∏è‚É£ Using `ConversationSummaryMemory` (with summarization)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)  # Uses LLM to auto-summarize past chat\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=summary_memory)\n",
    "print(chain.run(\"Explain the plot of Inception.\"))\n",
    "print(chain.run(\"Who was the main character?\"))  # Will have access to summary, not full text\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4Ô∏è‚É£ Custom Memory Example (storing variables)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"My name is Alex\"}, {\"output\": \"Nice to meet you, Alex!\"})\n",
    "memory.save_context({\"input\": \"I live in Delhi\"}, {\"output\": \"Delhi is a great city.\"})\n",
    "\n",
    "# Access stored memory\n",
    "print(memory.load_memory_variables({}))  # Returns entire conversation history\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 5Ô∏è‚É£ Use Memory with a ChatPromptTemplate\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"Human: {input}\\nAI:\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversation.run(\"Tell me a joke.\"))\n",
    "print(conversation.run(\"Another one please!\"))  # Keeps previous context\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084d6e",
   "metadata": {},
   "source": [
    "### ‚úÖ **Summary**\n",
    "\n",
    "- üß† **Memory makes LangChain apps stateful** ‚Äî just like real conversations.\n",
    "- üí¨ It keeps track of what was said earlier and gives the model **context**.\n",
    "- üîß Use different memory types based on your app‚Äôs need:\n",
    "  - Full history (Buffer)\n",
    "  - Recent messages only (Window)\n",
    "  - Summarized context (Summary)\n",
    "- üß© Combine Memory with Chains, Prompts, and Models to build **real conversational agents**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1cb74e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üóÇÔ∏è **5. `Indexes` ‚Äì Letting LLMs Use Your Private Data**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6020fe",
   "metadata": {},
   "source": [
    "### ü§î **Why Do We Need Indexes?**\n",
    "\n",
    "- ü§ñ LLMs like ChatGPT **do not know your private data**.\n",
    "  - ‚ùå \"What‚Äôs the leave policy of XYZ company?\" ‚Üí Can't answer.\n",
    "  - Because it's **not in the training data**.\n",
    "- ‚úÖ We solve this using **Indexes** in LangChain to:\n",
    "  - **Connect LLMs to external data** (e.g. PDFs, websites).\n",
    "  - **Search and retrieve only what‚Äôs needed** from this data.\n",
    "  - Use it for **answering questions** based on it.\n",
    "\n",
    "\n",
    "### üß± **The 4 Core Sub-Components of Indexes**\n",
    "\n",
    "| üî¢ | üîß Component | üìã Role |\n",
    "| --- | --- | --- |\n",
    "| 1Ô∏è‚É£ | **Document Loader** | Loads your file (PDF, CSV, Notion, Drive, etc.) |\n",
    "| 2Ô∏è‚É£ | **Text Splitter** | Breaks large text into smaller chunks |\n",
    "| 3Ô∏è‚É£ | **Vector Store** | Stores chunk embeddings for similarity search |\n",
    "| 4Ô∏è‚É£ | **Retriever** | Finds the best chunks for a user query |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### üìä **How It Works (Simplified Flow)**\n",
    "\n",
    "![DOC_indexing](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_oth_img/LC_00_01.png)\n",
    "\n",
    "```\n",
    "PDF file (RulesBook.pdf)\n",
    "     ‚Üì\n",
    "[1] Document Loader ‚ûú Load the document\n",
    "     ‚Üì\n",
    "[2] Text Splitter ‚ûú Split into small chunks\n",
    "     ‚Üì\n",
    "[3] Vector Store (Vector Database) ‚ûú Embed chunks + Store\n",
    "     ‚Üì\n",
    "[4] Retriever ‚ûú Genrates Embedding ‚ûú Semantic search on user query{Genrates Relevant}\n",
    "     ‚Üì\n",
    "     LLM answers based on (Relevant Chunks + User Query)\n",
    "\n",
    "```\n",
    "\n",
    "| üìÅ DocumentLoader | üî¢ TextSplitter | üß† VectorStore | üïµÔ∏è‚Äç‚ôÇÔ∏è Retriever |\n",
    "| --- | --- | --- | --- |\n",
    "| Load files | Split into chunks | Store vectors | Find relevant chunks |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a2155",
   "metadata": {},
   "source": [
    "## üíª **LangChain Indexes ‚Äì Code Example**\n",
    "\n",
    "\n",
    "### üßæ 1. Load Your PDF Document\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"XYZ_Company_Policy.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### ‚úÇÔ∏è 2. Split Into Chunks\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### üß† 3. Create Embeddings + Vector Store\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### üîç 4. Setup Retriever and Ask Questions\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "query = \"What is the official leave policy?\"\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(doc.page_content)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### ‚úÖ Optional: Use with RetrievalQA Chain\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(qa_chain.run(\"What is the resignation notice period?\"))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## üß† **Why Indexes Are Crucial**\n",
    "\n",
    "- üîì They unlock the **power of private, local, or custom data**.\n",
    "- ‚öôÔ∏è They work seamlessly with other components (like Prompts + Chains).\n",
    "- üìö Perfect for building:\n",
    "  - Internal company chatbots\n",
    "  - Personalized tutors\n",
    "  - Research assistants\n",
    "  - FAQ bots on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1321f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ **6. `Agents` ‚Äì The Smartest, Action-Oriented Component**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcd302",
   "metadata": {},
   "source": [
    "### üí° **What Are Agents?**\n",
    "\n",
    "- Think of Agents as **chatbots with superpowers** ‚ö°\n",
    "- They don‚Äôt just *respond* ‚Äì they can **think, decide, and act**\n",
    "- Agents combine:\n",
    "  - üß† Reasoning (`\"What should I do first?\"`)\n",
    "  - üîß Tool use (`\"Let me use a calculator or weather API\"`)\n",
    "  - üß© Integration with other LangChain components\n",
    "\n",
    "\n",
    "### üéØ **How Are Agents Different from Chatbots?**\n",
    "\n",
    "| Chatbot | Agent |\n",
    "| --- | --- |\n",
    "| Just responds to queries | **Performs actions** |\n",
    "| Can‚Äôt use APIs or tools | **Can call tools, APIs, functions** |\n",
    "| Gives answers | **Finds answers + performs real tasks** |\n",
    "\n",
    "\n",
    "### üß† **Two Key Superpowers of Agents**\n",
    "\n",
    "1. **üß© Reasoning**: They break problems down into logical steps.\n",
    "    - Often via ‚≠ê‚≠ê‚≠ê **Chain of Thought(CoT)** ‚≠ê‚≠ê‚≠ê prompting.\n",
    "    - >üß† Chain of Thought (CoT) prompting means guiding an AI to solve problems step by step instead of jumping to the answer‚Äîlike showing its working in math class. It boosts accuracy and makes reasoning transparent.\n",
    "    - üìå**Why It‚Äôs Useful**\n",
    "        - Better problem-solving for math, logic, and commonsense reasoning\n",
    "        - More interpretable outputs, especially in critical applications\n",
    "        - Reduces errors in multi-step tasks\n",
    "\n",
    "    - üõ†Ô∏è **Variants**\n",
    "        - `Zero-shot CoT`: Just add ‚ÄúLet‚Äôs think step by step‚Äù to the prompt\n",
    "        - `Few-shot CoT`: Provide examples with reasoning steps\n",
    "        - `Auto-CoT`: Automatically generate diverse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **üîß Tool Use**: They can use:\n",
    "    - üî¢ Calculator\n",
    "    - üå¶Ô∏è Weather API\n",
    "    - üîç Search\n",
    "    - üìÖ Calendar\n",
    "    - üìä Custom APIs\n",
    "    - üíæ Local Indexes\n",
    "    - and more...\n",
    "\n",
    "\n",
    "### üîÅ **How an Agent Works (Behind-the-Scenes Flow)**\n",
    "\n",
    "1. User: *‚ÄúMultiply today‚Äôs temperature in Delhi by 3‚Äù*\n",
    "2. Agent thinks:\n",
    "    - ‚ÄúI need to find Delhi‚Äôs temperature‚Äù\n",
    "    - ‚ÄúThen multiply it by 3‚Äù\n",
    "3. Agent uses üîß weather tool ‚Üí gets 25¬∞C\n",
    "4. Agent uses üîß calculator tool: 25 √ó 3 = 75\n",
    "5. Agent returns: **‚ÄúThe result is 75‚Äù**\n",
    "\n",
    "\n",
    "### üí° **10 Awesome Real-World Agent Examples**\n",
    "\n",
    "| üî¢ # | üåç Real-World Use Case | üß† Tools / Steps |\n",
    "| --- | --- | --- |\n",
    "| 1Ô∏è‚É£ | \"Convert today‚Äôs INR to USD\" | Currency API + calculator |\n",
    "| 2Ô∏è‚É£ | \"Remind me to call mom at 7 PM\" | Calendar API |\n",
    "| 3Ô∏è‚É£ | \"Summarize this YouTube transcript and email me\" | Summarizer + Email API |\n",
    "| 4Ô∏è‚É£ | \"Book a cab from home to airport\" | Location API + Cab Booking API |\n",
    "| 5Ô∏è‚É£ | \"Give me weather in 3 cities and compare them\" | Weather tool √ó 3 + comparison logic |\n",
    "| 6Ô∏è‚É£ | \"Tell me the latest stock price of Apple and calculate 5% profit on 10 shares\" | Stock API + calculator |\n",
    "| 7Ô∏è‚É£ | \"Find top 3 tourist places in Japan and translate to Hindi\" | Search + Translator tool |\n",
    "| 8Ô∏è‚É£ | \"Fetch leave balance from HR system and show calendar view\" | HR API + calendar integration |\n",
    "| 9Ô∏è‚É£ | \"Search PDF for 'termination policy' and translate to Marathi\" | PDF retriever + translator |\n",
    "| üîü | \"Ask a question, retrieve from my docs, and save the result to Notion\" | Vector index + Notion API |\n",
    "\n",
    "\n",
    "### üîß **Minimal Code Example: Agent with Tools**\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchRun, Calculator\n",
    "\n",
    "# Define Tools\n",
    "search = DuckDuckGoSearchRun()\n",
    "calc = Calculator()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Search\", func=search.run, description=\"Useful for web search\"),\n",
    "    Tool(name=\"Calculator\", func=calc.run, description=\"Useful for math calculations\")\n",
    "]\n",
    "\n",
    "# Load model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Initialize Agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ask a smart question\n",
    "agent.run(\"What's the population of Japan divided by 3?\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchRun, Calculator\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "calc = Calculator()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Search\", func=search.run, description=\"Web search\"),\n",
    "    Tool(name=\"Calculator\", func=calc.run, description=\"Math ops\")\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "### üß© **How Agents Connect to Other Components**\n",
    "\n",
    "| üß† Component | ü§ù Role |\n",
    "| --- | --- |\n",
    "| **Models** | For reasoning + responses |\n",
    "| **Prompts** | Guide agent thinking |\n",
    "| **Chains** | Internal pipelines agent may call |\n",
    "| **Memory** | Track long conversations or tasks |\n",
    "| **Indexes** | Retrieve knowledge to reason on |\n",
    "\n",
    "Agents are the **glue** that orchestrates all the above when needed.\n",
    "\n",
    "\n",
    "### üìå **Summary**\n",
    "\n",
    "- üì¶ **Agents = Intelligent Orchestrators**\n",
    "- üîç Understand what needs to be done\n",
    "- üõ†Ô∏è Use tools and APIs to **act**\n",
    "- üîó Leverage all other LangChain components\n",
    "- üöÄ Power behind **real-world, useful LLM apps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e5aa",
   "metadata": {},
   "source": [
    "# ![divider.png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_langCompIMG/divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc58aa",
   "metadata": {},
   "source": [
    "# ‚úÖ **Commonly Used Built-in Tools in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46839c62",
   "metadata": {},
   "source": [
    "### üìö 1. `WikipediaQueryRun`\n",
    "\n",
    "- Search and summarize Wikipedia content.\n",
    "\n",
    "```python\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîç 2. `DuckDuckGoSearchRun`\n",
    "\n",
    "- Performs web searches using DuckDuckGo.\n",
    "\n",
    "```python\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ 3. `Calculator`\n",
    "\n",
    "- Evaluates mathematical expressions using Python.\n",
    "\n",
    "```python\n",
    "from langchain.tools import Calculator\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìÜ 4. `LLMMathTool`\n",
    "\n",
    "- Parses and evaluates math problems with reasoning using the LLM + calculator.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ 5. `PythonREPLTool`\n",
    "\n",
    "- Runs Python code interactively inside a REPL.\n",
    "\n",
    "```python\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üåê 6. `SerpAPIWrapper`\n",
    "\n",
    "- Uses the Google Search API (SerpAPI) for rich search queries.\n",
    "\n",
    "```python\n",
    "from langchain.tools import SerpAPIWrapper\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ 7. `HumanInputRun`\n",
    "\n",
    "- Asks for manual input from a human (useful in CLI tools).\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ 8. `TerminalTool`\n",
    "\n",
    "- Allows executing real commands in a shell (use cautiously).\n",
    "\n",
    "---\n",
    "\n",
    "### üîê 9. `RequestsGetTool`, `RequestsPostTool`\n",
    "\n",
    "- Send HTTP GET or POST requests.\n",
    "\n",
    "```python\n",
    "from langchain.tools.requests.tool import RequestsGetTool\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† 10. `RetrievalQA` or `VectorStoreQATool`\n",
    "\n",
    "- Allows agents to query a **Vector Store** (e.g., FAISS, Pinecone) via semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Custom Tools\n",
    "\n",
    "You can define **any function as a tool**:\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_greeting(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "\n",
    "Then include it in the `tools` list when initializing an agent.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Specialized Integrations / Tools via LangChain Plugins\n",
    "\n",
    "LangChain also provides wrappers for:\n",
    "\n",
    "| Service/API | Tool Type |\n",
    "| --- | --- |\n",
    "| Wolfram Alpha | `WolframAlphaQueryRun` |\n",
    "| Google Search (Serp) | `SerpAPIWrapper` |\n",
    "| OpenWeatherMap | Custom API + `RequestsGetTool` |\n",
    "| SQL Databases | `SQLDatabaseToolkit` |\n",
    "| Python Code Execution | `PythonREPLTool` or `LLMMathTool` |\n",
    "| Notion API | Custom Tool using SDK |\n",
    "| Zapier API | `ZapierNLARunAction` |\n",
    "| File I/O Tools | Read/write files from local system (custom) |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß How Agent Uses Tools\n",
    "\n",
    "When you pass tools to the agent:\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools=[search, calc, custom_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "LangChain parses the prompt, detects tool requirements, and **automatically selects and invokes tools** in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary: Categories of Tools\n",
    "\n",
    "| Category | Tools |\n",
    "| --- | --- |\n",
    "| **Math** | `Calculator`, `LLMMathTool`  |\n",
    "| **Search** | `DuckDuckGoSearchRun`, `SerpAPIWrapper`, `Wikipedia` |\n",
    "| **Code** | `PythonREPLTool`, `TerminalTool` |\n",
    "| **Web Requests** | `RequestsGetTool`, `RequestsPostTool`  |\n",
    "| **Memory / Data** | `VectorStoreQATool`, `RetrievalQA`, `RedisTool`  |\n",
    "| **APIs** | `Wolfram`, `Notion`, `Zapier`, `OpenWeather`  |\n",
    "| **Human Input** | `HumanInputRun` |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
