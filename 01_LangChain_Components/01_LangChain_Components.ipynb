{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7320ab9d",
   "metadata": {},
   "source": [
    "# **ðŸ”—ðŸ¦œLangChain Components**\n",
    "\n",
    "![author](https://img.shields.io/badge/author-mohd--faizy-red)\n",
    "\n",
    "\n",
    "- [LangChain Integrations](https://python.langchain.com/docs/integrations/providers/)\n",
    "- [LangChain Components](https://python.langchain.com/docs/integrations/components/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdc576",
   "metadata": {},
   "source": [
    "**LangChain has 6 core components**\n",
    "\n",
    "\n",
    "![image.png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_langCompIMG/Lang_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e68e5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸª´ **1. `Models` â€“ The Core of LangChain**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87537f",
   "metadata": {},
   "source": [
    "- [â­LangChain Chat Modelsâ­](https://python.langchain.com/docs/integrations/chat/)\n",
    "- [â­LangChain Embedding Modelsâ­](https://python.langchain.com/docs/integrations/text_embedding/)\n",
    "- [â­Vector Storeâ­](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bd158",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **What Are LangChain Models?**\n",
    "\n",
    "- ðŸ§  The **Models component** is the **core interface** to interact with AI models (LLMs & Embedding Models).\n",
    "- ðŸ”„ LangChain is **model-agnostic** â€“ you can switch between different LLM providers with minimal code changes.\n",
    "- ðŸ› ï¸ Solves the **standardization problem** â€“ every provider (`OpenAI`, `Gemini`, `Anthropic`, etc.) has different APIs, but LangChain offers one unified interface.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatOpenAI (GPT-4)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4', temperature=0)\n",
    "\n",
    "result = model.invoke(\"Now divide the result by 1.5\")\n",
    "\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatAnthropic (Google Gemnai)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",   \n",
    "    max_output_tokens=50,              \n",
    "    temperature=0.7\n",
    "    )\n",
    "\n",
    "result = model.invoke(\"Hi who are you\")\n",
    "\n",
    "print(result.content)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ðŸš€ Code using Groq (Mixtral / LLaMA via Langchain)\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",  # or \"llama3-70b-8192\"\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "result = model.invoke(\"Tell me a fun fact about black holes.\")\n",
    "\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Code using ChatAnthropic (Claude 3 Opus)\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "\n",
    "result = model.invoke(\"Hi who are you\")\n",
    "\n",
    "print(result.content)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "# Chat Model: Local (Ollama / Llama2)\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama2\")\n",
    "response = llm.invoke(\"What are black holes?\")\n",
    "\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š **Why Are Models Important?**\n",
    "\n",
    "- âœ… Most important component of LangChain â€“ it's where the AI â€œthinks.â€\n",
    "- ðŸ¤– Handles both **language generation** (chatbots, agents) and **vector embedding** (search, retrieval).\n",
    "- ðŸ—ï¸ Acts as a **foundation** for the other 5 components: Prompts, Chains, Memory, Indexes, Agents.\n",
    "\n",
    "\n",
    "### ðŸ” **Challenges Solved by LangChain Models**\n",
    "\n",
    "1. ðŸ§± **Huge Size** of LLMs (100GB+) â†’ Solved via API access.\n",
    "2. ðŸ”Œ **Different APIs for Different Providers** â†’ LangChain unifies them.\n",
    "3. ðŸ” **No Standardized Output/Input Handling** â†’ LangChain parses and handles it uniformly.\n",
    "\n",
    "\n",
    "### ðŸ¤¹â€â™‚ï¸ **Types of Models in LangChain**\n",
    "\n",
    "1. ðŸ—£ï¸ **Language Models (LLMs)**\n",
    "    - Input: `Text`\n",
    "    - Output: `Text`\n",
    "    - Use cases: `Chatbots`, `summarization`, `translation`, `coding`.\n",
    "    - Providers: `OpenAI`, `Claude`, `Hugging Face`, `Bedrock`, `Mistral`, `Vertex AI`, `Azure`.\n",
    "  \n",
    "2. ðŸ§­ **Embedding Models**\n",
    "    - Input: Text\n",
    "    - Output: Vector (numerical representation)\n",
    "    - Use case: `Semantic Search`, `Vector DB`\n",
    "    - Providers: `OpenAI`, `Mistral` AI, `IBM`, `Llama`, etc.\n",
    "\n",
    "\n",
    "### ðŸ§ª **Features Supported Across Models**\n",
    "\n",
    "- ðŸ§° Tool calling\n",
    "- ðŸ“¦ JSON / Structured output\n",
    "- ðŸ§‘â€ðŸ’» Local execution\n",
    "- ðŸ“¸ Multimodal input (e.g., images + text)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e954cf",
   "metadata": {},
   "source": [
    "## **Language Mode vs Embedding Model** \n",
    "\n",
    "- **Language Models** understand and generate text, great for conversations and tasks.\n",
    "- **Embedding Models** understand semantic meaning of text as vectors, used for search and matching.\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“Š **Language Model vs Embedding Model Comparison**\n",
    "\n",
    "| Feature                           | **Language Model (LLM)**                                      | **Embedding Model**                                                 |\n",
    "| --------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
    "| **Primary Purpose**               | Generate and understand natural language text                 | Convert text into numerical vector representations                  |\n",
    "| **Input**                         | Natural language prompt (e.g., a question, instruction)       | Text string (e.g., sentence, document)                              |\n",
    "| **Output**                        | Natural language text (e.g., answer, summary, code, etc.)     | Dense vector (list of floats)                                       |\n",
    "| **Use Cases**                     | Chatbots, summarization, question answering, reasoning        | Semantic search, document retrieval, clustering, similarity scoring |\n",
    "| **Examples**                      | `GPT-4`, `Claude 3`, `Gemini Pro`, `Mistral`                          | `OpenAI Embeddings`, `Sentence-BERT`, Google `models/embedding-001`     |\n",
    "| **Key LangChain Class**           | `ChatOpenAI`, `ChatAnthropic`, `ChatGoogleGenerativeAI`, etc. | `OpenAIEmbeddings`, `GoogleGenerativeAIEmbeddings`, etc.            |\n",
    "| **Typical Output Format**         | Human-readable text                                           | List of float values (vector)                                       |\n",
    "| **Interaction Pattern**           | Conversational or completion-based                            | One-shot vectorization (no dialog)                                  |\n",
    "| **Supports Reasoning/Context?**   | âœ… Yes                                                         | âŒ No (just encodes semantics)                                       |\n",
    "| **Supports RAG / Vector Search?** | âœ… Yes (when paired with vector DBs)                           | âœ… Core component of vector search pipelines                         |\n",
    "| **Latency & Cost**                | Higher (especially for large models)                          | Lower (vectors are precomputed and reused)                          |\n",
    "| **Storage Requirements**          | No persistent storage required unless caching responses       | Vectors often stored in vector DBs like `FAISS`, `Pinecone`, `Weaviate`   |\n",
    "| **Composability**                 | Often part of full pipelines (e.g., RAG, agent workflows)     | Used in retrieval step of pipelines                                 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d969f",
   "metadata": {},
   "source": [
    "## ðŸ’¡ **Embedding Models**\n",
    "\n",
    "\n",
    "### ðŸ§  Embedding Model: OpenAI\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vector = embedder.embed_query(\"What is machine learning?\")\n",
    "\n",
    "print(vector[:5])  # First 5 values\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ§  Embedding Model: Google Gemini\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedder = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embedder.embed_query(\"Define artificial intelligence.\")\n",
    "\n",
    "print(vector[:5])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ§  Embedding Model: Groq (using OpenAI-compatible embeddings)\n",
    "\n",
    "```python\n",
    "# Groq currently runs OpenAI-compatible models, so you can reuse OpenAIEmbeddings with Groq's API key\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-groq-api-key\"\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vector = embedder.embed_query(\"How do transformers work?\")\n",
    "\n",
    "print(vector[:5])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”§ ðŸ”Ÿ Advanced: Get JSON Output from Chat Model\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "response = llm.invoke(\"Return a JSON of 3 countries and their capitals.\")\n",
    "\n",
    "# Example output: {\"France\": \"Paris\", \"Japan\": \"Tokyo\", \"India\": \"New Delhi\"}\n",
    "print(response.content)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58461927",
   "metadata": {},
   "source": [
    "### âœ… **Summary**\n",
    "\n",
    "- The **Models component** provides a **standardized, pluggable way** to interact with any LLM or embedding model.\n",
    "- Enables rapid experimentation and development with **minimal vendor lock-in**.\n",
    "- Supports both **language tasks (text in â†’ text out)** and **vector embeddings (text in â†’ vector out)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb6fe5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d46b31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ—¨ï¸ **2. `Prompts` â€“ Crafting the Right Questions for LLMs**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b709c7",
   "metadata": {},
   "source": [
    "## ðŸ§  **Types of Prompts in LangChain**\n",
    "\n",
    "### 1ï¸âƒ£ Dynamic & Reusable Prompts\n",
    "\n",
    "* ðŸ”§ Use placeholders like `{topic}` or `{tone}` that get filled dynamically.\n",
    "* âœ… Example: `\"Summarize this {topic} in a {tone} tone.\"`\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate  # âœ… Import for PromptTemplate\n",
    "\n",
    "# Create a reusable prompt template\n",
    "prompt = PromptTemplate.from_template('Summarize {topic} in {emotion} tone')\n",
    "\n",
    "# Fill in the placeholders\n",
    "print(prompt.format(topic='Cricket', emotion='fun'))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Role-Based Prompts\n",
    "\n",
    "* ðŸ§‘â€âš•ï¸ Use a system-level prompt like: `\"You are an experienced doctor.\"`\n",
    "* ðŸ‘¤ Then ask: `\"Explain symptoms of viral fever.\"`\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate  # âœ… Import for ChatPromptTemplate\n",
    "\n",
    "# Define the chat prompt with system and user roles\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Hi, you are an experienced {profession}\"),\n",
    "    (\"user\", \"Tell me about {topic}\"),\n",
    "])\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    profession=\"Doctor\", topic=\"Viral Fever\"\n",
    ")\n",
    "\n",
    "# View formatted prompt messages\n",
    "for msg in formatted_messages:\n",
    "    print(f\"{msg.type.upper()}: {msg.content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Few-Shot Prompts\n",
    "\n",
    "* ðŸŽ“ Give **input-output examples** to teach the model before the real query.\n",
    "* ðŸ“Š Example: Show how messages map to categories before asking it to classify a new one.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate  # âœ… Required imports\n",
    "\n",
    "# Step 1: Define examples (few-shot demonstrations)\n",
    "examples = [\n",
    "    {\"input\": \"I was charged twice for my subscription this month.\", \"output\": \"Billing Issue\"},\n",
    "    {\"input\": \"The app crashes every time I try to log in.\", \"output\": \"Technical Problem\"},\n",
    "    {\"input\": \"Can you explain how to upgrade my plan?\", \"output\": \"General Inquiry\"},\n",
    "    {\"input\": \"I need a refund for a payment I didn't authorize.\", \"output\": \"Billing Issue\"}\n",
    "]\n",
    "\n",
    "# Step 2: Define how each example should appear\n",
    "example_template = \"\"\"\n",
    "Ticket: {input}\n",
    "Category: {output}\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Build the few-shot prompt\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate(\n",
    "        input_variables=[\"input\", \"output\"],\n",
    "        template=example_template\n",
    "    ),\n",
    "    prefix=\"Classify the following customer support tickets into one of the categories: 'Billing Issue', 'Technical Problem', or 'General Inquiry'.\\n\",\n",
    "    suffix=\"Ticket: {user_input}\\nCategory:\",\n",
    "    input_variables=[\"user_input\"]\n",
    ")\n",
    "\n",
    "# Generate a prompt with a new user input\n",
    "final_prompt = few_shot_prompt.format(user_input=\"I am unable to connect to the internet using your service.\")\n",
    "print(final_prompt)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```text\n",
    "Classify the following customer support tickets into one of the categories: 'Billing Issue', 'Technical Problem', or 'General Inquiry'.\n",
    "\n",
    "Ticket: I was charged twice for my subscription this month.\n",
    "Category: Billing Issue\n",
    "\n",
    "Ticket: The app crashes every time I try to log in.\n",
    "Category: Technical Problem\n",
    "\n",
    "Ticket: Can you explain how to upgrade my plan?\n",
    "Category: General Inquiry\n",
    "\n",
    "Ticket: I need a refund for a payment I didn't authorize.\n",
    "Category: Billing Issue\n",
    "\n",
    "Ticket: I am unable to connect to the internet using your service.\n",
    "Category:\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f4930",
   "metadata": {},
   "source": [
    "### âœ… **Summary**\n",
    "\n",
    "- The **Prompts component** gives **full control** over how you talk to LLMs.\n",
    "- Enables **reusable, flexible, and structured** prompt design.\n",
    "- Makes your apps **more reliable and intelligent** by controlling how LLMs interpret input.\n",
    "- âœ¨ Essential for building smart, adaptive, and role-aware AI apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a5b1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# â›“ï¸ **3. `Chains` â€“ Build Smart Pipelines for LLM Workflows**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e462a00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ðŸ“Œ **What Are Chains in LangChain?**\n",
    "\n",
    "- ðŸ”— Chains are used to **link multiple steps** of an LLM app into a **single automated pipeline**.\n",
    "- ðŸ¤– LangChain is **named after Chains** â€“ thatâ€™s how fundamental they are!\n",
    "- âš™ï¸ They let you build **sequential**, **parallel**, or **conditional** flows between components like LLMs, tools, and memory.\n",
    "\n",
    "\n",
    "### âš¡ **Why Use Chains?**\n",
    "\n",
    "- ðŸ”„ Automatically passes the **output of one step as the input** to the next.\n",
    "- ðŸ§¼ Avoids repetitive manual code to handle data transfer between steps.\n",
    "- ðŸš€ Lets you design **multi-step AI applications** that work as one smooth pipeline.\n",
    "\n",
    "\n",
    "### ðŸ› ï¸ **Real-World Use Case (Sequential Chain Example)**\n",
    "\n",
    "### ðŸ” English Text âž¡ï¸ Hindi Translation âž¡ï¸ Hindi Summary\n",
    "\n",
    "1. Step 1: Translate English to Hindi (LLM 1)\n",
    "2. Step 2: Summarize Hindi text (LLM 2)\n",
    "\n",
    "    âœ… Chains handle this flow without manual intervention â€” just input English text and get the final Hindi summary.\n",
    "\n",
    "\n",
    "### ðŸ” **Types of Chains in LangChain**\n",
    "\n",
    "#### 1ï¸âƒ£ **Sequential Chains**\n",
    "\n",
    "- Steps run **one after another** in order.\n",
    "- *Example*: Translate â†’ Summarize â†’ Format â†’ Output\n",
    "\n",
    "#### 2ï¸âƒ£ **Parallel Chains**\n",
    "\n",
    "- ðŸ§  Run multiple LLMs **simultaneously** and combine results.\n",
    "- *Example*: Same input sent to 3 LLMs to generate different takes â†’ Combine in final report.\n",
    "\n",
    "#### 3ï¸âƒ£ **Conditional Chains**\n",
    "\n",
    "- ðŸ¤” Branching logic: behavior changes based on input/response.\n",
    "- *Example*: If user feedback is negative â†’ Send alert to support; else â†’ Send thank-you note.\n",
    "\n",
    "\n",
    "## ðŸ’» **Code Examples for LangChain Chains**\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ Basic LLMChain (1-step flow)\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = PromptTemplate.from_template(\"Translate the following English text to Hindi:\\n\\n{text}\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "result = chain.run(\"I love learning about AI.\")\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ SequentialChain: Translation âž¡ï¸ Summarization\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Hindi:\\n\\n{text}\")\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize this Hindi text in under 100 words:\\n\\n{text}\")\n",
    "\n",
    "translate_chain = LLMChain(llm=llm, prompt=translate_prompt, output_key=\"translated\")\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, input_key=\"translated\")\n",
    "\n",
    "full_chain = SequentialChain(\n",
    "    chains=[translate_chain, summary_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"translated\", \"text\"]\n",
    ")\n",
    "\n",
    "result = full_chain.run({\"text\": \"Artificial Intelligence is transforming the world.\"})\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ Simple Conditional Chain (If-Else Logic)\n",
    "\n",
    "```python\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "def route_feedback(inputs):\n",
    "    feedback = inputs[\"feedback\"]\n",
    "    return {\"action\": \"thank user\" if \"good\" in feedback.lower() else \"alert support\"}\n",
    "\n",
    "router = TransformChain(input_variables=[\"feedback\"], output_variables=[\"action\"], transform=route_feedback)\n",
    "\n",
    "result = router.run({\"feedback\": \"The product is not working.\"})\n",
    "print(result)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ Parallel Chain (Mock Conceptual Example)\n",
    "\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "prompt2 = PromptTemplate.from_template(\"Write a joke about {topic}\")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "# In practice, you could run these chains in parallel using asyncio or LangGraph (experimental)\n",
    "poem = chain1.run(\"robots\")\n",
    "joke = chain2.run(\"robots\")\n",
    "\n",
    "print(\"Poem:\\n\", poem)\n",
    "print(\"Joke:\\n\", joke)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ Chain with Memory Integration (Preview)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "prompt = PromptTemplate.from_template(\"You are a chatbot. User said: {input}\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "print(chain.run(\"Hello, how are you?\"))\n",
    "print(chain.run(\"What did I just say?\"))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### âœ… **Summary**\n",
    "\n",
    "- ðŸ§© **Chains** simplify **multi-step workflows** in LLM applications.\n",
    "- ðŸ’¡ They abstract away manual code and let you focus on logic and flow.\n",
    "- ðŸ§  Types:\n",
    "  - **Sequential** â€“ one step after another\n",
    "  - **Parallel** â€“ multiple steps at once\n",
    "  - **Conditional** â€“ smart branching\n",
    "- ðŸ“¦ Combine Chains with Prompts, Memory, and Agents for powerful apps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313972c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§  **4. `Memory` â€“ Remembering Past Conversations in LangChain**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a68afe",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **What Is Memory in LangChain?**\n",
    "\n",
    "- ðŸ”âš ï¸ Most LLMs like GPT are **stateless** â€” they forget everything after each message.\n",
    "- âŒ If you ask:\n",
    "  - \"Who is Narendra Modi?\"\n",
    "  - Then: \"How old is he?\"\n",
    "  - â†’ The model doesnâ€™t remember who *\"he\"* is.\n",
    "- ðŸ§  **Memory solves this problem** by maintaining **context across turns** in a conversation.\n",
    "\n",
    "\n",
    "\n",
    "### ðŸš€ **Why Is Memory Important?**\n",
    "\n",
    "- ðŸ—£ï¸ Makes **chatbots and assistants feel natural and human-like**.\n",
    "- ðŸ§¾ Keeps track of what users say â€” no need to repeat questions.\n",
    "- ðŸ¤– Essential for building **stateful AI applications** like customer service bots, AI tutors, assistants, etc.\n",
    "\n",
    "\n",
    "### ðŸ” **Types of Memory in LangChain**\n",
    "\n",
    "| ðŸ§  Type | ðŸ“‹ Description | ðŸ’¡ Use Case |\n",
    "| --- | --- | --- |\n",
    "| **`ConversationBufferMemory`** | Stores **full chat history** | Best for short conversations |\n",
    "| **`ConversationBufferWindowMemory`** | Stores **last N messages** | Great for recent context without overloading |\n",
    "| **`ConversationSummaryMemory`** | Stores a **summary of conversation** | Ideal for long chats, saves cost |\n",
    "| **`Custom Memory`** | Store **special facts or variables** | Good for personalized assistants |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb58a26",
   "metadata": {},
   "source": [
    "## ðŸ’» **Code Examples for LangChain Memory**\n",
    "\n",
    "\n",
    "### 1ï¸âƒ£ Basic Memory Integration with `LLMChain`\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are a helpful bot. User said: {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# Simulate conversation\n",
    "print(chain.run(\"Who is Virat Kohli?\"))\n",
    "print(chain.run(\"What team does he play for?\"))  # Remembers previous message\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ Using `ConversationBufferWindowMemory`\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)  # Remembers last 2 interactions\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "print(chain.run(\"Explain Machine Learning.\"))\n",
    "print(chain.run(\"Give an example.\"))\n",
    "print(chain.run(\"What did I just say?\"))  # Only remembers 2 last messages\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ Using `ConversationSummaryMemory` (with summarization)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)  # Uses LLM to auto-summarize past chat\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=summary_memory)\n",
    "print(chain.run(\"Explain the plot of Inception.\"))\n",
    "print(chain.run(\"Who was the main character?\"))  # Will have access to summary, not full text\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ Custom Memory Example (storing variables)\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"My name is Alex\"}, {\"output\": \"Nice to meet you, Alex!\"})\n",
    "memory.save_context({\"input\": \"I live in Delhi\"}, {\"output\": \"Delhi is a great city.\"})\n",
    "\n",
    "# Access stored memory\n",
    "print(memory.load_memory_variables({}))  # Returns entire conversation history\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ Use Memory with a ChatPromptTemplate\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"Human: {input}\\nAI:\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "print(conversation.run(\"Tell me a joke.\"))\n",
    "print(conversation.run(\"Another one please!\"))  # Keeps previous context\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084d6e",
   "metadata": {},
   "source": [
    "### âœ… **Summary**\n",
    "\n",
    "- ðŸ§  **Memory makes LangChain apps stateful** â€” just like real conversations.\n",
    "- ðŸ’¬ It keeps track of what was said earlier and gives the model **context**.\n",
    "- ðŸ”§ Use different memory types based on your appâ€™s need:\n",
    "  - Full history (Buffer)\n",
    "  - Recent messages only (Window)\n",
    "  - Summarized context (Summary)\n",
    "- ðŸ§© Combine Memory with Chains, Prompts, and Models to build **real conversational agents**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1cb74e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ—‚ï¸ **5. `Indexes` â€“ Letting LLMs Use Your Private Data**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6020fe",
   "metadata": {},
   "source": [
    "### ðŸ¤” **Why Do We Need Indexes?**\n",
    "\n",
    "- ðŸ¤– LLMs like ChatGPT **do not know your private data**.\n",
    "  - âŒ \"Whatâ€™s the leave policy of XYZ company?\" â†’ Can't answer.\n",
    "  - Because it's **not in the training data**.\n",
    "- âœ… We solve this using **Indexes** in LangChain to:\n",
    "  - **Connect LLMs to external data** (e.g. PDFs, websites).\n",
    "  - **Search and retrieve only whatâ€™s needed** from this data.\n",
    "  - Use it for **answering questions** based on it.\n",
    "\n",
    "\n",
    "### ðŸ§± **The 4 Core Sub-Components of Indexes**\n",
    "\n",
    "| ðŸ”¢ | ðŸ”§ Component | ðŸ“‹ Role |\n",
    "| --- | --- | --- |\n",
    "| 1ï¸âƒ£ | **Document Loader** | Loads your file (PDF, CSV, Notion, Drive, etc.) |\n",
    "| 2ï¸âƒ£ | **Text Splitter** | Breaks large text into smaller chunks |\n",
    "| 3ï¸âƒ£ | **Vector Store** | Stores chunk embeddings for similarity search |\n",
    "| 4ï¸âƒ£ | **Retriever** | Finds the best chunks for a user query |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“Š **How It Works (Simplified Flow)**\n",
    "\n",
    "![DOC_indexing](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_oth_img/LC_00_01.png)\n",
    "\n",
    "```\n",
    "PDF file (RulesBook.pdf)\n",
    "     â†“\n",
    "[1] Document Loader âžœ Load the document\n",
    "     â†“\n",
    "[2] Text Splitter âžœ Split into small chunks\n",
    "     â†“\n",
    "[3] Vector Store (Vector Database) âžœ Embed chunks + Store\n",
    "     â†“\n",
    "[4] Retriever âžœ Genrates Embedding âžœ Semantic search on user query{Genrates Relevant}\n",
    "     â†“\n",
    "     LLM answers based on (Relevant Chunks + User Query)\n",
    "\n",
    "```\n",
    "\n",
    "| ðŸ“ DocumentLoader | ðŸ”¢ TextSplitter | ðŸ§  VectorStore | ðŸ•µï¸â€â™‚ï¸ Retriever |\n",
    "| --- | --- | --- | --- |\n",
    "| Load files | Split into chunks | Store vectors | Find relevant chunks |\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a2155",
   "metadata": {},
   "source": [
    "## ðŸ’» **LangChain Indexes â€“ Code Example**\n",
    "\n",
    "\n",
    "### ðŸ§¾ 1. Load Your PDF Document\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"XYZ_Company_Policy.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### âœ‚ï¸ 2. Split Into Chunks\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### ðŸ§  3. Create Embeddings + Vector Store\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### ðŸ” 4. Setup Retriever and Ask Questions\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "query = \"What is the official leave policy?\"\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(doc.page_content)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### âœ… Optional: Use with RetrievalQA Chain\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(qa_chain.run(\"What is the resignation notice period?\"))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## ðŸ§  **Why Indexes Are Crucial**\n",
    "\n",
    "- ðŸ”“ They unlock the **power of private, local, or custom data**.\n",
    "- âš™ï¸ They work seamlessly with other components (like Prompts + Chains).\n",
    "- ðŸ“š Perfect for building:\n",
    "  - Internal company chatbots\n",
    "  - Personalized tutors\n",
    "  - Research assistants\n",
    "  - FAQ bots on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1321f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ¤– **6. `Agents` â€“ The Smartest, Action-Oriented Component**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcd302",
   "metadata": {},
   "source": [
    "### ðŸ’¡ **What Are Agents?**\n",
    "\n",
    "- Think of Agents as **chatbots with superpowers** âš¡\n",
    "- They donâ€™t just *respond* â€“ they can **think, decide, and act**\n",
    "- Agents combine:\n",
    "  - ðŸ§  Reasoning (`\"What should I do first?\"`)\n",
    "  - ðŸ”§ Tool use (`\"Let me use a calculator or weather API\"`)\n",
    "  - ðŸ§© Integration with other LangChain components\n",
    "\n",
    "\n",
    "### ðŸŽ¯ **How Are Agents Different from Chatbots?**\n",
    "\n",
    "| Chatbot | Agent |\n",
    "| --- | --- |\n",
    "| Just responds to queries | **Performs actions** |\n",
    "| Canâ€™t use APIs or tools | **Can call tools, APIs, functions** |\n",
    "| Gives answers | **Finds answers + performs real tasks** |\n",
    "\n",
    "\n",
    "### ðŸ§  **Two Key Superpowers of Agents**\n",
    "\n",
    "1. **ðŸ§© Reasoning**: They break problems down into logical steps.\n",
    "    - Often via â­â­â­ **Chain of Thought(CoT)** â­â­â­ prompting.\n",
    "    - >ðŸ§  Chain of Thought (CoT) prompting means guiding an AI to solve problems step by step instead of jumping to the answerâ€”like showing its working in math class. It boosts accuracy and makes reasoning transparent.\n",
    "    - ðŸ“Œ**Why Itâ€™s Useful**\n",
    "        - Better problem-solving for math, logic, and commonsense reasoning\n",
    "        - More interpretable outputs, especially in critical applications\n",
    "        - Reduces errors in multi-step tasks\n",
    "\n",
    "    - ðŸ› ï¸ **Variants**\n",
    "        - `Zero-shot CoT`: Just add â€œLetâ€™s think step by stepâ€ to the prompt\n",
    "        - `Few-shot CoT`: Provide examples with reasoning steps\n",
    "        - `Auto-CoT`: Automatically generate diverse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **ðŸ”§ Tool Use**: They can use:\n",
    "    - ðŸ”¢ Calculator\n",
    "    - ðŸŒ¦ï¸ Weather API\n",
    "    - ðŸ” Search\n",
    "    - ðŸ“… Calendar\n",
    "    - ðŸ“Š Custom APIs\n",
    "    - ðŸ’¾ Local Indexes\n",
    "    - and more...\n",
    "\n",
    "\n",
    "### ðŸ” **How an Agent Works (Behind-the-Scenes Flow)**\n",
    "\n",
    "1. User: *â€œMultiply todayâ€™s temperature in Delhi by 3â€*\n",
    "2. Agent thinks:\n",
    "    - â€œI need to find Delhiâ€™s temperatureâ€\n",
    "    - â€œThen multiply it by 3â€\n",
    "3. Agent uses ðŸ”§ weather tool â†’ gets 25Â°C\n",
    "4. Agent uses ðŸ”§ calculator tool: 25 Ã— 3 = 75\n",
    "5. Agent returns: **â€œThe result is 75â€**\n",
    "\n",
    "\n",
    "### ðŸ’¡ **10 Awesome Real-World Agent Examples**\n",
    "\n",
    "| ðŸ”¢ # | ðŸŒ Real-World Use Case | ðŸ§  Tools / Steps |\n",
    "| --- | --- | --- |\n",
    "| 1ï¸âƒ£ | \"Convert todayâ€™s INR to USD\" | Currency API + calculator |\n",
    "| 2ï¸âƒ£ | \"Remind me to call mom at 7 PM\" | Calendar API |\n",
    "| 3ï¸âƒ£ | \"Summarize this YouTube transcript and email me\" | Summarizer + Email API |\n",
    "| 4ï¸âƒ£ | \"Book a cab from home to airport\" | Location API + Cab Booking API |\n",
    "| 5ï¸âƒ£ | \"Give me weather in 3 cities and compare them\" | Weather tool Ã— 3 + comparison logic |\n",
    "| 6ï¸âƒ£ | \"Tell me the latest stock price of Apple and calculate 5% profit on 10 shares\" | Stock API + calculator |\n",
    "| 7ï¸âƒ£ | \"Find top 3 tourist places in Japan and translate to Hindi\" | Search + Translator tool |\n",
    "| 8ï¸âƒ£ | \"Fetch leave balance from HR system and show calendar view\" | HR API + calendar integration |\n",
    "| 9ï¸âƒ£ | \"Search PDF for 'termination policy' and translate to Marathi\" | PDF retriever + translator |\n",
    "| ðŸ”Ÿ | \"Ask a question, retrieve from my docs, and save the result to Notion\" | Vector index + Notion API |\n",
    "\n",
    "\n",
    "### ðŸ”§ **Minimal Code Example: Agent with Tools**\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchRun, Calculator\n",
    "\n",
    "# Define Tools\n",
    "search = DuckDuckGoSearchRun()\n",
    "calc = Calculator()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Search\", func=search.run, description=\"Useful for web search\"),\n",
    "    Tool(name=\"Calculator\", func=calc.run, description=\"Useful for math calculations\")\n",
    "]\n",
    "\n",
    "# Load model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Initialize Agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ask a smart question\n",
    "agent.run(\"What's the population of Japan divided by 3?\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchRun, Calculator\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "calc = Calculator()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Search\", func=search.run, description=\"Web search\"),\n",
    "    Tool(name=\"Calculator\", func=calc.run, description=\"Math ops\")\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "### ðŸ§© **How Agents Connect to Other Components**\n",
    "\n",
    "| ðŸ§  Component | ðŸ¤ Role |\n",
    "| --- | --- |\n",
    "| **Models** | For reasoning + responses |\n",
    "| **Prompts** | Guide agent thinking |\n",
    "| **Chains** | Internal pipelines agent may call |\n",
    "| **Memory** | Track long conversations or tasks |\n",
    "| **Indexes** | Retrieve knowledge to reason on |\n",
    "\n",
    "Agents are the **glue** that orchestrates all the above when needed.\n",
    "\n",
    "\n",
    "### ðŸ“Œ **Summary**\n",
    "\n",
    "- ðŸ“¦ **Agents = Intelligent Orchestrators**\n",
    "- ðŸ” Understand what needs to be done\n",
    "- ðŸ› ï¸ Use tools and APIs to **act**\n",
    "- ðŸ”— Leverage all other LangChain components\n",
    "- ðŸš€ Power behind **real-world, useful LLM apps**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365e5aa",
   "metadata": {},
   "source": [
    "# ![divider.png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_langCompIMG/divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc58aa",
   "metadata": {},
   "source": [
    "# âœ… **Commonly Used Built-in Tools in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46839c62",
   "metadata": {},
   "source": [
    "### ðŸ“š 1. `WikipediaQueryRun`\n",
    "\n",
    "- Search and summarize Wikipedia content.\n",
    "\n",
    "```python\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” 2. `DuckDuckGoSearchRun`\n",
    "\n",
    "- Performs web searches using DuckDuckGo.\n",
    "\n",
    "```python\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® 3. `Calculator`\n",
    "\n",
    "- Evaluates mathematical expressions using Python.\n",
    "\n",
    "```python\n",
    "from langchain.tools import Calculator\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“† 4. `LLMMathTool`\n",
    "\n",
    "- Parses and evaluates math problems with reasoning using the LLM + calculator.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‚ 5. `PythonREPLTool`\n",
    "\n",
    "- Runs Python code interactively inside a REPL.\n",
    "\n",
    "```python\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ 6. `SerpAPIWrapper`\n",
    "\n",
    "- Uses the Google Search API (SerpAPI) for rich search queries.\n",
    "\n",
    "```python\n",
    "from langchain.tools import SerpAPIWrapper\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ 7. `HumanInputRun`\n",
    "\n",
    "- Asks for manual input from a human (useful in CLI tools).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§¾ 8. `TerminalTool`\n",
    "\n",
    "- Allows executing real commands in a shell (use cautiously).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” 9. `RequestsGetTool`, `RequestsPostTool`\n",
    "\n",
    "- Send HTTP GET or POST requests.\n",
    "\n",
    "```python\n",
    "from langchain.tools.requests.tool import RequestsGetTool\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  10. `RetrievalQA` or `VectorStoreQATool`\n",
    "\n",
    "- Allows agents to query a **Vector Store** (e.g., FAISS, Pinecone) via semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Custom Tools\n",
    "\n",
    "You can define **any function as a tool**:\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_greeting(name: str) -> str:\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "\n",
    "Then include it in the `tools` list when initializing an agent.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Specialized Integrations / Tools via LangChain Plugins\n",
    "\n",
    "LangChain also provides wrappers for:\n",
    "\n",
    "| Service/API | Tool Type |\n",
    "| --- | --- |\n",
    "| Wolfram Alpha | `WolframAlphaQueryRun` |\n",
    "| Google Search (Serp) | `SerpAPIWrapper` |\n",
    "| OpenWeatherMap | Custom API + `RequestsGetTool` |\n",
    "| SQL Databases | `SQLDatabaseToolkit` |\n",
    "| Python Code Execution | `PythonREPLTool` or `LLMMathTool` |\n",
    "| Notion API | Custom Tool using SDK |\n",
    "| Zapier API | `ZapierNLARunAction` |\n",
    "| File I/O Tools | Read/write files from local system (custom) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ How Agent Uses Tools\n",
    "\n",
    "When you pass tools to the agent:\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools=[search, calc, custom_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "LangChain parses the prompt, detects tool requirements, and **automatically selects and invokes tools** in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary: Categories of Tools\n",
    "\n",
    "| Category | Tools |\n",
    "| --- | --- |\n",
    "| **Math** | `Calculator`, `LLMMathTool`  |\n",
    "| **Search** | `DuckDuckGoSearchRun`, `SerpAPIWrapper`, `Wikipedia` |\n",
    "| **Code** | `PythonREPLTool`, `TerminalTool` |\n",
    "| **Web Requests** | `RequestsGetTool`, `RequestsPostTool`  |\n",
    "| **Memory / Data** | `VectorStoreQATool`, `RetrievalQA`, `RedisTool`  |\n",
    "| **APIs** | `Wolfram`, `Notion`, `Zapier`, `OpenWeather`  |\n",
    "| **Human Input** | `HumanInputRun` |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
