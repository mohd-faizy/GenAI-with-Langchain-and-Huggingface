{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain RoadMap:**"
      ],
      "metadata": {
        "id": "NNqIFXrA0aPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌RoadMap**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jnhqa2Gv8h0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐1. Fundamentals**\n",
        "\n",
        "- a. **What is LangChain?**\n",
        "\n",
        "    - **LangChain** is an open-source framework designed to simplify the development of **LLM-based applications**. It provides **modular components** and **end-to-end tools** that help developers build complex AI systems such as:\n",
        "\n",
        "        * 🤖 Chatbots  \n",
        "        * ❓ Question-answering systems  \n",
        "        * 🔍 Retrieval-Augmented Generation (RAG) applications  \n",
        "        * 🧠 Autonomous AI agents  \n",
        "        * 🧰 Tool-using agents and more  \n",
        "\n",
        "- 🔑 **Key Features:**\n",
        "\n",
        "    1. ✅ **Supports all major LLMs** (OpenAI, Anthropic, Gemnai, Hugging Face, etc.)\n",
        "    - **Model-agnostic Framework** means LangChain can work with multiple large language models (LLMs) like `OpenAI`, `Anthropic`, `Hugging Face`, `gemini`, and more.\n",
        "    2. ⚙️ **Simplifies LLM-based app development** through reusable components like `Chains`, `Prompts`, `Memory`, and `Runnables`.\n",
        "    3. 🔌 **Integrates with key tools** such as vector stores, APIs, and databases\n",
        "    4. 🆓 **Free and open-source**, with active community and continuous updates\n",
        "    5. 🌐 **Supports all major GenAI use cases**, making it a versatile framework for developers\n",
        "\n"
      ],
      "metadata": {
        "id": "5FJuUZxyXfLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐2. LangChain Components**\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Da8h19cA9gsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- a. **`Models`**: Interface for LLMs and chat models.\n",
        "- b. **`Prompts`**: Templates for structured inputs to LLMs.\n",
        "- c. **`Chains`**: Sequences of calls (models, tools).\n",
        "- d. **`Indexes`**: This include documents loders, text splitters, vectorstores and retrivers for structured data retrieval.\n",
        "- e. **`Memory`**: Allow the LLM to maintain a flow in the conversation bykeeping the entire conversation in contex.\n",
        "- f. **`Agents`**: Leverage LLMs as a reasoning engine to determine whichactions to take."
      ],
      "metadata": {
        "id": "Iaybk3R99hOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐3. Models**\n",
        "\n",
        "- LangChain supports multiple LLMs (OpenAI, HuggingFace, Cohere, etc.) via standard interfaces.\n",
        "- You define which model to use and optionally provide API keys or configurations.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "922mzd79801G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐4. Prompts**\n",
        "- Prompts are templates or examples designed to guide LLM responses.  \n",
        "- LangChain uses **PromptTemplates** to standardize and reuse prompts.  \n",
        "- You can include variables and format them at runtime.\n",
        "\n"
      ],
      "metadata": {
        "id": "x4e0K7YJ-_O2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐5. Parsing Output**\n",
        "\n",
        "Post-processing is often necessary to convert LLM output into structuredformats. LangChain provides:\n",
        "\n",
        "* **Regex parsers**  \n",
        "* **Pydantic-based output parsers**  \n",
        "* **Structured JSON parsers**\n",
        "\n"
      ],
      "metadata": {
        "id": "6VTIHIP5_BbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐6. Runnables & LCEL (LangChain Expression Language)**\n",
        "\n",
        "LangChain 0.1+ introduces **Runnables**, a functional abstraction over allLangChain components.  \n",
        "LCEL allows you to compose logic using `|` and `+`, building complex pipelines declaratively.\n",
        "\n",
        "```python\n",
        "chain = prompt | llm | output_parser\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_shBu9zD_EVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐7. Chains**\n",
        "\n",
        "Chains are sequences of calls that produce a final result. LangChainprovides:\n",
        "\n",
        "* **Simple chains** (e.g., Prompt + LLM)  \n",
        "* **Sequential chains**  \n",
        "* **Custom chains** (your logic)\n",
        "\n",
        "Useful for workflows like:\n",
        "\n",
        "* Input → Prompt → LLM → Output  \n",
        "* Multi-step reasoning tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZYhSRxj_GfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐8. Memory**\n",
        "\n",
        "Memory allows LangChain apps to remember past interactions. Useful in:\n",
        "\n",
        "* Chatbots (session history)  \n",
        "* Agents (tool-use history)  \n",
        "\n",
        "Types include:\n",
        "\n",
        "* **BufferMemory**: Stores raw input/output  \n",
        "* **SummaryMemory**: Stores summarized history  \n",
        "* **TokenBufferMemory**: Limited by token count"
      ],
      "metadata": {
        "id": "DA58QTIi_Ihl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐Retrieval-Augmented Generation**\n",
        "\n",
        "- **RAG**\n",
        "\n",
        "    - a. **Document Loaders**\n",
        "\n",
        "        These ingest data from various sources:\n",
        "\n",
        "        * Files (PDF, .txt, CSV)  \n",
        "        * Web pages (via scraping)  \n",
        "        * APIs or databases  \n",
        "        LangChain supports many prebuilt loaders.\n",
        "\n",
        "    - b. **Text Splitters**\n",
        "\n",
        "        Large documents need splitting to avoid token limits. Splitters include:\n",
        "\n",
        "        * **RecursiveCharacterTextSplitter**: Preserves structure  \n",
        "        * **TokenTextSplitter**: Splits by model tokens  \n",
        "        Custom splitting strategies are also supported.\n",
        "\n",
        "    - c. **Embeddings**\n",
        "\n",
        "        Embeddings convert text chunks into vector representations for semantic similarity. Supported models:\n",
        "\n",
        "        * OpenAI Embeddings  \n",
        "        * HuggingFace Sentence Transformers  \n",
        "        * Cohere, Google, etc.\n",
        "\n",
        "    - d. **Vector Stores**\n",
        "\n",
        "        Storage solutions for embeddings. They support similarity search (k-NN). Popular vector stores:\n",
        "\n",
        "        * FAISS (in-memory)  \n",
        "        * Pinecone  \n",
        "        * Chroma  \n",
        "        * Weaviate\n",
        "\n",
        "    - e. **Retrievers**\n",
        "\n",
        "        Interfaces to get relevant chunks from vector stores.  \n",
        "        Can apply filters, scores, and combine retrievers for hybrid search (e.g., BM25 + vectors).\n",
        "\n",
        "    - f. **Building a RAG Application**\n",
        "\n",
        "        Combines:\n",
        "\n",
        "        - 1. Document ingestion (loaders)  \n",
        "        - 2. Chunking (splitters)  \n",
        "        - 3. Embedding + storing (vector store)  \n",
        "        - 4. Query-time retrieval  \n",
        "        - 5. LLM answering using retrieved context  \n",
        "\n",
        "        Use `RetrievalQA` chain or LCEL for more control.\n",
        "\n"
      ],
      "metadata": {
        "id": "LWxaAEk4-g2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐Agents**\n",
        "\n",
        "- a. **Tools & Toolkits**\n",
        "    - Tools are functions an agent can call. A tool may be:\n",
        "        - `Web search`  \n",
        "        - `Calculator`  \n",
        "        - `API call`\n",
        "        - `Database query`\n",
        "\n",
        "    > Toolkits group related tools (e.g., a SQL toolkit includes a queryexecutor + schema inspector).\n",
        "\n",
        "- b. **Tool Calling**\n",
        "    - Agents decide whether and when to use tools via reasoning (viaprompts). LangChain supports:\n",
        "        - OpenAI function calling (tool descriptions)  \n",
        "        - JSON mode (structured calls)  \n",
        "    \n",
        "    > Agents use intermediate steps (thoughts, actions, observations) toreach answers.\n",
        "\n",
        "- c. **Building an AI Agent**\n",
        "    - Steps:\n",
        "        - 1. Define tools  \n",
        "        - 2. Create prompt to guide agent behavior  \n",
        "        - 3. Choose agent type (`zero-shot-react`, `function-calling`, etc.)  \n",
        "        - 4. Initialize with `initialize_agent(...)`\n",
        "\n",
        "    - Agents can:\n",
        "        - Reason over multiple steps  \n",
        "        - Maintain state (via memory)  \n",
        "        - Decide which tool to use, and how"
      ],
      "metadata": {
        "id": "2BpZFe-p-kLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **⭕Example:**\n",
        "\n"
      ],
      "metadata": {
        "id": "Cb1zL0ObwygP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **⭐PDF-based RAG (Retrieval-Augmented Generation) Pipeline Architecture**"
      ],
      "metadata": {
        "id": "ULZzr5r5AHge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![LC_00_01](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_oth_img/LC_00_01.png)\n",
        "\n",
        "- **PDF Upload**: A PDF is uploaded to `AWS S3`.\n",
        "- **Document Loading**: The file is fetched using a Doc Loader.\n",
        "- **Text Splitting**: The PDF is split into individual pages or chunks.\n",
        "- **Embedding Generation**: Each chunk is converted into embeddings.\n",
        "- **Vector Database**: These embeddings are stored in a vector database.\n",
        "- **User Query**: The user query is also converted into an embedding.\n",
        "- **Semantic Search**: A semantic `similarity` search is performed to find relevant pages/chunks.\n",
        "- **Context Assembly**: Relevant document chunks and the user query are combined into a `system prompt`.\n",
        "- **LLM Processing**: This prompt is sent to an LLM `API` to generate the final answer."
      ],
      "metadata": {
        "id": "l79DKGQ1ALXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🔍 **LLM Framework Comparison: `LlamaIndex` vs `Haystack` vs `LangChain`**\n",
        "\n",
        "| Feature / Criteria                    | **LlamaIndex**                              | **Haystack**                                          | **LangChain**                                 |                              |\n",
        "| ------------------------------------- | ------------------------------------------- | ----------------------------------------------------- | --------------------------------------------- | ---------------------------- |\n",
        "| **Primary Use Case**                  | Document indexing & RAG pipelines           | Search & QA pipelines (initially Elasticsearch-based) | Modular LLM app building (RAG, agents, tools) |                              |\n",
        "| **Modular Components**                | ✅ Nodes, Indexes, Retrievers, Engines       | ✅ Pipelines, Nodes                                    | ✅ Chains, Runnables, Agents, Memory           |                              |\n",
        "| **Agent Support**                     | ⚠️ Limited (experimental or via plugins)    | ⚠️ Limited / basic (beta-level agent support)         | ✅ Full support (tools, memory, multi-step)    |                              |\n",
        "| **Retrieval-Augmented Generation**    | ✅ Native & optimized                        | ✅ Well-supported                                      | ✅ Fully supported with LCEL & RAG chains      |                              |\n",
        "| **Data Ingestion**                    | ✅ Document loaders & structured data        | ✅ Strong file + database support                      | ✅ Rich set of loaders (via integrations)      |                              |\n",
        "| **Integration with Vector Stores**    | ✅ FAISS, Chroma, Weaviate, Pinecone, etc.   | ✅ FAISS, Milvus, Elasticsearch, etc.                  | ✅ Wide integration with popular stores        |                              |\n",
        "| **Embeddings Support**                | ✅ OpenAI, HuggingFace, Cohere, etc.         | ✅ Wide support via transformers                       | ✅ Full support via plugins + LLM wrappers     |                              |\n",
        "| **Memory Management**                 | ⚠️ Basic context injection                  | ⚠️ Session-based history                              | ✅ Buffer, Summary, TokenBufferMemory types    |                              |\n",
        "| **Tool Use (Calculator, APIs, etc.)** | ❌ Not native (needs LangChain-style agents) | ⚠️ Custom node needed                                 | ✅ Native tool use with agents                 |                              |\n",
        "| **Custom Logic Composition**          | ✅ High-level with Composability APIs        | ✅ With custom pipelines                               | ✅ LCEL (\\`                                    | `, `+\\`) + Pythonic chaining |\n",
        "| **Ease of Use**                       | ✅ Simple RAG workflows                      | ⚠️ Moderate (YAML or Python)                          | ⚠️ Moderate–steep learning curve              |                              |\n",
        "| **Community & Docs**                  | ✅ Growing, great tutorials                  | ✅ Strong docs, active GitHub                          | ✅ Large community, frequent updates           |                              |\n",
        "| **Language Support**                  | 🟡 Python-only                              | 🟢 Python (core), some JS/TS & Java work              | 🟢 Python (main), TypeScript JS SDK (beta)    |                              |\n",
        "| **Production Readiness**              | ✅ Stable core with experimental agents      | ✅ Used in enterprise (deployed search apps)           | ✅ Production-ready (widely adopted)           |                              |\n",
        "| **Best For**                          | Fast RAG setup with structured data         | Hybrid QA systems, search with transformers           | Advanced LLM workflows, agents, tool use      |                              |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CnNK-yARjMk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **✅ Summary**\n",
        "\n",
        "| Use Case                   | Best Framework |\n",
        "| -------------------------- | -------------- |\n",
        "| Simple, fast RAG setup     | **LlamaIndex** |\n",
        "| Hybrid search pipelines    | **Haystack**   |\n",
        "| Tool-using agents & chains | **LangChain**  |"
      ],
      "metadata": {
        "id": "9Ujot2OYjtC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![divider.png](https://raw.githubusercontent.com/mohd-faizy/GenAI-with-Langchain-and-Huggingface/refs/heads/main/_img/_langCompIMG/divider.png)"
      ],
      "metadata": {
        "id": "IJJnrzUnJsmD"
      }
    }
  ]
}